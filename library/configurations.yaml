

PIPELINE_RUNNER:
  dataset_path: "./dataset/dynamic_dataset.csv"
  model_task: "classification"
  pipelines_names:
    not_baseline: ["ensembled", "tree_based", "support_vector_machine",
                              "naive_bayes", "feed_forward_neural_network", "stacking"]
    baseline: ["baselines"]
  include_plots: True
  serialize_results: True

dataset_runner:
    split_df: 
      p: 0.80
      step: .05
    encoding:
      y_column: "Category"
      train_size: 0.8
      validation_size: 0.1
      test_size: 0.1

feature_analysis_runner:
    features_to_encode: ["Reboot"]
    manual_feature_selection:
      mutual_information:
        threshold: 0.2
        delete_features: True
      low_variances:
        threshold: 0.01
        delete_features: True
      vif:
        threshold: 10
        delete_features: True
      pca:
        threshold: 0.95
        delete_features: True
    automatic_feature_selection:
      l1:
        max_iter: 1000
        delete_features: True
      boruta:
        max_iter: 10
        delete_features: True

modelling_runner:
    models_to_exclude:
      ensembled: ["Gradient Boosting", "Random Forest"]
      tree_based: ["Decision Tree"]
      support_vector_machine: ["Linear Support Vector Machine", "Non-linear Support Vector Machine"]
      naive_bayes: []
      feed_forward_neural_network: []
      stacking: []
      baselines: ["Logistic Regression (baseline)", ]
    hyperparameters:
      grid_space:
        random_forest:
          n_estimators: [50, 100, 150, 200]
          max_depth: [None, 10, 20, 30]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 4]
        decision_tree:
          criterion: ['gini', 'entropy']
          max_depth: [None, 10, 20, 30]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 5]
          max_features: [None, 'sqrt', 'log2']
          ccp_alpha: [0.0, 0.01, 0.1]
        naive_bayes:
          var_smoothing: [1e-12, 1e-6, 1e-3]
      tuner_params:
        max_iter: 2
        epochs: 2
        
    neural_network:
       initial_architecture:
          batch_size: 128
          epochs: 2
          n_layers: 1
          units_per_layer: [128]
          activations: ['relu']
          learning_rate: 0.001
    model_assesment:
      comments: "I know cate will not like this comment"
      cross_model_metrics: ["f1-score", "recall", "precision", "accuracy"]
      intra_model_metrics: ["f1-score", "recall", "precision", "accuracy"]
      results_summary:
        training_metric: "timeToFit"
        performance_metric: "accuracy"
      results_df_metrics: ["timeToFit", "timeToPredict"]
      per_epoch_metrics: ["accuracy", "loss"]
      best_model_selection_metric: "f1-score"
    serialize_models:
      serialize_best_performing_model: True
      models_to_serialize: []
      pipelines_to_serialize: []
    
    



BOT:
  channel: "#general"