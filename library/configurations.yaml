
# THIS CURRENT VERSION HAS THE BARE MINIMUM REQUIREMENTS TO RUN THE WHOLE PIPELINE WITH ALL THE MODELS.
# WHILE THIS MAY BE USEFUL FOR DEBUGGING, YOU WILL LIKELY NEED TO ADJUT THIS FILE
# IN ORDER TO GET ACCURATE INSIGHTS ON PERFORMANCE AND ACCURACY

PIPELINE_RUNNER:
  dataset_path: "./dataset/dynamic_dataset.csv"
  model_task: "classification"
  pipelines_names:
    not_baseline: ["ensembled", "tree_based", "support_vector_machine",
                              "naive_bayes", "feed_forward_neural_network", "stacking"]
    baseline: ["baselines"]
  include_plots: True
  serialize_results: True

dataset_runner:
    split_df: 
      p: 0.80
      step: .05
    encoding:
      y_column: "Category"
      train_size: 0.8
      validation_size: 0.1
      test_size: 0.1

feature_analysis_runner:
    features_to_encode: ["Reboot"]
    manual_feature_selection:
      mutual_information:
        threshold: 0.2
        delete_features: True
      low_variances:
        threshold: 0.01
        delete_features: True
      vif:
        threshold: 10
        delete_features: True
      pca:
        threshold: 0.95
        delete_features: True
    automatic_feature_selection:
      l1:
        max_iter: 1000
        delete_features: True
      boruta:
        max_iter: 20
        delete_features: True

modelling_runner:
    models_to_exclude:
      ensembled: ["Gradient Boosting", ]
      tree_based: []
      support_vector_machine: ["Linear Support Vector Machine", "Non-linear Support Vector Machine"]
      naive_bayes: []
      feed_forward_neural_network: []
      stacking: []
      baselines: ["Logistic Regression (baseline)" ]
    hyperparameters:
      grid_space:
        gradient_boosting:
          learning_rate: [0.01, 0.05, 0.1, 0.2]
          subsample: [0.5, 0.75, 1.0]
          n_estimators: [50, 100, 150, 200]
          max_depth: [10, 20, 30]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 4]
        random_forest:
          n_estimators: [50, 100, 150, 200]
          max_depth: [10, 20, 30]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 4]
        decision_tree:
          criterion: ['gini', 'entropy']
          max_depth: [10, 20, 30]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 5]
          max_features: ['sqrt', 'log2']
          ccp_alpha: [0.0, 0.01, 0.1]
      tuner_params:
        max_iter: 20
        epochs: 1
        
    neural_network:
       initial_architecture:
          batch_size: 128
          epochs: 30
          n_layers: 2
          units_per_layer: [512, 512]
          activations: ['relu', 'relu']
          learning_rate: 0.001
    model_assesment:
      comments: "HEAVY TRAINING GOING ON"
      cross_model_metrics: ["f1-score", "recall", "precision", "accuracy"]
      intra_model_metrics: ["f1-score", "recall", "precision", "accuracy"]
      results_summary:
        training_metric: "timeToFit"
        performance_metric: "accuracy"
      results_df_metrics: ["timeToFit", "timeToPredict"]
      per_epoch_metrics: ["accuracy", "loss"]
      best_model_selection_metric: "f1-score"
    serialize_models:
      serialize_best_performing_model: True
      models_to_serialize: []
      pipelines_to_serialize: []
    
    



BOT:
  channel: "#general"