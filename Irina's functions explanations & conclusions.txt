Model Interpretability: Feature Importance and Permutation Importance
* Feature Importance: Measures how valuable each feature is in improving the model’s predictions. In tree-based models, it is directly computed based on the splits that improve the prediction quality.
* Permutation Importance: Estimates feature relevance by randomly shuffling each feature and observing the impact on model performance. It can be applied to any model, not just trees.
Mathematical foundation
Feature Importance in Trees: In decision tree-based models (e.g., Random Forest, XGBoost), feature importance is typically calculated by the total reduction of a splitting criterion (e.g., Gini impurity or entropy) brought by each feature. Mathematically:
  

Where:
* N is the total number of training samples.
* Nn is the number of samples reaching node 
* ΔI(n) is the impurity decrease from splitting node 
This method is efficient and directly available for tree-based models, as they naturally compute these statistics during training.
Permutation Importance: This is model-agnostic. For each feature:
1. Calculate model's baseline performance (e.g., accuracy).
2. Shuffle the feature's values.
3. Measure the drop in performance.
4. The larger the drop, the more important the feature.
Permutation importance is typically used when the model does not expose native feature importances or when a more consistent, comparable measure across different model types is desired.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////


What Insights Are There?
The tree-based models — Gradient Boosting, Random Forest, and Decision Tree — consistently highlight a relatively small subset of features as being highly important. In particular, the Gradient Boosting and Decision Tree plots show a steep drop-off in importance beyond the top 10–15 features, suggesting a concentration of predictive power in a focused group of variables. Random Forest displays a slightly more distributed importance, but the top features remain dominant.
Conversely, the models using permutation importance — Logistic Regression and Naive Bayes — distribute importance more broadly. However, they still emphasize a similar core group of features, reinforcing the relevance of these variables across fundamentally different model types.
A notable observation in the Logistic Regression and Naive Bayes permutation importance results is the presence of features with negative importance. This occurs when permuting a feature improves the model's performance, suggesting that such features may introduce noise, multicollinearity, or irrelevant variance.
The Neural Network, also assessed via permutation importance, demonstrates a significant sparsity in importance distribution. The chart shows that a very small number of features carry almost all the predictive signal, with most features contributing near-zero importance. The clustering of importance scores near zero — and the heavy skew toward the top few features — suggests that the model is highly sensitive to a narrow subset of inputs. This behavior is expected in neural networks trained on high-dimensional data, especially when regularization or dropout helps filter out noise. The visual clutter in the plot, caused by an overwhelming number of irrelevant features, further supports the need for dimensionality reduction or feature pruning.
How These Insights Inform Further Pipeline Iterations
Feature Selection:
Features consistently ranked highly across both tree-based and permutation-based models — including the Neural Network — should be retained and prioritized in future iterations. The NN’s emphasis on a very narrow subset of variables further validates the importance of certain features while also highlighting the redundancy of many others. Features with negative or near-zero importance, especially in Logistic Regression, Naive Bayes, and the Neural Network, should be considered for removal or transformation to reduce overfitting and improve model interpretability and training efficiency.
Feature Engineering:
The dominance of certain features across models presents a strong case for creating higher-order or interaction terms from them. In particular, the NN's reliance on only a few features suggests that embedding or encoding those features more effectively (e.g., through domain-informed transformations) could amplify their impact without adding unnecessary complexity.
Model Selection and Regularization:
The sharper isolation of important features by Gradient Boosting, Decision Trees, and Neural Networks suggests these models are better suited for high-dimensional spaces with sparse relevance. Logistic Regression and Naive Bayes benefit significantly from careful feature curation and strong regularization. This contrast supports the use of automated feature selection techniques and tailored regularization strategies depending on the model class being applied.
Excluded Alternatives / Methodological Decisions
Why Permutation Importance for Logistic Regression, Naive Bayes, and Neural Network:
These models do not provide built-in feature importance metrics like tree-based models. Permutation importance enables a consistent, model-agnostic approach to evaluate how each feature affects performance. In the case of the Neural Network, this is particularly valuable as interpreting deep learning models is inherently more complex due to their lack of transparency. However, the cost is a noisier signal and higher computation
No Dimensionality Reduction:
Dimensionality reduction methods such as PCA or feature clustering were intentionally excluded to maintain feature interpretability. This decision is especially important in settings where understanding model behavior is essential, such as in regulatory, security, or diagnostic contexts. However, given the clear redundancy observed in the Neural Network importance chart, applying dimensionality reduction or feature pruning may be valuable in future iterations without compromising too much interpretability.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Model Interpretability with SHAP
SHAP (SHapley Additive exPlanations) uses cooperative game theory to assign an importance value to each feature for a given prediction.
Mathematical foundation
Each feature's SHAP value ( \phi_j ) is the average marginal contribution across all possible coalitions:
  



Where:
* F: the set of all features
* S: any subset of features that doesn't include j
* f(S): model prediction using only features in S
SHAP values guarantee consistency and local accuracy, making them powerful for both global and local interpretability.
SHAP is a powerful method for interpreting machine learning models. However, its compatibility and efficiency depend on the type of model used.
For tree-based models SHAP usually works particularly well. This is because it can leverage the internal structure of trees using its TreeExplainer, resulting in fast and precise feature attributions.
For linear models SHAP can also be applied. Here, it uses either the LinearExplainer or the more general KernelExplainer. While the method remains valid, it is computationally more expensive and slightly less direct compared to tree models.
Naive Bayes models can technically be explained with SHAP, using the KernelExplainer. However, this approach is less common, slower, and may yield explanations that are less stable or interpretable. Similarly, models like non-linear Support Vector Machines (SVMs), especially those that do not implement predict_proba, rely on approximate methods like KernelExplainer. This makes SHAP explanations feasible but computationally intensive and less precise for large datasets.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////


What Insights Are There?
* Key Feature: API_Base64_android.util.Base64_decode stands out as the most influential feature, especially for Class 2 and Class 7. This suggests that it has the strongest and most consistent influence across multiple classes.
* Multiclass Impact: Features like Runtime_exec influence many classes, suggesting broad but less specific predictive value.
* Shared Predictors: Several APIs contribute to multiple classes, indicating overlapping behaviors among classes. This reinforces the idea that certain APIs serve as class-specific signals and are not globally useful or harmful across all outputs.
How These Insights Inform Further Pipeline Iterations
* Fine-Grained Feature Engineering:
SHAP reveals the directional and class-specific impact of features, unlike global permutation importance. This allows pipeline improvements such as creating one-hot flags for highly influential APIs only when used in conjunction with others.
* Multiclass Calibration Focus:
Since features are influencing multiple classes, future model iterations may benefit from explicitly modeling inter-class relationships to exploit shared feature patterns..
* Targeted Regularization:
Features with high SHAP values across too many classes (like Runtime_exec) may suggest overfitting or reliance on generic signals. Introducing class-specific penalties or limiting their influence (via interaction terms or constrained regularization) could enhance generalization.
Excluded Alternatives / Methodological Decisions
   * Why SHAP for Logistic Regression: SHAP was chosen for its ability to provide class-specific, model-consistent explanations. While linear models are often interpretable via coefficients, SHAP goes further by quantifying how each feature contributes to each prediction, taking into account interactions with other features..
   * Sampling Subset: A subset of 100 instances was used to reduce computation time. While this limits generalizability, it still captures key trends across classes and maintains feasibility during development.
   * SHAP vs. Coefficients: Traditional coefficient-based analysis was avoided here because SHAP offers a richer, more intuitive representation, especially in multiclass settings where raw coefficients may not translate clearly into prediction importance.
   * Why Not SHAP for Gradient Boosting: Tree-based models with many branches and classes often lead to unstable or overly diffuse SHAP explanations in multiclass settings, making results harder to interpret and less reliable than global importances.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Model Interpretability with LIME
LIME (Local Interpretable Model-agnostic Explanations) provides local explanations by approximating the model around a specific instance with a simple interpretable model (e.g., linear model).
Procedure
   1. Perturb the instance and generate similar samples.
   2. Get model predictions on the perturbed samples.
   3. Fit a weighted interpretable model to the predictions.
   4. Use this local model to explain the original prediction.
For tree-based models such as gradient boosting and random forests, LIME works particularly well. These models are ensembles of decision trees, and LIME can generate local approximations by sampling from the data and explaining individual predictions. The structure of these models allows LIME to provide meaningful insights into specific predictions.
For decision trees, LIME can still be useful, although decision trees are already relatively interpretable on their own. LIME may provide additional clarity by offering detailed explanations for individual instances, particularly when combined with a more complex pipeline.
For non-linear support vector machines (SVMs), LIME may struggle. These models involve complex kernel functions that are not easily approximated by LIME, leading to less reliable explanations. While it is still technically possible to apply LIME, the approximations may not be accurate enough, especially with non-linear kernels.
Naive Bayes, on the other hand, is based on strong independence assumptions between features, and its predictions are typically straightforward. While LIME can be applied, it is not as useful as for more complex models. LIME may provide explanations, but these explanations may not offer much added value since Naive Bayes is already interpretable by design.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////


What Insights Are There?
The LIME explanations for both the Gradient Boosting and Logistic Regression models reveal that a small group of API-related features consistently contribute the most to the model predictions. In the case of Gradient Boosting, features such as API_DexClassLoader, API_DeviceData, and API_SystemManager stand out with relatively higher contribution values, while many other features show near-zero influence. This suggests that the Gradient Boosting model is focusing its predictive power on a narrow, high-signal subset of the feature space.
On the other hand, the Logistic Regression model also highlights API_DexClassLoader and API_DeviceInfo among its most influential features, though the contributions appear slightly more evenly spread. This cross-model consistency reinforces the critical role these API calls play in distinguishing between classes in the dataset.
Interestingly, both models show several features with zero or even negative contribution values in the LIME explanations. This suggests that these features might be adding noise or redundancy, particularly in the Logistic Regression model, where multicollinearity and irrelevant features can directly degrade linear decision boundaries.
How These Insights Inform Further Pipeline Iterations
These LIME results suggest that the pipeline could be optimized by focusing on the small subset of API features that consistently show predictive value. Features that repeatedly have zero or minimal contribution should be reviewed for removal, as they may only introduce noise or complexity without adding discriminative power. Additionally, the concentration of importance in certain API calls points to an opportunity for more targeted feature engineering around these key signals.
By streamlining the feature set and enhancing the representation of the top contributors, future iterations of the pipeline can achieve better performance and interpretability, particularly for models sensitive to feature quality like Logistic Regression.
Excluded Alternatives / Methodological Decisions
   * Why LIME Explanations: LIME was chosen for its ability to provide local, model-agnostic explanations that highlight which features influence individual predictions. This is particularly useful given that Logistic Regression lacks native feature importance metrics, and even for Gradient Boosting, LIME offers an intuitive way to interpret prediction reasoning at the instance level.
   * No Dimensionality Reduction: Dimensionality reduction techniques such as PCA or clustering were intentionally omitted to preserve the original feature interpretability. This decision aligns with the requirement for explainability, especially when dealing with API call logs that have operational or security implications.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Intra-model feature importance methods comparisons: logistic regression
When comparing feature importance, SHAP, and LIME for the same logistic regression model, we see both agreements and differences. The feature importance scores show that "Network_TotalTransmittedBytes" and "Network_TotalReceivedBytes" are the top global contributors, while many API-related features have zero impact. Memory features like "Memory_PrivateDirty" and "Memory_PssTotal" also appear important but with a smaller effect compared to the network features. This method gives a clear but broad ranking without much detail on class-specific or instance-specific behavior.


SHAP values, shown in the left plot, reinforce the importance of network features, especially "Network_TotalReceivedBytes," but go further by breaking down the contribution across different classes. SHAP highlights that memory features like "Memory_PssTotal" and "Memory_PrivateDirty" have varying influence depending on the class, providing a richer, more detailed explanation than the general feature importance ranking.


LIME, on the right, offers a local explanation for class 1. It also points out the relevance of "Network_TotalReceivedBytes" and "Memory_PssTotal," aligning with the other methods. However, LIME uniquely identifies specific API calls, like "API_DeviceData_android.location.Location_getLongitude," which were globally unimportant but play a role in this particular prediction. This shows LIME's strength in capturing local behavior that global methods might overlook.


In short, feature importance gives a broad view, SHAP adds class-level detail, and LIME highlights features critical for individual predictions.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Intra-model feature importance methods comparisons: neural network
When analyzing the three explanation techniques applied to the neural network, both relevant similarities and differences emerge. According to the global feature importance scores, memory-related features dominate: "Memory_PssTotal" stands out as the most influential, followed by "Memory_PssClean" and "Memory_PrivateDirty." Network packet transmission and reception totals also show significant weight. Meanwhile, APIs related to device, network, and data access have zero global importance, suggesting that their impact is not generalizable across the entire dataset.
On the other hand, SHAP values confirm the relevance of memory metrics, especially "Memory_PssTotal," and highlight that these features are not only important globally but also show significant variations depending on the predicted class. SHAP makes it possible to see how these features contribute positively or negatively to different classes, offering a level of detail that goes beyond simple global ranking.
LIME, in contrast, provides a local perspective focused on a specific prediction for class 1. In this explanation, several APIs that were globally irrelevant emerge as key, such as "API_DeviceData_android.media.AudioRecord_startRecording" and "API_DeviceInfo_android.net.wifi.WifiInfo_getNetworkId." Additionally, "Memory_PssTotal" appears with a negative influence in this specific instance. This demonstrates LIME's strength in identifying factors that are decisive in individual cases, even if they are overlooked in global analyses.
In summary, feature importance offers a broad and frequency-based view, SHAP adds detail on how each feature influences depending on the class, and LIME reveals the particularities of individual cases, highlighting local patterns that other methods might miss.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Model Calibration Analysis: Reliability Diagram
How the Math Works
A reliability diagram compares the predicted probabilities from a classifier against the actual observed frequencies of outcomes. For multiclass problems, calibration is computed per class using the One-vs-Rest strategy.
For each class k, we compute the calibration curve over bins of predicted probabilities:
  

Where:
   * p^i(k) is the mean predicted probability for class k in bin i
   * Bi is the set of samples in bin i (based on predicted probabilities for class k)
   * 𝟙[⋅] is the indicator function: 1 if yj=k, otherwise 0
A perfectly calibrated classifier will have:
  

Which appears on the diagonal line in the reliability diagram: y = x.


///////////////////////////////////////////////////////////////////////////////////////////////////////////////


What Insights Are There?
Gradient Boosting shows significantly better calibration across most classes compared to Logistic Regression. The curves for Gradient Boosting are closer to the diagonal "perfectly calibrated" line, suggesting its predicted probabilities align more closely with actual outcomes.
Logistic Regression appears poorly calibrated for many classes. Most of its curves deviate strongly from the diagonal, with some classes showing underconfidence or overconfidence, and a few showing erratic behavior due to sparse predicted probabilities.
For both models, some classes perform much better than others, implying that the difficulty or representation of classes in the data may vary.
Gradient Boosting provides meaningful probabilities across the full range (0 to 1), whereas Logistic Regression's predicted probabilities are often clustered in the lower range, limiting their interpretability.
How These Insights Inform Further Pipeline Iterations
Gradient Boosting is the preferable base model if probability calibration is important (e.g., in risk-sensitive applications like medical or financial decision-making).
Although Gradient Boosting already performs well, additional calibration techniques (like isotonic regression) could be tested for marginal gains, especially for the worst-performing classes.
Logistic Regression may benefit from resampling strategies, feature engineering, or regularization tuning to improve the quality of probability outputs.
Consider using class-specific calibration or ensemble methods that combine well-calibrated and high-accuracy models to balance both metrics.
Given that some classes are poorly calibrated in both models, it may be worth revisiting class distribution, label quality, or using hierarchical/multitask learning for improvements.
Excluded Alternatives / Methodological Decisions
The reliability diagrams used sigmoid calibration with internal cross-validation, which may not be optimal for all models or classes. Alternatives like isotonic regression or Bayesian Binning were not evaluated.
We used a fixed number of 10 bins for calibration curves; adaptive binning or using ECE/MCE scores could provide a more nuanced view.
Only one-vs-rest calibration was applied in a multiclass setting. More complex methods like Dirichlet calibration could handle multiclass probabilities more effectively.
Calibration was applied post-hoc; some models (like neural nets) could be trained with temperature scaling or focal loss to produce better-calibrated outputs natively.
We are using isotonic regression over Platt scaling (sigmoid method) for probability calibration because isotonic calibration provides more flexibility and typically performs better when sufficient calibration data is available, particularly in multiclass or non-linear settings.




///////////////////////////////////////////////////////////////////////////////////////////////////////////////


Model Calibration for All Models


The calibration curve above summarizes the performance of several models that predict probabilities for Class 0. A calibration curve should closely follow the dashed diagonal line - which indicates perfect calibration - where the predicted probabilities match the observed outcomes.


From the plot, we can see that the Gradient Boosting model has great calibration, especially at the high probability bins, it closely follows the perfect calibration line. The Random Forest and Decision Tree models show relatively good calibration for Class 0, but somewhat underpredict probabilities at the midpoint. SVM models (both linear & non-linear) show some underpredictions in some range but others overpredicting which means they did not calibrate as well. The Naive Bayes model demonstrates the most significant deviations from calibrated probabilities, especially with mid-level probabilities, stating that it provides more likely outcomes in terms of positives. The Logistic Regression model utilized for a baseline model, generally tracks closer to the diagonal and displays reasonably well calibration but especially at the lower ends of probabilities.


Overall, ensemble methods such as Gradient Boosting appear better calibrated than simpler models like Naive Bayes or the SVMs, with Logistic Regression offering a solid baseline.