{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> DYNAMIC MALWARE CLASSIFICATION</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised, classification, multi-class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szqv1Y9Edzc5"
      },
      "source": [
        "# 0. Basic Set-Up and General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: there are a lot of non-used imports (to be removed at the end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "from skopt.space import Real\n",
        "\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import StackingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "from boruta import BorutaPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from library.utils.ownModels.majorityClassModel import MajorityClassClassifier # Importing self-developed model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "# Import your modules initially\n",
        "import library.pipeline\n",
        "\n",
        "# Reload the modules in case they were updated\n",
        "importlib.reload(library.pipeline)\n",
        "\n",
        "# Now you can import specific classes/functions\n",
        "from library.pipeline.pipeline import Pipeline\n",
        "from library.pipeline.pipeline_manager import PipelineManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation Standarization\n",
        "Follow the following guidelines to make sure our document is consistent and easier to undestand\n",
        "- All constant should be writte in full capitall letters (e.g: MY_CONSTANT). Notebook-level constants should be written in the cell assigned to it (see below)\n",
        "- Every new major section (EDA, Feature engineering) should be written with '# My title'#\n",
        "- Subsequent sections should use '##', '###' or '####' hierarchacically\n",
        "- At the beginning of each major section write a content index with the content included in that section (see 'feature engineering' section as an example)\n",
        "- Please write paragraphs before and after each cell explain what u are about to do and the conclusions, correspondingly. Do not assume they are too obvious.\n",
        "- Avoid excessive ChatGPT-originated comments\n",
        "- Avoid writing more than 20 lines per code cell (exceptions for subroutines, which should be written in utilitied_functions.py)\n",
        "- Ideally, add a \"Questions\" and \"Things to be done\" section in each major sections where u write about futher iterations u want to do (while sharing in with the rest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path =\"./dataset/dynamic_dataset.csv\"\n",
        "results_path = \"results/model_evaluation/results.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start off with a pipeline for all the models. Then at some moment, whenever we think the pipelines will diverge, we remove the by-object-refrence (that we will create in the next cell), and create a new copy with our modifications ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Selection\n",
        "We will having pipelines for:\n",
        "- ensembled_models (random forest, gradient boosting)\n",
        "- tree-based (decision trees)\n",
        "- support vectors machines (non-linear*)\n",
        "- naive bayes (gaussian naive bayes)\n",
        "- stacking (stacking models)\n",
        "\n",
        "\n",
        "* We originally conceived to add linear for the sake of model diversity. While the theory already pointed us out that the linear SVM wont be able to find the separating hyperplane (i.e., model is able to fit), we wanted to see if by setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline = ensembled_pipeline = tree_pipeline = supportVectorsMachine_pipeline = baseline_pipeline = naiveBayes_pipeline = stacking_pipeline = feedForwardNN_pipeline = example = Pipeline(\n",
        "                        dataset_path=dataset_path, \n",
        "                        results_path=results_path,\n",
        "                        model_type=\"classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipelines = {\n",
        "            \"not-baseline\": {\n",
        "                  \"ensembled\": ensembled_pipeline,\n",
        "                  \"tree-based\": tree_pipeline,\n",
        "                  \"linear\": supportVectorsMachine_pipeline,\n",
        "                  \"naive-bayes\": naiveBayes_pipeline,\n",
        "                  \"stacking\": stacking_pipeline,\n",
        "                  }, \n",
        "            \"baseline\": {\n",
        "                  \"baselines\": baseline_pipeline, \n",
        "                  \"example\":  example               \n",
        "            }\n",
        "}\n",
        "pipeline_manager = PipelineManager(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I show an example of how we will be working with by-object-reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline.dataset.example_attribute = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.example_attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we do a (deep) copy:\n",
        "Note that the difference between a deep and a shallow copy is that the former copies all the objects that are inside the class. We need that.\n",
        "We will overwrite the prior attribute and show is this is not propagated to the other results anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_pipeline = copy.deepcopy(default_pipeline)\n",
        "new_pipeline.dataset.example_attribute = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.example_attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It may also be of interest for you to propagate the same method's invokation to all the pipelines within the pipeline manager. For instance, after a divergence in, lets say, feature scaling, you may need to fit all the models in the corresponding pipelines. You would want to avoid reduntandly calling the same method with the same parameters for all the Pipeline class instances. In order to do so, we present an example that avoids such annoyance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.create_pipeline_divergence(category=\"baseline\", pipelineName=\"example\", print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines[\"baseline\"][\"example\"].feature_analysis.feature_selection.automatic_feature_selection.speak(\"Whats goooddd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"speak\", message=\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A method present deeper down the class:\n",
        "\n",
        "Instead of doing:\n",
        "'pipeline_manager.pipelines[\"baseline\"][\"logistic\"].feature_analysis.feature_selection.automatic_feature_selection.speak(\"Whats goooddd\") for all objects'\n",
        "\n",
        "we can do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"feature_analysis.feature_selection.automatic_feature_selection.speak\", message=\"Whats good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets delete the example obj\n",
        "del new_pipeline \n",
        "del pipeline_manager.pipelines[\"baseline\"][\"example\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start Of The Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "F0L2m8yFXhJ9",
        "outputId": "c94d1d43-6998-486d-d0ea-8b643d64efa6"
      },
      "outputs": [],
      "source": [
        "default_pipeline.dataset.df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in default_pipeline.dataset.df.columns.to_list():\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.df[\"Category\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.df.rename(columns={\"reboot\": \"Reboot\"}, inplace=True) # consistency with other features' names\n",
        "default_pipeline.dataset.df.drop(columns=[\"Family\", \"Hash\"], inplace=True) # We have decided to use only category as target variable; Hash is temporary while im debugging (it will be deleted in EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Description of our features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each of our entries in the dataset we have a feature describing the state of the (operating) system under which the malware was running. Here is an intial description for each of them.\n",
        "Please access this document for a complete problem context description: https://docs.google.com/document/d/1yH9gvnJVSH9GLv9ATQ5JQWA2z8Jy4umxxRfMF-y2fiU/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section below shall be deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.data_preprocessing.remove_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.data_preprocessing.get_missing_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we conduct an Exploratory Data Analysis (EDA) on the dynamic malware dataset to gain initial insights into the structure, distribution, and quality of the data prior to modeling. The dataset includes behavioral features extracted from Android applications, along with labels indicating their respective malware categories. Understanding the composition of the dataset, such as class imbalance, feature correlations, and the presence of outliers, is crucial to ensure robust preprocessing, informed feature engineering, and  the success of machine learning classifiers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SUGGESTIONS FOR IMPROVEMENTS\n",
        "- Cluster analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. DATA SPLITTING\n",
        "### TO BE DONE\n",
        "- Statistical analysis of this\n",
        "- Make sure they follow the same distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting: Category as target variable\n",
        "Originally, we will focus only on category\n",
        "\n",
        "Lets first get the X and y extracted from our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also object!\n",
        "Lets get back to the splitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before, we split the dataset lets observe the SE of accuracy variation based on our choice of split. \n",
        "Brief explanation: we can model accuracy via a Binomial distribution. We know each event in a binomial distribution can be modelled through a bernoulli distribution, where the outcome represents the probability predicting the correct class or not. We make the assumption that each classification error is independent from each other. For:\n",
        "$$\n",
        "\\text{Bin} \\sim (n, p)\n",
        "$$\n",
        " let us assume that the parameter of this distribution is p = .85 and n is given by the choice of sample split for the test set. The SE of the sample proportion can be modeled via: \n",
        "$$\n",
        "\\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\n",
        "\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we continue to assess all possible choices of split based on a variant n, also note that that choice of evenly distributed split (e.g: 10% for each hold-out set) between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n",
        "\n",
        "Finally, also note that that choice of evenly distributed split between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.split.asses_split_classifier(p=.85, step=.05, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[We can see a diminishing-returns class graph](https://en.wikipedia.org/wiki/Knee_of_a_curve). The more we decline the training set percentage the slower and more steadier the current SE varies as well as the difference to prior SE, in percentage. We can see that the knee in the curve is between 80 and 90 training set percentage. This represents the area where when you start going below the lower bound, no **significant** improve appears. Given the fact that our criteria for choice of split percentage is to keep as much training possible while increasing hold-out sets size only if the decrease in SE is significant **we are going to select 80% for the training set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.split.split_data(y_column=\"Category\",\n",
        "                                   train_size=.8, \n",
        "                                   validation_size=.1,\n",
        "                                   test_size=.1, \n",
        "                                   plot_distribution=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.X_train.shape, default_pipeline.dataset.X_val.shape, default_pipeline.dataset.X_test.shape, default_pipeline.dataset.y_train.shape, default_pipeline.dataset.y_val.shape, default_pipeline.dataset.y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section below shall be properly written. It is set now for early debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## !> Pipeline example\n",
        "Let us show an example of the pipeline divergence. We will create a divergence for the baseline pipeline. We will exempt it from being scaled. This specific example is not meant to be kept, but rather show the purpose of the corresponding functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline = pipeline_manager.create_pipeline_divergence(category=\"baseline\", pipelineName=\"baselines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.data_preprocessing.scale_features(scaler=\"robust\",\n",
        "                                      columnsToScale=default_pipeline.dataset.X_train.select_dtypes(include=[\"number\"]).columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is not updated (as expected)\n",
        "baseline_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outliers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.preprocessing.get_outliers_df(plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. FEATURE ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will take adjust the features that compose the learning inputs to our model. The correctness of this section is pivotal for proper learning by the model\n",
        "\n",
        "\n",
        "## FEATURE ENGINEERING\n",
        "- Domain-specific features\n",
        "- Binning\n",
        "- Interaction terms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "featuresToEncode = [\"Reboot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_maps_perPipeline = pipeline_manager.all_pipelines_execute(methodName=\"feature_analysis.feature_transformation.get_categorical_features_encoded\", verbose=True, features=featuresToEncode, encode_y=True)\n",
        "encoded_maps_perPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.encoded_map = encoded_maps_perPipeline[\"baseline\"][\"baselines\"] # setting baseline as an arbitrary example (all are the same as for now)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the results of the encoding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline.dataset.X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline.dataset.y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FEATURE SELECTION\n",
        "- Analyze correlation and low-variances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection \n",
        "As explained before, we now proceed to carefully reduce the present high-dimensionality. High-dimensionality increases the chances of the model overfitting (capturing noise from irrelevant features; increasing variance and reducing bias), as well as introducing a significant computational overhead. We match this high-dimensionality with a highly filtering models_pipeline of feature selection.\n",
        "The most extensive cut comes given at the first level, with the mutual information threshold-based cut. This metric captures the level of uncertainity between the feature and the target variable (cnt). In marked contrast with pearson coefficeint (correlation), it is able to model non-linear and linea relationships altoghther. \n",
        "This feature-selectin models_pipeline is compromised of five (3 as of the final models_pipeline) cuts:\n",
        " - mutual information\n",
        " - low variance\n",
        " - multicolinearity analysis\n",
        " - PCA\n",
        " - Boruta and/or Lasso\n",
        "\n",
        "The different thresholds for each of this cuts have been altered over the different models_pipeline iterations. Specifically, in the bias-variance tradeoff (to be elaborated in further detail later), I increased all the thresholds in order to avoid por performance.\n",
        "\n",
        "#### Feature to target variable mutual information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"MutualInformation\", threshold=.2, delete_features=True, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Eliminating low-variances features\n",
        "Features with low variances provide little new information for the model to learn from, thus they could introduce statistical noise. Due to this reason, they should be elimanted from the dataset. The reason why we don't focus on high-variance is because this symbolizes outliers, which have been dealt with before in data preporcesing.\n",
        "This function call eliminate low-variance (based on threshold) and all cosntant variables (regardless of threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start off this analysis eliminating univariate features (i.e: the featuers with constant values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# default_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"LowVariances\", \n",
        "#                                                                          threshold=0.5, \n",
        "#                                                                          delete_features=True, \n",
        "#                                                                          plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As explained in scikit-learn's in (1.13.1 Feature selection)[https://scikit-learn.org/stable/modules/feature_selection.html], the variance threshold must be selected carefully. Too low may delete few variables, and too may be too restrictive, deleting more variables than it should."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Eliminating highly correlated feature \n",
        "Highly correlated variables (multicolinearity) are problem for models because they introduce a redundancy (features that contain significantly related similar information are not bringing much new insight into the model's input) to the model that can introduce significant variance. This is due to the fact that small changes in the data may make the coefficeints of the highly correlated variables **swing** more than it should\n",
        "\n",
        "A note on the shape of this heatmap: due to the high amount of features, and the redundancy to measure the correlation between features (where corr(A,B) = corr(B,A)) we set 'np.triu(np.ones_like(corr, dtype=bool))' in the utilities functions in order to show only new non-redudant correlations between features, thus the right triangle shape.\n",
        "Well, thats a lot to digest! We can see some solid red (high positive correlation) and medium-solid blue (some high negative correlation). \n",
        "Lets use a non-visual methodology to confirm our initial hypothesis. We will use variance inflaction factor (VIF) along with checking manually.\n",
        "A brief explanation on VIF. \n",
        "Formula: \n",
        "$$\n",
        "VIF_i = \\frac{1}{1 - R_i^2}\n",
        "$$\n",
        "You regress (i.e: do linear regression) on the ith feature as target variable, and all other features as predictors. You then compute the coefficient of determination as a way to measure how well the predictors fit the target variable. VIF values ranges from [1, inf], where lower bound signifies little multicolinearity (R^2 = 0), and upper bound occurs when R^2=1 (perfect multicolineairty). 5 is considered a standard threshold for VIF as it symbolized an 80% R^2.\n",
        "Once you ve obtained the results of VIF, you need to delete the variable with the highest VIF, and recompute VIF until there is no multicolinearity. For example, say there are three features that are 4 features, 3 of them being linear combinations of each other. You would delete the variables with VIF until there is no VIF (when only one of those linear combinations remains). You cant delete all n-1 variables where n are the amount of variables with exceeding (with respect to the chosen threshold), because you may be deleting one that has high VIF due to another feature.\n",
        "\n",
        "A relevant note on why one-hot-encoding must be done dropping the first one:\n",
        "If we were to not remove one of the labels of one hot encoding, you would be able to predict which level of the categorical variable based on all other ones (there is only degree of freedom for categories levels; they are a linear combination). You would essentially see inf VIF in that area and delete it in this section. \n",
        "The VIF has to be computed every time we delete a feature due to high multicolinearity. Lets do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"VIF\", threshold=10, delete_features=True, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA\n",
        "PCA can still bring some more value to feature selection. It will take into account interaction effects by itself and find the principals that capture as much variance as we specify. Thus, its inclusion in the feature selection models_pipeline.\n",
        "It has been excldued due to underperformance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"PCA\", threshold=.95, delete_features=False, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Automatic Feature Selection\n",
        "#### L1 regularization\n",
        "Because of L1 regularization being able to set weights 0, we can briefly train our logistic regression model with such regulartization and see which features it uses. The reason why it sets 0 to insignificant feature is because the objective function is not only the MSE but added a component of the wieght magnitude (which is trying to minimize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# excluded_features, predictive_features, coefficients = models_pipeline.feature_analysis.feature_selection.automatic_feature_selection.fit(type=\"L1\",\n",
        "#                                                                                                                                     max_iter=1000, \n",
        "#                                                                                                                                     print_results=True, delete_features=False)\n",
        "# excluded_features, len(excluded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BORUTA\n",
        "Boruta is a more powerful feature selection method (thus we use it as a reference for variable deletion). It is more powerful that L1 becuase it compares the importance of features to shuffled versions, ensuring robust feature selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# excluded_features, predictive_features = models_pipeline.feature_analysis.feature_selection.automatic_feature_selection.fit(type=\"Boruta\", max_iter=10, print_results=True, delete_features=False)\n",
        "# excluded_features, len(excluded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome, lets move onto the actual modelling part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QUESTIONS FOR THIS SECTION\n",
        "- Can the ROC curve be used for multiclass?\n",
        "- Can a unsupervised learning algorithm (e.g: KNN) be used for this problem even tough its nature is to be supervised?\n",
        "- Does val to train deltas are meaningful here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TO BE DONE FOR THIS SECTION\n",
        "- Multiple models as classifiers\n",
        "- Learn more about each alogrithms' paremters\n",
        "- Can KNN be used here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest & Decision Trees\n",
        "Instead of the originally planned logistic regression, we will be using an ensembled model first: random forest (collection of week decision trees). This model is not only likely to outperform the original choice because its nature to handle multiclass better, but also does this several orders of magnitude faster. We will add its not ensembled version too, along with gradient boosted machine\n",
        "Note we are not using not-by-default multiclass classifiers (e.g: logistic regressions, svms)\n",
        "\n",
        "## Non-optimized fitting\n",
        "We first fit all these models with the default paramterers. This is done to constrat more starkly the difference between pre and post tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "gradientBoostingModel = GradientBoostingClassifier()\n",
        "randomForestModel = RandomForestClassifier()\n",
        "\n",
        "# Tree-based models\n",
        "decisionTreeModel = DecisionTreeClassifier()\n",
        "\n",
        "# Support Vector Machines models\n",
        "nonLinearSupportVectorModel = SVC()\n",
        "linearSupportVectorModel = LinearSVC()\n",
        "\n",
        "# Naive Bayes model\n",
        "naiveBayesModel = GaussianNB()\n",
        "\n",
        "# Stacking model\n",
        "stackingModel = StackingClassifier(\n",
        "      estimators=[\n",
        "            (\"Random Forest\", randomForestModel),\n",
        "            (\"Decision Tree\", decisionTreeModel),\n",
        "            (\"Non-Linear SVM\", nonLinearSupportVectorModel),\n",
        "            (\"Linear SVM\", linearSupportVectorModel),\n",
        "            (\"Gaussian Naive Bayes\", naiveBayesModel)\n",
        "      ],\n",
        "      final_estimator=DecisionTreeClassifier(),\n",
        "      cv=5,\n",
        "      verbose=3\n",
        ")\n",
        "\n",
        "# Baseline models\n",
        "logisticRegressionModel = LogisticRegression()\n",
        "majorityClassModel = MajorityClassClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MODEL PERFORMANCE, HYPOTHESIS\n",
        "- Trees (e.g: random forest, decision trees):\n",
        "  - Time to fit:\n",
        "    - Ensemble models (random forest) take more time to train due to the fact that they are larger and heavier than their non-ensembled version. \n",
        "  - Correctness:\n",
        "     - High\n",
        "- Binary-classifiers by default models (e.g: SMV, logistic regression)\n",
        "  - Time to fit:\n",
        "    - compute C models for all C number of classes. Each trained to detect a single class, then when we make predictions, we select the ones that has the highest probability in its predictions (\"the most confident in its prediction\"). This strategy is called One-vs-Rest, note however, a single logistic regression may be used if we used the softmax objective function (instead of log-odds) (it still heavy computationally, tough). They will be sloder than ensemble models\n",
        "  - Correctness:\n",
        "     - Very low if the problem is non-linear which it is for the SVM and logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipelines always need to diverge from training onwards. Otherwise they will have each other results (which does not follow the isolation pattern we have programmed this with)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensembled_pipeline = pipeline_manager.create_pipeline_divergence(category=\"not-baseline\", pipelineName=\"ensembled\")\n",
        "supportVectorsMachine_pipeline = pipeline_manager.create_pipeline_divergence(category=\"not-baseline\", pipelineName=\"linear\")\n",
        "naiveBayes_pipeline = pipeline_manager.create_pipeline_divergence(category=\"not-baseline\", pipelineName=\"naive-bayes\")\n",
        "stacking_pipeline = pipeline_manager.create_pipeline_divergence(category=\"not-baseline\", pipelineName=\"stacking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "ensembled_pipeline.model_selection.add_model(\"Gradient Boosting\", gradientBoostingModel)\n",
        "ensembled_pipeline.model_selection.add_model(\"Random Forest\", randomForestModel)\n",
        "\n",
        "# Tree-based models\n",
        "tree_pipeline.model_selection.add_model(\"Decision Tree\", decisionTreeModel)\n",
        "\n",
        "# Support Vector Machines models\n",
        "supportVectorsMachine_pipeline.model_selection.add_model(\"Non-Linear SVM\", nonLinearSupportVectorModel) \n",
        "supportVectorsMachine_pipeline.model_selection.add_model(\"Linear SVM\", linearSupportVectorModel) \n",
        "\n",
        "# Naive Bayes\n",
        "naiveBayes_pipeline.model_selection.add_model(\"Gaussian Naive Bayes\", naiveBayesModel)\n",
        "\n",
        "# Stacking\n",
        "stacking_pipeline.model_selection.add_model(\"Stacking\", stackingModel)\n",
        "\n",
        "# Baseline\n",
        "baseline_pipeline.model_selection.add_model(\"Logistic Regression (baseline)\", logisticRegressionModel)\n",
        "baseline_pipeline.model_selection.add_model(\"Majority Class (baseline)\", majorityClassModel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### THIS WILL BE DELETED\n",
        "While we debug, lets exlclude some models we dont need for now (they are very slow to train).\n",
        "\n",
        "#### THIS WONT BE DELETED\n",
        "As indicated throughout the notebook this is the part where we eliminate the models that have been excluded from the final training.\n",
        "\n",
        "| Model | Reason for Exclusion |\n",
        "|-------|----------------------|\n",
        "|Linear SVM | Ran the training for 1h never converged| "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "ensembled_pipeline.model_selection.models_to_exclude = []\n",
        "\n",
        "# Tree-based models\n",
        "tree_pipeline.model_selection.models_to_exclude = []\n",
        "\n",
        "# Support Vector Machines models\n",
        "supportVectorsMachine_pipeline.model_selection.models_to_exclude = [\"Linear SVM\"]\n",
        "\n",
        "# Naive Bayes models\n",
        "naiveBayes_pipeline.model_selection.models_to_exclude = []\n",
        "\n",
        "# Baseline models\n",
        "baseline_pipeline.model_selection.models_to_exclude = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently the stack model has as base predictos all models from all the pipelines. We would now need to update them based on the prior decisions to exclude from the training certain models. This is under the assumption that if a given base model it is excluded from its individual training it is not intended to be triggered an indivual training when the meta model (from the stack model) spots it is untrained.\n",
        "Lets make sure they are deleted!:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "estimators = []\n",
        "\n",
        "for index, estimator in enumerate(stackingModel.estimators):\n",
        "      estimators.append(estimator[0])\n",
        "estimators\n",
        "\n",
        "n_of_estimators = len(stackingModel.estimators)\n",
        "\n",
        "for pipelineName, pipeline in pipeline_manager.pipelines[\"not-baseline\"].items():\n",
        "      for modelToExclude in pipeline.model_selection.models_to_exclude:\n",
        "            if modelToExclude in estimators:\n",
        "                  for index, estimator in enumerate(stackingModel.estimators):\n",
        "                        if estimator[0] == modelToExclude:\n",
        "                              stackingModel.estimators.pop(index)\n",
        "\n",
        "stackingModel.estimators\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets finally start fitting the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipeline_state = \"pre\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"model_selection.fit_models\",\n",
        "                                       verbose=False, \n",
        "                                       exclude_pipeline_names=[\"stacking\"],\n",
        "                                       current_phase=pipeline_manager.pipeline_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make sure the predictions vary between holdout sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<i> the aforeshown diagram was originally done with the sole intention to debug an error that made predictions be the same across sets. Insights may not be much meaningful after correctio, but it is worth keeping until the end of the notebook development </i>\n",
        "\n",
        "### PREDICTIONS RESULTS \n",
        "Before we get into the actual results, lets elaborate briefly on all the metrics that we are using to asses our classifiers:\n",
        "\n",
        "- Accuracy => total correctly predicted elemetnts (sigma over the moments we predicted x_i and it was actually x_i / number_of_samples)\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\sum_{i} \\mathbf{1}(\\hat{y}_i = y_i)}{N}\n",
        "$$\n",
        "- Precision => out of how many predicted for that class were actually from that class (predicted for class x when it was x/ predicted for class x when it was x + predicted for class x when it was NOT x)\n",
        "$$\n",
        "\\text{Precision}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FP}_x}\n",
        "$$\n",
        "- Recall => out of all cases that were positive how many got predicted correctly?\n",
        "$$\n",
        "\\text{Recall}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FN}_x}\n",
        "$$\n",
        "- F1-score => harmonic mean of precision and recall (balances both metrics, heavily penalize spreadness between ratios that are being averaged out)\n",
        "$$\n",
        "\\text{F1}_x = 2 \\times \\frac{\\text{Precision}_x \\times \\text{Recall}_x}{\\text{Precision}_x + \\text{Recall}_x}\n",
        "$$\n",
        "- Support => number of actual occurences of class in the dataset\n",
        "- macro avg => averages given metric across all classes\n",
        "$$\n",
        "\\text{Macro Avg} = \\frac{1}{C} \\sum_{i=1}^{C} M_i\n",
        "$$\n",
        "- weighted avg => averages with weights per class occurence (considers frequency of class in average computation)\n",
        "$$\n",
        "\\text{Weighted Avg} = \\sum_{i=1}^{C} \\frac{\\text{Support}_i}{\\text{Total Instances}} \\times M_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "comments = \"wiLL THIS work?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines[\"baseline\"][\"baselines\"].model_selection.list_of_models[\"Logistic Regression (baseline)\"].tuning_states[\"pre\"].assesment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines[\"baseline\"][\"baselines\"].model_selection.list_of_models[\"Majority Class (baseline)\"].tuning_states[\"pre\"].assesment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines[\"baseline\"][\"baselines\"].model_selection.list_of_models[\"Majority Class (baseline)\"].tuning_states[\"pre\"].assesment[\"model_sklearn\"].get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_results = pipeline_manager.all_pipelines_execute(methodName=\"model_selection.evaluate_and_store_models\", \n",
        "                                                       verbose=False,\n",
        "                                                       exclude_pipeline_names=[\"stacking\"],\n",
        "                                                       comments=comments, \n",
        "                                                       current_phase=pipeline_manager.pipeline_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Evaluation \n",
        "Below are shown all the metrics we can compare our plots to:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Model Evaluation (pre-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis._compute_classification_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_cross_model_comparison(metric=[\"f1-score\", \"recall\", \"precision\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = pipeline_manager.pipelines_analysis.plot_results_df(metrics=[\"timeToFit\", \"timeToPredict\"])\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Summary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_results_summary(training_metric=\"timeToFit\", performance_metric=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intra-model Evaluation (pre-tuning)\n",
        "We plot 3 metrics (f1, recall and precision) for each model constrasting its performance between trainign and validation. It is not subject of interest cross-model evaluation, but rather, intra-model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_intra_model_comparison(metrics=[\"f1-score\", \"recall\", \"precision\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual analysis (pre-tuning)\n",
        "\n",
        "This contains the confusion matrices (weighted and not weighted). \n",
        "It also returns the specific elements that were erroneously classified. The analysis of this residuals may be meaningful for further inspection (clustering analysis, EDA...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals, confusion_matrices = pipeline_manager.pipelines_analysis.plot_confusion_matrix()\n",
        "residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importances (pre-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances_dfs = pipeline_manager.pipelines_analysis.plot_feature_importance()\n",
        "importances_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Optimization\n",
        "1. Bind the grids to each model in each pipeline\n",
        "2. Start all optimizations in parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipeline_state = \"in\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_grid = {\n",
        "      'n_estimators': [50, 100, 150, 200], \n",
        "      'max_depth': [None, 10, 20, 30], \n",
        "      'min_samples_split': [2, 5, 10], \n",
        "      'min_samples_leaf': [1, 2, 4,]\n",
        "}\n",
        "\n",
        "dt_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': [None, 'sqrt', 'log2'],\n",
        "    'ccp_alpha': [0.0, 0.01, 0.1]\n",
        "\n",
        "} \n",
        "\n",
        "gnb_grid = {\n",
        "    'var_smoothing': Real(1e-12, 1e-6, prior='log-uniform')\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelNameToOptimizer = {\n",
        "      \"Random Forest\": {\n",
        "            \"optimizer_type\": \"bayes\",\n",
        "            \"param_grid\": rf_grid,\n",
        "            \"max_iter\": 5\n",
        "      },\n",
        "      \"Decision Tree\": {\n",
        "            \"optimizer_type\": \"bayes\",\n",
        "            \"param_grid\": dt_grid,\n",
        "            \"max_iter\": 5\n",
        "      },\n",
        "      \"Gaussian Naive Bayes\": {\n",
        "        \"optimizer_type\": \"bayes\",\n",
        "        \"param_grid\": gnb_grid,\n",
        "        \"max_iter\": 5\n",
        "      }\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"model_selection.fit_models\", \n",
        "                                       current_phase=pipeline_manager.pipeline_state,\n",
        "                                       modelNameToOptimizer=modelNameToOptimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking Model: Training and Hyperparamter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "stacking_grid = {\n",
        "    'final_estimator__max_depth': [10, 20, 30],\n",
        "    'final_estimator__min_samples_split': [2, 5, 10],\n",
        "    'final_estimator__min_samples_leaf': [1, 2, 4],\n",
        "    'final_estimator__max_features': [None, 'sqrt', 'log2'],\n",
        "    'final_estimator__ccp_alpha': [0.0, 0.01, 0.1],\n",
        "    'stack_method': ['predict', 'predict_proba'],  \n",
        "    'passthrough': [False, True]  \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelNameToOptimizer = {\n",
        "      \"Stacking\": {\n",
        "            \"optimizer_type\": \"bayes\",\n",
        "            \"param_grid\":stacking_grid,\n",
        "            \"max_iter\": 5\n",
        "      }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"model_selection.fit_models\", \n",
        "                                       current_phase=pipeline_manager.pipeline_state,\n",
        "                                       modelNameToOptimizer=modelNameToOptimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-tuning evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_results = pipeline_manager.all_pipelines_execute(methodName=\"model_selection.evaluate_and_store_models\", \n",
        "                                                       exclude_categories=[\"baseline\"],\n",
        "                                                       comments=comments, \n",
        "                                                       current_phase=pipeline_manager.pipeline_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Model Evaluation (in-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis._compute_classification_report().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_cross_model_comparison(metric=[\"f1-score\", \"recall\", \"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_results_df(metrics=[\"timeToFit\", \"timeToPredict\"])\n",
        "# Note I still have not added a way to extract the fitting time from the in-tuning phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Summary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_results_summary(training_metric=\"timeToPredict\", performance_metric=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual analysis (in-tuning)\n",
        "\n",
        "This contains the confusion matrices (weighted and not weighted). \n",
        "It also returns the specific elements that were erroneously classified. The analysis of this residuals may be meaningful for further inspection (clustering analysis, EDA...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals, confusion_matrices = pipeline_manager.pipelines_analysis.plot_confusion_matrix()\n",
        "residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importances (in-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances_dfs = pipeline_manager.pipelines_analysis.plot_feature_importance()\n",
        "importances_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizer Covergance\n",
        "Before we proceed, note that you can access the optimizser object (grid, random or bayes) as per shown in the following example. You may see more detailed information on the tuning process there. For now, we limit ourselves to solely plot the convergence map of the bayes optimizer\n",
        "\n",
        "Questions:\n",
        "- what can further be analyzed from this process in order to get better insights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines[\"not-baseline\"][\"ensembled\"].model_selection.list_of_models[\"Random Forest\"].tuning_states[\"in\"].assesment # just an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"model_selection.plot_convergence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-tuning Evaluation\n",
        "Lets start off by selecting the best perfoming model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model, best_score = pipeline_manager.select_best_performing_model(metric=\"precision\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.fit_final_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.evaluate_store_final_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metric Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipeline_state = \"post\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Model Evaluation (post-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_report_df = pipeline_manager.pipelines_analysis._compute_classification_report(include_training=True)\n",
        "class_report_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_cross_model_comparison(metric=[\"f1-score\", \"recall\", \"precision\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_results_df(metrics=[\"timeToFit\", \"timeToPredict\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Summary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_results_summary(training_metric=\"timeToFit\", performance_metric=\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intra-model Evaluation (post-tuning)\n",
        "We plot 3 metrics (f1, recall and precision) for each model constrasting its performance between trainign and validation. It is not subject of interest cross-model evaluation, but rather, intra-model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_manager.pipelines_analysis.plot_intra_model_comparison(metrics=[\"f1-score\", \"recall\", \"precision\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Residual analysis (post-tuning)\n",
        "\n",
        "This contains the confusion matrices (weighted and not weighted). \n",
        "It also returns the specific elements that were erroneously classified. The analysis of this residuals may be meaningful for further inspection (clustering analysis, EDA...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residuals, confusion_matrices = pipeline_manager.pipelines_analysis.plot_confusion_matrix()\n",
        "residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importances (post-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances_dfs = pipeline_manager.pipelines_analysis.plot_feature_importance()\n",
        "importances_dfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pipeline_manager.store_best_performing_sklearn_model_object(includeModelObject=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pipeline_manager.store_pipelines(includeModelObject=False) # this is going to heavy in size\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "28_glozSPDm7",
        "LDW_CStuioNs",
        "Uv7LWIIU5X5i",
        "eaHsU58AdvlN",
        "vtG27WXGd3_R",
        "vR3xQcHnLUsk",
        "tJI5MrERTBNz",
        "VDydAnW3alRu",
        "Pi1pwj-9q_v5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
