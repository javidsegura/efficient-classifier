{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> DYNAMIC MALWARE CLASSIFICATION</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised, classification, multi-class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CONTENT INDEX\n",
        "\n",
        "1. SET-UP\n",
        "  - Report\n",
        "2. EDA\n",
        "  - Data types\n",
        "  - Descriptive statistics\n",
        "    - Plots, summary statistics \n",
        "  - Relationships between variables \n",
        "  - other...\n",
        "  - Conclusion \n",
        "3. DATA SPLITTING\n",
        "\t- Encoding\n",
        "4. Data preprocessing\n",
        "\t- Feature scaling\n",
        "\t- Duplicate data anlayis\n",
        "  - Missing data analysis\n",
        "  - Outlier detection analysis\n",
        "  - Data imputing\n",
        "  - Class imbalance\n",
        "5. FEATURE ANALYSIS\n",
        "  - Feature engineering\n",
        "  - Feature selection\n",
        "6. MODELLING\n",
        "  - Non-optimized training\n",
        "  - Optimized training\n",
        "  - Conclusions mkdelling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szqv1Y9Edzc5"
      },
      "source": [
        "# 0. Basic Set-Up and General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: there are a lot of non-used imports (to be removed at the end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If u make a change to utilities_function u may need to restart the kernel to use the latest versions. This is too tedious. There must be another way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
        "                              BaggingClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "from boruta import BorutaPy\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "# Import your modules initially\n",
        "import library.dataset\n",
        "import library.modelling.shallow.regressor\n",
        "\n",
        "# Reload the modules in case they were updated\n",
        "importlib.reload(library.dataset)\n",
        "importlib.reload(library.modelling.shallow.regressor)\n",
        "\n",
        "# Now you can import specific classes/functions\n",
        "from library.dataset import Dataset \n",
        "from library.modelling.shallow.classifier import ClassifierAssesment\n",
        "from library.plots import Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation Standarization\n",
        "Follow the following guidelines to make sure our document is consistent and easier to undestand\n",
        "- All constant should be writte in full capitall letters (e.g: MY_CONSTANT). Notebook-level constants should be written in the cell assigned to it (see below)\n",
        "- Every new major section (EDA, Feature engineering) should be written with '# My title'#\n",
        "- Subsequent sections should use '##', '###' or '####' hierarchacically\n",
        "- At the beginning of each major section write a content index with the content included in that section (see 'feature engineering' section as an example)\n",
        "- Please write paragraphs before and after each cell explain what u are about to do and the conclusions, correspondingly. Do not assume they are too obvious.\n",
        "- Avoid excessive ChatGPT-originated comments\n",
        "- Avoid writing more than 20 lines per code cell (exceptions for subroutines, which should be written in utilitied_functions.py)\n",
        "- Ideally, add a \"Questions\" and \"Things to be done\" section in each major sections where u write about futher iterations u want to do (while sharing in with the rest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path =\"./dataset/dynamic_dataset.csv\"\n",
        "results_path = \"results/results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_columns = [\"model\", \"accuracy\", \"precision\", \"recall\", \"F1\", \"comment\", \"features_used\", \"hyperParameterOptimization\", \"hyperParameters\",\"comments\"]\n",
        "columns_to_check_duplicates = [\"model\", \"features_used\", \"hyperParameterOptimization\", \"hyperParameters\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initializing all the libraries' classes\n",
        "dataset = Dataset(file_path)\n",
        "plots = Plots(dataset)\n",
        "modelAssesment = ClassifierAssesment(dataset, results_path, results_columns, columns_to_check_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "F0L2m8yFXhJ9",
        "outputId": "c94d1d43-6998-486d-d0ea-8b643d64efa6"
      },
      "outputs": [],
      "source": [
        "dataset.df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.df.rename(columns={\"reboot\": \"Reboot\"}, inplace=True) # consistency with other features' names\n",
        "dataset.df.drop(columns=[\"Family\", \"Hash\"], inplace=True) # We have decided to use only category as target variable; Hash is temporary while im debugging (it will be deleted in EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TspwPmuXrwg",
        "outputId": "ba126ff5-a264-4606-8b04-9716dc7a0ddf"
      },
      "outputs": [],
      "source": [
        "dataset.get_basic_info(print_info=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook-level constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "ONLY_NUMERICAL_COLUMNS = dataset.df.select_dtypes(include='number')\n",
        "RANDOM_STATE = 99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Description of our features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each of our entries in the dataset we have a feature describing the state of the (operating) system under which the malware was running. Here is an intial description for each of them.\n",
        "Please access this document for a complete problem context description: https://docs.google.com/document/d/1yH9gvnJVSH9GLv9ATQ5JQWA2z8Jy4umxxRfMF-y2fiU/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we conduct an Exploratory Data Analysis (EDA) on the dynamic malware dataset to gain initial insights into the structure, distribution, and quality of the data prior to modeling. The dataset includes behavioral features extracted from Android applications, along with labels indicating their respective malware categories. Understanding the composition of the dataset, such as class imbalance, feature correlations, and the presence of outliers, is crucial to ensure robust preprocessing, informed feature engineering, and  the success of machine learning classifiers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SUGGESTIONS FOR IMPROVEMENTS\n",
        "- Cluster analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Type Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the data types of each column\n",
        "data_types = dataset.df.dtypes\n",
        "\n",
        "# Count the frequency of each data type\n",
        "type_counts = data_types.value_counts()\n",
        "\n",
        "# Plotting the frequency of data types\n",
        "type_counts.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Frequency of Data Types in DataFrame')\n",
        "plt.xlabel('Data Type')\n",
        "plt.ylabel('Frequency')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that most columns are numerical. Lets gets to see which are the variables that are of type object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_onlyCols = dataset.df.select_dtypes(include=[\"object\"]).columns\n",
        "df_onlyCols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28_glozSPDm7"
      },
      "source": [
        "## Histograms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVSB5cgoYRDr",
        "outputId": "6da84feb-1032-4185-b3e6-de73e05279df"
      },
      "outputs": [],
      "source": [
        "# Select only numerical features\n",
        "numerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define number of rows and columns for subplots\n",
        "num_features = len(numerical_cols)\n",
        "cols = 4  # Number of columns per row\n",
        "rows = math.ceil(num_features / cols)  # Calculate required rows\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\n",
        "axes = axes.flatten()  # Flatten to easily iterate\n",
        "\n",
        "# Plot histograms\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])  # kde=True for smooth curve\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(num_features, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see most distributions tend to be right-skewed and only a small portion follows a normal distribution. This right-skewness will be dealt in feature-engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YDO2OqktbPDN",
        "outputId": "59795112-1aaf-4e5d-99fb-999c1b58b662"
      },
      "outputs": [],
      "source": [
        "# Select only numerical features\n",
        "numerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define number of rows and columns for subplots\n",
        "num_features = len(numerical_cols)\n",
        "cols = 4  # Number of columns per row\n",
        "rows = math.ceil(num_features / cols)  # Calculate required rows\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\n",
        "axes = axes.flatten()  # Flatten to easily iterate\n",
        "\n",
        "# Plot boxplots\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.boxplot(x=dataset.df[col], ax=axes[i])  # Boxplot for each feature\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(num_features, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDW_CStuioNs"
      },
      "source": [
        "## Numerical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnC_-AYbitFA"
      },
      "source": [
        "Seemed to be grouped by prefixes: Memory, Network, Battery, Logcat, Process y API.\n",
        "\n",
        "According to dataset authors to capture how various malware families and categories behave at runtime, the analysis relies on six distinct sets of features obtained after executing each sample within a controlled emulated environment. These feature groups offer a comprehensive view of the malware's dynamic activity.\n",
        "\n",
        "This categories appear before the first _ in every feature label and are defined as:\n",
        "\n",
        "\n",
        "\"Memory: Memory features define activities performed by malware by utilizing memory.\n",
        "\n",
        "API: Application Programming Interface (API) features delineate the communication between two applications.\n",
        "\n",
        "Network: Network features describe the data transmitted and received between other devices in the network. It indicates foreground and background network usage.\n",
        "\n",
        "Battery: Battery features describe the access to battery wakelock and services by malware.\n",
        "\n",
        "Logcat: Logcat features write log messages corresponding to a function performed by malware.\n",
        "\n",
        "Process: Process features count the interaction of malware with total number of processes.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raKBvgK5ircs",
        "outputId": "6109fdf7-3a42-4496-a39b-f34bfe204c1b"
      },
      "outputs": [],
      "source": [
        "numeric_cols = dataset.df.select_dtypes(include='number').columns\n",
        "\n",
        "# Grouping based on the first prefix before \"_\"\n",
        "prefix_groups = defaultdict(list)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    prefix = col.split(\"_\")[0]  # Get the first word before the underscore\n",
        "    prefix_groups[prefix].append(col)\n",
        "\n",
        "for prefix, columns in prefix_groups.items():\n",
        "    print(f\"\\n {prefix} ({len(columns)} features):\")\n",
        "    for col in columns:\n",
        "        print(f\"  - {col}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv7LWIIU5X5i"
      },
      "source": [
        "## Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "gA-v2L6x5ImZ",
        "outputId": "e7b19c28-c316-4b5a-eec7-90ee38c6dd0b"
      },
      "outputs": [],
      "source": [
        "#Statistical summary for categorical features\n",
        "dataset.df.describe(include=[\"object\", \"category\", \"bool\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJbNriVFrRYE",
        "outputId": "2935f3d0-a88b-4bd4-8b98-f4c76a641092"
      },
      "outputs": [],
      "source": [
        "print(dataset.df[['Hash', 'Category', 'Family']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWBDyuc4q47"
      },
      "source": [
        "Hash: unique identifier that represents each malware sample. <<<>>>THIS IS PROBABLY WRONG<<<>>>\n",
        "\n",
        "Category: general classification of the malware sample based on its behavior.\n",
        "\n",
        "Family: more fine-grained grouping of malware based on its codebase or origin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC5leffBsW5W"
      },
      "source": [
        "For hash, it will first be checked if the same malware before and after reboot contains the same hash value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ep2R1vwzRyy",
        "outputId": "3048de1d-c0cd-437d-9044-68a3ed1c3fc3"
      },
      "outputs": [],
      "source": [
        "# Count how many times each hash appears in 'before' and 'after'\n",
        "hash_reboot_counts =dataset.df.groupby(['Hash', 'reboot']).size().unstack(fill_value=0)\n",
        "\n",
        "# Hashes in both with exactly one in each\n",
        "hashes_with_one_each = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] == 1) & (hash_reboot_counts['after'] == 1)\n",
        "].index\n",
        "\n",
        "# Hashes in both but with extra rows\n",
        "hashes_in_both_but_not_clean = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] > 0) &\n",
        "    (hash_reboot_counts['after'] > 0) &\n",
        "    ~((hash_reboot_counts['before'] == 1) & (hash_reboot_counts['after'] == 1))\n",
        "].index\n",
        "\n",
        "# Total unique hashes\n",
        "total_unique_hashes = dataset.df['Hash'].nunique()\n",
        "\n",
        "# Hashes in only one reboot condition\n",
        "hashes_in_one_condition = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] == 0) | (hash_reboot_counts['after'] == 0)\n",
        "]\n",
        "\n",
        "# Only once in one reboot condition\n",
        "only_once_in_one = hashes_in_one_condition[\n",
        "    (hashes_in_one_condition['before'] == 1) | (hashes_in_one_condition['after'] == 1)\n",
        "]\n",
        "\n",
        "# More than once in one reboot condition\n",
        "more_than_once_in_one = hashes_in_one_condition[\n",
        "    ((hashes_in_one_condition['before'] > 1) & (hashes_in_one_condition['after'] == 0)) |\n",
        "    ((hashes_in_one_condition['after'] > 1) & (hashes_in_one_condition['before'] == 0))\n",
        "]\n",
        "\n",
        "# Split those into counts\n",
        "more_than_once_in_before = more_than_once_in_one[more_than_once_in_one['before'] > 1]\n",
        "more_than_once_in_after = more_than_once_in_one[more_than_once_in_one['after'] > 1]\n",
        "\n",
        "# --- PRINT RESULTS ---\n",
        "print(f\"Hashes with EXACTLY one row in BOTH before and after: {len(hashes_with_one_each)}\")\n",
        "print(f\"Hashes in BOTH, BUT with extra rows: {len(hashes_in_both_but_not_clean)}\")\n",
        "\n",
        "print(f\"\\nHashes in ONLY ONE reboot condition:\")\n",
        "print(f\"• Appearing ONLY ONCE: {len(only_once_in_one)}\")\n",
        "print(f\"• Appearing MORE THAN ONCE: {len(more_than_once_in_one)}\")\n",
        "print(f\"   - More than once in BEFORE: {len(more_than_once_in_before)}\")\n",
        "print(f\"   - More than once in AFTER: {len(more_than_once_in_after)}\")\n",
        "\n",
        "print(f\"\\nTotal breakdown:\")\n",
        "print(f\"• In BOTH (any): {len(hashes_with_one_each) + len(hashes_in_both_but_not_clean)}\")\n",
        "print(f\"• In ONLY ONE reboot: {len(hashes_in_one_condition)}\")\n",
        "print(f\"• TOTAL unique hashes: {total_unique_hashes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2IGnd2izsOY"
      },
      "source": [
        "A total of 19,169 hashes appear exactly once in both before and after conditions. These are highly reliable for paired  comparisons, ideal for understanding how reboot affects malware behavior.\n",
        "\n",
        "\n",
        "There are 158 hashes that appear in both reboot states but not exactly once in each. These extra instances may come from inconsistencies in data capture like multiple logs for the same sample and should be checked.\n",
        "\n",
        "A significant portion of samples appear only in one reboot condition. This is consistent with limitations described in the original dataset paper, where some malware samples failed to execute after the reboot. However, what is curious is that some still have been logged more than once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "NzJxov9LLOGu",
        "outputId": "a0691e50-f676-4ef2-e150-166b1e96a5ff"
      },
      "outputs": [],
      "source": [
        "dataset.df.drop(columns=['Hash'], inplace=True)\n",
        "'''\n",
        "The Hash column is a high-cardinality feature, containing unique values for a high number of rows in the dataset.\n",
        "It serves as an identifier for each malware sample. Including this column in modeling\n",
        "would not only offer no predictive value but could also lead to overfitting or cause issues with algorithms that are\n",
        "sensitive to high-cardinality categorical features.\n",
        " <<<>>> J.N: may be better to focus the argumentation on ID not being useful rather than high-cardinality per se. Also write the \n",
        "  argumentation in a text cell not in this type of comments. <<<>>>\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZUupRE1vx5"
      },
      "source": [
        "This research will be using both Category and Family as the target variables for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaHsU58AdvlN"
      },
      "source": [
        "## Reboot Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ymeBe4fKdNi",
        "outputId": "ce478b18-ff6a-4720-f19c-e5ecabfbbbbb"
      },
      "outputs": [],
      "source": [
        "print(dataset.df[\"reboot\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV8hkdL2SHVe"
      },
      "source": [
        "The imbalance observed in the dataset, with 28,380 samples collected before reboot and only 25,059 after reboot, is explained by limitations found during the dynamic analysis. The authors of the dataset note that \"there was no entry point in some Android malware samples and some Android malware samples stopped abruptly.\" This means that certain malware applications either failed to launch or terminated unexpectedly during execution, preventing the collection of dynamic behavior data, particularly after the reboot phase.\n",
        "\n",
        "Additionally, the study highlights another critical limitation: \"the dynamic analysis is performed in an emulator. Some malware samples are able to detect the emulated environment and are not executed.\" This behavior reflects common anti-analysis techniques used by sophisticated malware, which can detect when they are running in a sandbox or emulator and intentionally suspend their malicious actions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<<<>>>THIS ANALYSIS IS SUPER GOOD (you can delete this comment)<<<>>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuIT6CKgeqcZ"
      },
      "source": [
        "The displayed features are the top 10  most affected by reboot showing a clear reboot-sensitive behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "i9p8S9woa19A",
        "outputId": "7c14fef2-103d-40a4-85e0-50c138fdb448"
      },
      "outputs": [],
      "source": [
        "#Category distribution across reboot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=dataset.df, x='Category', hue='reboot')\n",
        "plt.title(\"Malware Categories by Reboot Condition\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwaTM_JeoWk"
      },
      "source": [
        "To identify which numeric features are most influenced by the reboot condition, the dataset will be grouped by the reboot variable, separating entries collected before and after the device reboot. Within each group, the mean of every numeric feature will be computed, allowing for the comparison of average behavior across both states.\n",
        "\n",
        "A new column labeled 'diff' was then added, representing the difference between the mean values after and before the reboot for each feature. A positive value indicates that the feature increased after reboot, while a negative value shows it decreased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "jG1RQO7AcIWG",
        "outputId": "00489ed7-d243-4fae-d577-849381abbca2"
      },
      "outputs": [],
      "source": [
        "reboot_means = dataset.df.groupby('reboot').mean(numeric_only=True).T\n",
        "reboot_means['diff'] = reboot_means['after'] - reboot_means['before']\n",
        "reboot_means_sorted = reboot_means.sort_values(by='diff', ascending=False)\n",
        "\n",
        "reboot_means_sorted.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7cm_Z81fWqq"
      },
      "source": [
        "The results reveal that several features show clear shifts after reboot. Specially, network-related features such as Network_TotalReceivedBytes and Network_TotalTransmittedBytes demonstrate significant increases, suggesting that some malware types intensify data transmission once the device has rebooted. Memory features like Memory_SharedClean, Memory_HeapSize, and Memory_HeapAlloc also show increased values after reboot, indicating greater memory use or altered memory management after reboot.\n",
        "This shows that the reboot condition plays an important role in runtime behavior and should be treated as an important factor in exploratory analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#How many categories each family belongs to\n",
        "dataset.df.groupby(\"Family\")[\"Category\"].nunique().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Almost every family is either unknown or unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<<Error: NameError: name 'family_to_category' is not defined>>> (this Irina's code; copied from Argentinan guy's notebook)\n",
        "# multi_cat_families = family_to_category[family_to_category > 1]\n",
        "# print(f\"Number of families mapping to multiple categories: {len(multi_cat_families)}\")\n",
        "# print(multi_cat_families)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is only one Family that maps to multiple categories, and is the placeholder unknown.\n",
        "\n",
        "The following code displays how many samples with unknown family labels belong to each malware category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Count unique families per category\n",
        "family_amount = dataset.df.groupby(\"Category\")[\"Family\"].nunique()\n",
        "\n",
        "# Step 2: Total number of instances per category\n",
        "total_per_category = dataset.df[\"Category\"].value_counts()\n",
        "\n",
        "# Step 3: Count how many of those are <unknown> per category\n",
        "unknown_amount = dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()\n",
        "\n",
        "# Step 4: Combine all stats into a summary table\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Family_amount\": family_amount,\n",
        "    \"Total_category\": total_per_category,\n",
        "    \"Unknown_amount\": unknown_amount\n",
        "}).fillna(0).astype({\"Unknown_amount\": int})\n",
        "\n",
        "# Step 5: Calculate percentage of unknowns per category\n",
        "summary_df[\"%_Unknown\"] = (summary_df[\"Unknown_amount\"] / summary_df[\"Total_category\"] * 100).round(2)\n",
        "\n",
        "# Reorder columns for readability\n",
        "summary_df = summary_df[[\"Family_amount\", \"Total_category\", \"Unknown_amount\", \"%_Unknown\"]]\n",
        "\n",
        "# Display the summary\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unknown_count = (dataset.df[\"Family\"] == \"<unknown>\").sum()\n",
        "print(f\"Number of rows with Family == '<unknown>': {unknown_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the analysis of family distribution across categories:\n",
        "\n",
        "The Adware category stands out with zero instances labeled as <unknown> and a balanced distribution across 43 families. This makes it a strong candidate for modeling.\n",
        "\n",
        "In contrast, Zero_Day and No_Category The categories Zero_Day and No_Category exhibit extremely high family dispersion, with 2576 and 335 unique families. These values are significantly higher than all other categories, which generally have fewer than 50 families each.\n",
        "\n",
        "\n",
        "This suggests they function more as placeholder labels. In particular, Zero_Day likely serves as a catch-all label for unknown or uncategorized threats, making it ambiguous. In cybersecurity, this term is refered to a new unknown vulnerability, not yet classified in terms of malware behavior, this is why samples are varied. They do not seem to represent a consistent type. On the other hand, No_Category explicitly denotes a lack of category. So, including these instances would only bring noise to the training process, preventing the model from learning meaningful patterns.\n",
        "Therefore, they are excluded from the final dataset to preserve the quality and consistency of the classification task.\n",
        "\n",
        "\n",
        "Additionally, categories like FileInfector show a high percentage of <unknown> families (6.85%) despite having a small total count, raising concerns about label quality. Most other categories maintain a relatively stable level of unknowns (around 3–5%), indicating that the presence of <unknown> is manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA SPLITTING\n",
        "### TO BE DONE\n",
        "- Statistical analysis of this\n",
        "- Make sure they follow the same distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting: Category as target variable\n",
        "Originally, we will focus only on category\n",
        "\n",
        "Lets first get the X and y extracted from our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also object!\n",
        "Lets get back to the splitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before, we split the dataset lets observe the SE of accuracy variation based on our choice of split. \n",
        "Brief explanation: we can model accuracy via a Binomial distribution. We know each event in a binomial distribution can be modelled through a bernoulli distribution, where the outcome represents the probability predicting the correct class or not. We make the assumption that each classification error is independent from each other. For:\n",
        "$$\n",
        "\\text{Bin} \\sim (n, p)\n",
        "$$\n",
        " let us assume that the parameter of this distribution is p = .85 and n is given by the choice of sample split for the test set. The SE of the sample proportion can be modeled via: \n",
        "$$\n",
        "\\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\n",
        "\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we continue to assess all possible choices of split based on a variant n, also note that that choice of evenly distributed split (e.g: 10% for each hold-out set) between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n",
        "\n",
        "Finally, also note that that choice of evenly distributed split between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.asses_split_classifier(p=.85, step=.05, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[We can see a diminishing-returns class graph](https://en.wikipedia.org/wiki/Knee_of_a_curve). The more we decline the training set percentage the slower and more steadier the current SE varies as well as the difference to prior SE, in percentage. We can see that the knee in the curve is between 80 and 90 training set percentage. This represents the area where when you start going below the lower bound, no **significant** improve appears. Given the fact that our criteria for choice of split percentage is to keep as much training possible while increasing hold-out sets size only if the decrease in SE is significant **we are going to select 80% for the training set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              " 48143            35115             2936               12944   \n",
              " 10388            71418            11352                8960   \n",
              " 11029           120450            30872               10704   \n",
              " 36609            42669             4156               12544   \n",
              " 26413            65679             3256               10792   \n",
              " ...                ...              ...                 ...   \n",
              " 42697            35571              208               11060   \n",
              " 36008            52445             5196               12452   \n",
              " 46265            37220             3464               12612   \n",
              " 23587           113947            39996               10852   \n",
              " 29313            79501             5476               10600   \n",
              " \n",
              "        Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              " 48143                25560               83552                 3072   \n",
              " 10388                46700               84844                11468   \n",
              " 11029                71532              132144                31660   \n",
              " 36609                29872               91184                 4172   \n",
              " 26413                52684              101276                 6004   \n",
              " ...                    ...                 ...                  ...   \n",
              " 42697                32352               56960                  212   \n",
              " 36008                36008               98696                 7208   \n",
              " 46265                27708               82884                 3488   \n",
              " 23587                60764               99972                41624   \n",
              " 29313                66556               94636                 5580   \n",
              " \n",
              "        Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              " 48143                    0             9086              5558   \n",
              " 10388                    0            15178             11581   \n",
              " 11029                    0            37109             30762   \n",
              " 36609                    0            13283             10668   \n",
              " 26413                    0            32054             24837   \n",
              " ...                    ...              ...               ...   \n",
              " 42697                    0             9027              5661   \n",
              " 36008                    0            17994             15059   \n",
              " 46265                    0            10049              7028   \n",
              " 23587                    0            43932             31028   \n",
              " 29313                    0            27634             20688   \n",
              " \n",
              "        Memory_HeapFree  ...  Logcat_info  Logcat_error  Logcat_warning  \\\n",
              " 48143             3527  ...         4372          2820            1369   \n",
              " 10388             3596  ...          693            99            2863   \n",
              " 11029             6346  ...         2422          2959             925   \n",
              " 36609             2614  ...         2323           149             783   \n",
              " 26413             7216  ...         2271          7564             626   \n",
              " ...                ...  ...          ...           ...             ...   \n",
              " 42697             3365  ...         1801          1512            1660   \n",
              " 36008             2934  ...          113          2628             802   \n",
              " 46265             3020  ...         3582          4443            1225   \n",
              " 23587            12903  ...          926          3027            1510   \n",
              " 29313             6945  ...          820           106            2003   \n",
              " \n",
              "        Logcat_debug  Logcat_verbose  Logcat_total  Process_total  \\\n",
              " 48143          1567            4737         14865            188   \n",
              " 10388          1702            2804          8161            191   \n",
              " 11029            98            3416          9820            196   \n",
              " 36609          1954            2908          8117            192   \n",
              " 26413          1826             139         12426            192   \n",
              " ...             ...             ...           ...            ...   \n",
              " 42697          1308             404          6685            189   \n",
              " 36008          2621            6419         12583            193   \n",
              " 46265          1391            2992         13633            189   \n",
              " 23587          1908            5238         12609            187   \n",
              " 29313          3506            2601          9036            191   \n",
              " \n",
              "                                                     Hash  \\\n",
              " 48143  0e51a78a7396362ceafe1d19efcbb72847e0cfd1c73629...   \n",
              " 10388  5477c05b2e734cd55a93d6e2491982d70f463b41bd14c2...   \n",
              " 11029  fd9b8aabc47fc3c65aac47246f0a83d2e1b4e2e8688542...   \n",
              " 36609  ea54370f86bde293aab822ed8e5414c8f2b78123fa3602...   \n",
              " 26413  0295787a745896055976c08998d34e23495dc0e2ea3a96...   \n",
              " ...                                                  ...   \n",
              " 42697  5fe64c6f0ebe4b1711a5d9ed631942b7a13a6cdaac70f6...   \n",
              " 36008  aa91891d128f29d115f3094f1421d017977d250a3c330a...   \n",
              " 46265  a1a4f031214706368dd7c158f2c2e5e488168a407cc598...   \n",
              " 23587  a2ac45f26dac2236af1cab64540a95ba61f35ae1db505e...   \n",
              " 29313  e248f59c8957b59a0eb12bc83c541ae0189ddc2cd28c34...   \n",
              " \n",
              "                                            Family  reboot  \n",
              " 48143                                      mobtes  before  \n",
              " 10388                                       kuguo   after  \n",
              " 11029                                      adflex   after  \n",
              " 36609                                      smsreg   after  \n",
              " 26413  SINGLETON:84b6b708eab30ea7876f41fedb8c8ee4   after  \n",
              " ...                                           ...     ...  \n",
              " 42697                                      smforw  before  \n",
              " 36008                                      smsreg   after  \n",
              " 46265                                      typstu  before  \n",
              " 23587                                      smspay  before  \n",
              " 29313                                    mytrackp   after  \n",
              " \n",
              " [42751 rows x 144 columns],\n",
              "        Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              " 18288            89260            35468               10708   \n",
              " 40648            48666            12528               10656   \n",
              " 8718             46730             5972               12100   \n",
              " 5961             52168             5372               12236   \n",
              " 32688            92303            25632               10584   \n",
              " ...                ...              ...                 ...   \n",
              " 20920            56561             1940               10620   \n",
              " 33656           114597            13464               11176   \n",
              " 32607           117458            37848               10640   \n",
              " 8923            101636            42696               11792   \n",
              " 38099            51156             5052               11076   \n",
              " \n",
              "        Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              " 18288                43912              101788                37024   \n",
              " 40648                26508               71024                12660   \n",
              " 8718                 32660               93356                 6100   \n",
              " 5961                 33636               89756                 7224   \n",
              " 32688                58220               89048                26916   \n",
              " ...                    ...                 ...                  ...   \n",
              " 20920                44556               95240                 4684   \n",
              " 33656                88344               83848                17952   \n",
              " 32607                62936              112004                44524   \n",
              " 8923                 45016               99032                44444   \n",
              " 38099                37104               89148                 7908   \n",
              " \n",
              "        Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              " 18288                    0            13438             10484   \n",
              " 40648                    0             7571              4444   \n",
              " 8718                     0            21473             17870   \n",
              " 5961                     0            17434             14267   \n",
              " 32688                    0            31904             24735   \n",
              " ...                    ...              ...               ...   \n",
              " 20920                    0            19531             16995   \n",
              " 33656                    0            33963             26513   \n",
              " 32607                    0            43343             37854   \n",
              " 8923                     0            26615             22085   \n",
              " 38099                    0            19344             16648   \n",
              " \n",
              "        Memory_HeapFree  ...  Logcat_info  Logcat_error  Logcat_warning  \\\n",
              " 18288             2953  ...         1260          1890            3822   \n",
              " 40648             3126  ...         3730          2898             113   \n",
              " 8718              3602  ...         1363           465            2332   \n",
              " 5961              3166  ...         4297          1748            3056   \n",
              " 32688             7168  ...         1442          1665             734   \n",
              " ...                ...  ...          ...           ...             ...   \n",
              " 20920             2535  ...          705          1833            2840   \n",
              " 33656             7449  ...         2736          2081            2651   \n",
              " 32607             5488  ...         5119           269             743   \n",
              " 8923              4529  ...         2489           902             237   \n",
              " 38099             2695  ...         3312          1138             414   \n",
              " \n",
              "        Logcat_debug  Logcat_verbose  Logcat_total  Process_total  \\\n",
              " 18288          4762            3532         15266            191   \n",
              " 40648           970            3147         10858            191   \n",
              " 8718            138            1468          5766            190   \n",
              " 5961           2978            1014         13093            191   \n",
              " 32688           110            2522          6473            192   \n",
              " ...             ...             ...           ...            ...   \n",
              " 20920          1516            4116         11010            187   \n",
              " 33656           633            1692          9793            194   \n",
              " 32607          3890            2460         12481            193   \n",
              " 8923           3404            2364          9396            194   \n",
              " 38099          3637            1974         10475            193   \n",
              " \n",
              "                                                     Hash       Family  reboot  \n",
              " 18288  fd5fc552e4b9820755b10a97b205db0793ddf215a1489c...       smsreg  before  \n",
              " 40648  2115e5ba29824e89b345f54277351eabeeb253974039a8...    <unknown>   after  \n",
              " 8718   490dfe01de215088dbf1b5bfcc60baaccabb53dc967d5a...       dowgin   after  \n",
              " 5961   4372bc15636f815cd635596b492b25c9d603c7602622c2...        inoco  before  \n",
              " 32688  050eac22de39c56684deac44ed13cc0fc24c629fcacd74...      autosms   after  \n",
              " ...                                                  ...          ...     ...  \n",
              " 20920  5eda18450dba49eb7c00e78906f9b8717fdf1772b453bf...       smsreg  before  \n",
              " 33656  99b3fd1a090d6e0f5eb0e04451e9d5f28531d726d0d5c9...  goldentouch  before  \n",
              " 32607  3ce813494ba82081229e9da8f42f9c5f9f838834dec639...     styricka   after  \n",
              " 8923   0a4e6bae4e6ffb75f2828bf206f749e547919dd4d92231...       shedun   after  \n",
              " 38099  0d12ea43744834f6e6ee08681876e977c9073dc6b6de0b...       smsreg   after  \n",
              " \n",
              " [5344 rows x 144 columns],\n",
              "        Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              " 28550           142029             9644               10804   \n",
              " 43091            35900                0               11228   \n",
              " 8287            120639            32164               10724   \n",
              " 47969            32216             3552               12936   \n",
              " 29839            80737             2556               11012   \n",
              " ...                ...              ...                 ...   \n",
              " 53065            56160             8456               12012   \n",
              " 49176            40409             1268               11204   \n",
              " 30395            71344             4576               10624   \n",
              " 4491             39970             1692               10688   \n",
              " 2304             66112            26888               10180   \n",
              " \n",
              "        Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              " 28550               123028               67116                10220   \n",
              " 43091                33240               52236                    4   \n",
              " 8287                 79364              102076                33536   \n",
              " 47969                23972               66432                 3564   \n",
              " 29839                64316               90308                10252   \n",
              " ...                    ...                 ...                  ...   \n",
              " 53065                34720               96740                11544   \n",
              " 49176                35692               62172                 1272   \n",
              " 30395                57848              107704                 4616   \n",
              " 4491                 29372               88400                 3964   \n",
              " 2304                 26108               58108                27336   \n",
              " \n",
              "        Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              " 28550                    0            40778             30782   \n",
              " 43091                    0             9026              5604   \n",
              " 8287                     0            37049             29643   \n",
              " 47969                    0             7897              4791   \n",
              " 29839                    0            47900             36743   \n",
              " ...                    ...              ...               ...   \n",
              " 53065                    0            11466              8843   \n",
              " 49176                    0             7921              5985   \n",
              " 30395                    0            24922             19530   \n",
              " 4491                     0            12228              9262   \n",
              " 2304                     0             8796              6031   \n",
              " \n",
              "        Memory_HeapFree  ...  Logcat_info  Logcat_error  Logcat_warning  \\\n",
              " 28550             9995  ...         1854          1162            1512   \n",
              " 43091             3421  ...          753          1673            2602   \n",
              " 8287              7405  ...          115          2370            3863   \n",
              " 47969             3105  ...         2657          1561            4380   \n",
              " 29839            11156  ...          626          2142            2488   \n",
              " ...                ...  ...          ...           ...             ...   \n",
              " 53065             2622  ...         2503          1609           10482   \n",
              " 49176             1935  ...         1529           937            3319   \n",
              " 30395             5391  ...         3398          1916             734   \n",
              " 4491              2965  ...         1560          2403             932   \n",
              " 2304              2764  ...         1070         10940            1514   \n",
              " \n",
              "        Logcat_debug  Logcat_verbose  Logcat_total  Process_total  \\\n",
              " 28550          1975            2807          9310            189   \n",
              " 43091          1541            3550         10119            191   \n",
              " 8287           1387            1691          9426            193   \n",
              " 47969          3265            1415         13278            188   \n",
              " 29839           195            4399          9850            193   \n",
              " ...             ...             ...           ...            ...   \n",
              " 53065           956            3464         19014            188   \n",
              " 49176          1538            2902         10225            189   \n",
              " 30395           106            2481          8635            192   \n",
              " 4491           2394            2423          9712            186   \n",
              " 2304           3870            3745         21139            188   \n",
              " \n",
              "                                                     Hash  \\\n",
              " 28550  61cc19382ddf0c8549a98f692e9914cfa5e5fce560b18d...   \n",
              " 43091  12be378ad13b761a2f19c519b80636491f48d5530af713...   \n",
              " 8287   42bcd343198064b452030f17de53064217ac120f5ac548...   \n",
              " 47969  dcdc8dae00926edb98114c5f6231dd1f032f32534bb078...   \n",
              " 29839  c5b5b800f7f739a71b56971520dcc9fc65bae2d5114603...   \n",
              " ...                                                  ...   \n",
              " 53065  bec6314f67c62fc2e9b5ab7540c5c8b09ecf9910208d17...   \n",
              " 49176  dfd079088de7b38f23e681eee881e76a9dc2b00bdf956c...   \n",
              " 30395  25ba1fae8e6af35c790cf85614679fb6e85a00862a419a...   \n",
              " 4491   613e726a0f14ab5af1133cb2a52c2eba9b87c7b5ebd378...   \n",
              " 2304   75758b85c54c9c93b04df0954de7ca59d6c4bddaf18d1c...   \n",
              " \n",
              "                                            Family  reboot  \n",
              " 28550  SINGLETON:b6b1c343b8276f1f788ee92dc611d7be  before  \n",
              " 43091                                      smforw  before  \n",
              " 8287                                       inmobi   after  \n",
              " 47969                                    iconosys  before  \n",
              " 29839                                      wkload   after  \n",
              " ...                                           ...     ...  \n",
              " 53065                                       youmi  before  \n",
              " 49176                                      smsspy  before  \n",
              " 30395                                    mytrackp   after  \n",
              " 4491                                        frupi  before  \n",
              " 2304                                       shedun  before  \n",
              " \n",
              " [5344 rows x 144 columns],\n",
              " 48143        Trojan\n",
              " 10388        Adware\n",
              " 11029        Adware\n",
              " 36609      Riskware\n",
              " 26413      Zero_Day\n",
              "             ...    \n",
              " 42697    Trojan_Spy\n",
              " 36008      Riskware\n",
              " 46265        Trojan\n",
              " 23587      Riskware\n",
              " 29313        Trojan\n",
              " Name: Category, Length: 42751, dtype: object,\n",
              " 18288       Riskware\n",
              " 40648       Riskware\n",
              " 8718          Adware\n",
              " 5961          Adware\n",
              " 32688         Trojan\n",
              "             ...     \n",
              " 20920       Riskware\n",
              " 33656    No_Category\n",
              " 32607         Trojan\n",
              " 8923          Adware\n",
              " 38099       Riskware\n",
              " Name: Category, Length: 5344, dtype: object,\n",
              " 28550      Zero_Day\n",
              " 43091    Trojan_Spy\n",
              " 8287         Adware\n",
              " 47969        Trojan\n",
              " 29839        Trojan\n",
              "             ...    \n",
              " 53065           PUA\n",
              " 49176    Ransomware\n",
              " 30395        Trojan\n",
              " 4491         Adware\n",
              " 2304         Adware\n",
              " Name: Category, Length: 5344, dtype: object)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.split_data(y_column=\"Category\", train_size=.8, validation_size=.1, test_size=.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.X_train.shape, dataset.X_validation.shape, dataset.X_test.shape, dataset.y_train.shape, dataset.y_validation.shape, dataset.y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FEATURE ENCODING\n",
        "\n",
        "#### Encoding the X matrices (one-hot encoder)\n",
        "Lets fit the X encoder for the object column (reboot)\n",
        "#### Encoding y column vector (labeller encoding) \n",
        "We know our models needs the target variable to be numerical. Lets transform this series object then!\n",
        "We will fit the encoder in the training set, and extend it to the other sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.get_dataset_encoded(encode_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the results of the encoding..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.X_train_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.X_val_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.X_test_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.y_train_encoded,  \"-\"*30, dataset.y_val_encoded,  \"-\"*30,  dataset.y_test_encoded,  \"-\"*30, dataset.encoding_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling\n",
        "We would usually use normalization when we're dealing with some sort of Gaussian Distribution and, on the other hand, would use standarization if we're dealing with some kind of normal distribution. We're going to figure out the distributions of the fields to determine which of the 2 is more appropriate.\n",
        "\n",
        "First, we're going to test for scale. If we see a lot of difference between scales, we'll go for normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FEATURE ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will take adjust the features that compose the learning inputs to our model. The correctness of this section is pivotal for proper learning by the model\n",
        "\n",
        "INDEX OF THIS SECTION:\n",
        "- Feature engineering\n",
        "- Feature selection\n",
        "\n",
        "QUESTIONS FOR THIS SUBECTION\n",
        "\n",
        "\n",
        "## FEATURE ENGINEERING\n",
        "- Domain-specific features\n",
        "- Binning\n",
        "- Interaction terms\n",
        "\n",
        "NOTE: This requires further understanding of the concepts around our problem's context. This will be deferred for a second iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FEATURE SELECTION\n",
        "- Analyze correlation and low-variances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1) Eliminating low-variances features\n",
        "We will start off feature selection by analyzing the variables that have low variances. Features with low variances provide little new information for the model to learn from, thus they could introduce statistical noise. Due to this reason, they should be elimanted from the dataset. The reason why we don't focus on high-variance is because this symbolizes outliers, which have been dealt with before in data preporcesing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start off this analysis eliminating univariate features (i.e: the featuers with constant values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_number_of_features = dataset.df.shape[1]\n",
        "zero_variance_features = ONLY_NUMERICAL_COLUMNS.std() == 0\n",
        "if zero_variance_features.any():\n",
        "    print(\"Zero-variance features found:\")\n",
        "    print(zero_variance_features[zero_variance_features].index)\n",
        "    dataset.df.drop(columns=zero_variance_features[zero_variance_features].index, inplace=True)\n",
        "    print(f\"Removed {original_number_of_features - dataset.df.shape[1]} features with zero variance\")\n",
        "    ONLY_NUMERICAL_COLUMNS = dataset.df.select_dtypes(include='number') #updating global variable\n",
        "else:\n",
        "    print(\"No zero-variance features found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets focus on features with too few variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "standard_deviations = ONLY_NUMERICAL_COLUMNS.std()\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.hist(standard_deviations, bins=30, edgecolor='black')\n",
        "plt.title('Distribution of Standard Deviations for Numeric Features')\n",
        "plt.xlabel('Standard Deviation')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As explained in scikit-learn's in (1.13.1 Feature selection)[https://scikit-learn.org/stable/modules/feature_selection.html], the variance threshold must be selected carefully. Too low may delete few variables, and too may be too restrictive, deleting more variables than it should."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "STD_THRESHOLD = .4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_to_keep = standard_deviations[(standard_deviations < STD_THRESHOLD)].index\n",
        "print(f\"Removed {len(standard_deviations) - len(cols_to_keep)} features with low variance\")\n",
        "\n",
        "df = dataset.df[cols_to_keep]\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1pwj-9q_v5"
      },
      "source": [
        "#### 1.2) Eliminating highly correlated feature\n",
        "Highly correlated variables (multicolinearity) are problem for models because they introduce a redundancy (features that contain significantly related similar information are not bringing much new insight into the model's input) to the model that can introduce significant variance. This is due to the fact that small changes in the data may make the coefficeints of the highly correlated variables **swing** more than it should"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plots.plot_correlation_matrix(size=\"l\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A note on the shape of this heatmap: due to the high amount of features, and the redundancy to measure the correlation between features (where corr(A,B) = corr(B,A)) we set 'np.triu(np.ones_like(corr, dtype=bool))' in the utilities functions in order to show only new non-redudant correlations between features, thus the right triangle shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, thats a lot to digest! We can see some solid red (high positive correlation) and medium-solid blue (some high negative correlation). \n",
        "Lets use a non-visual methodology to confirm our initial hypothesis. We will use variance inflaction factor (VIF) along with checking manually.\n",
        "A brief explanation on VIF. \n",
        "Formula: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "VIF_i = \\frac{1}{1 - R_i^2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You regress (i.e: do linear regression) on the ith feature as target variable, and all other features as predictors. You then compute the coefficient of determination as a way to measure how well the predictors fit the target variable. VIF values ranges from [1, inf], where lower bound signifies little multicolinearity (R^2 = 0), and upper bound occurs when R^2=1 (perfect multicolineairty). 5 is considered a standard threshold for VIF as it symbolized an 80% R^2.\n",
        "Once you ve obtained the results of VIF, you need to delete the variable with the highest VIF, and recompute VIF until there is no multicolinearity. For example, say there are three features that are 4 features, 3 of them being linear combinations of each other. You would delete the variables with VIF until there is no VIF (when only one of those linear combinations remains). You cant delete all n-1 variables where n are the amount of variables with exceeding (with respect to the chosen threshold), because you may be deleting one that has high VIF due to another feature.\n",
        "\n",
        "A relevant note on why one-hot-encoding must be done dropping the first one:\n",
        "If we were to not remove one of the labels of one hot encoding, you would be able to predict which level of the categorical variable based on all other ones (there is only degree of freedom for categories levels; they are a linear combination). You would essentially see inf VIF in that area and delete it in this section. \n",
        "The VIF has to be computed every time we delete a feature due to high multicolinearity. Lets do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_features = ONLY_NUMERICAL_COLUMNS.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ONLY_NUMERICAL_COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_vif(df):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Feature\"] = df.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n",
        "    return vif_data\n",
        "\n",
        "# Iteratively remove features with high VIF (threshold = 10)\n",
        "def remove_high_vif_features(df, threshold=8):\n",
        "    number_of_iterations = 0\n",
        "    while True:\n",
        "        number_of_iterations += 1\n",
        "        vif_data = calculate_vif(df)\n",
        "        print(f\"VIF computed for iteration {number_of_iterations}:\")\n",
        "        max_vif = vif_data[\"VIF\"].max()\n",
        "        if max_vif < threshold:\n",
        "            break\n",
        "        feature_to_drop = vif_data.loc[vif_data[\"VIF\"].idxmax(), \"Feature\"]\n",
        "        df.drop(columns=[feature_to_drop], inplace=True)\n",
        "        print(f\"Dropped: {feature_to_drop}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note, running this cell will take a while\n",
        "remove_high_vif_features(ONLY_NUMERICAL_COLUMNS, 10)\n",
        "ONLY_NUMERICAL_COLUMNS = df.select_dtypes(include='number')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ONLY_NUMERICAL_COLUMNS = df.select_dtypes(include='number')\n",
        "print(f\"Removed {df.shape[1] - number_of_features} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets compute the correlation matrix one more time post-deletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plots.plot_correlation_matrix(size=\"l\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Automatic Feature Selection\n",
        "#### L1 regularization\n",
        "Because of L1 regularization being able to set weights 0, we can briefly train our logistic regression model with such regulartization and see which features it uses. The reason why it sets 0 to insignificant feature is because the objective function is not only the MSE but added a component of the wieght magnitude (which is trying to minimize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1)\n",
        "ModelAssesment.automatic_feature_selection_l1(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BORUTA\n",
        "Brief explanation: (to be done)\n",
        "Sources: (Tree-based feature selection)[https://scikit-learn.org/stable/modules/feature_selection.html#:~:text=1.13.4.2.%20Tree%2Dbased%20feature%20selection,-%23]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,    \n",
        "    n_jobs=-1, # multithreading\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "boruta_selector = BorutaPy(\n",
        "    rf,\n",
        "    n_estimators='auto',\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictivePowerFeatures, excludedFeatures = ModelAssesment.automatic_feature_selection_boruta(boruta_selector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.eliminate_features_from_all_sets(excludedFeatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boruta said that only 61 variables were important, if that were the case the current dimensions for each set would have that nubmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.X_train_encoded.shape, dataset.X_val_encoded.shape, dataset.X_test_encoded.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome, lets move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- QUESTION ON THIS SECTION\n",
        "   - How can statistical measurement help on the split?\n",
        "- To be done\n",
        "   -  as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QUESTIONS FOR THIS SECTION\n",
        "- Can the ROC curve be used for multiclass?\n",
        "- Can a unsupervised learning algorithm (e.g: KNN) be used for this problem even tough its nature is to be supervised?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TO BE DONE FOR THIS SECTION\n",
        "- Multiple models as classifiers\n",
        "- Learn more about each alogrithms' paremters\n",
        "- Can KNN be used here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest & Decision Trees\n",
        "Instead of the originally planned logistic regression, we will be using an ensembled model first: random forest (collection of week decision trees). This model is not only likely to outperform the original choice because its nature to handle multiclass better, but also does this several orders of magnitude faster. We will add its not ensembled version too, along with gradient boosted machine\n",
        "Note we are not using not-by-default multiclass classifiers (e.g: logistic regressions, svms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-optimized fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "randomForestModel = RandomForestClassifier(random_state=RANDOM_STATE, \n",
        "                             n_estimators=100,\n",
        "                             max_depth=None,\n",
        "                             min_samples_split=2, \n",
        "                             min_samples_leaf=1)\n",
        "\n",
        "decisionTreeModel = DecisionTreeClassifier(\n",
        "                                          random_state=RANDOM_STATE)\n",
        "\n",
        "supportVectorModel = SVC(random_state=RANDOM_STATE,\n",
        "             kernel='rbf',\n",
        "             decision_function_shape='ovr',\n",
        "             class_weight='balanced')\n",
        "\n",
        "logisticRegressionModel = LogisticRegression(penalty='l2', \n",
        "                                             solver='sag',\n",
        "                                             max_iter=1000,\n",
        "                                             class_weight='balanced')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MODEL PERFORMANCE, HYPOTHESIS\n",
        "- Trees (e.g: random forest, decision trees):\n",
        "  - Time to fit:\n",
        "    - Ensemble models (random forest) take more time to train due to the fact that they are larger and heavier than their non-ensembled version. \n",
        "  - Correctness:\n",
        "     - High\n",
        "- Binary-classifiers by default models (e.g: SMV, logistic regression)\n",
        "  - Time to fit:\n",
        "    - compute C models for all C number of classes. Each trained to detect a single class, then when we make predictions, we select the ones that has the highest probability in its predictions (\"the most confident in its prediction\"). This strategy is called One-vs-Rest, note however, a single logistic regression may be used if we used the softmax objective function (instead of log-odds) (it still heavy computationally, tough). They will be sloder than ensemble models\n",
        "  - Correctness:\n",
        "     - Very low if the problem is non-linear which it is for the SVM and logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelAssesment.add_model(\"Random Forest\", randomForestModel)\n",
        "modelAssesment.add_model(\"Decision Tree\", decisionTreeModel)\n",
        "modelAssesment.add_model(\"SVM\", supportVectorModel)\n",
        "modelAssesment.add_model(\"Logistic Regression\", logisticRegressionModel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets fit the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelAssesment.fit_models(print_results=True, modelsToExclude=[\"Logistic Regression\", \"SVM\"]) # exlcluding these models cause they are too slow to fit locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make sure the predictions vary between holdout sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelAssesment.constrast_sets_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<i> the aforeshown diagram was originally done with the sole intention to debug an error that made predictions be the same across sets. Insights may not be much meaningful after correctio, but it is worth keeping until the end of the notebook development </i>\n",
        "\n",
        "### PREDICTIONS RESULTS \n",
        "Before we get into the actual results, lets elaborate briefly on all the metrics that we are using to asses our classifiers:\n",
        "\n",
        "- Accuracy => total correctly predicted elemetnts (sigma over the moments we predicted x_i and it was actually x_i / number_of_samples)\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\sum_{i} \\mathbf{1}(\\hat{y}_i = y_i)}{N}\n",
        "$$\n",
        "- Precision => out of how many predicted for that class were actually from that class (predicted for class x when it was x/ predicted for class x when it was x + predicted for class x when it was NOT x)\n",
        "$$\n",
        "\\text{Precision}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FP}_x}\n",
        "$$\n",
        "- Recall => out of all cases that were positive how many got predicted correctly?\n",
        "$$\n",
        "\\text{Recall}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FN}_x}\n",
        "$$\n",
        "- F1-score => harmonic mean of precision and recall (balances both metrics, heavily penalize spreadness between ratios that are being averaged out)\n",
        "$$\n",
        "\\text{F1}_x = 2 \\times \\frac{\\text{Precision}_x \\times \\text{Recall}_x}{\\text{Precision}_x + \\text{Recall}_x}\n",
        "$$\n",
        "- Support => number of actual occurences of class in the dataset\n",
        "- macro avg => averages given metric across all classes\n",
        "$$\n",
        "\\text{Macro Avg} = \\frac{1}{C} \\sum_{i=1}^{C} M_i\n",
        "$$\n",
        "- weighted avg => averages with weights per class occurence (considers frequency of class in average computation)\n",
        "$$\n",
        "\\text{Weighted Avg} = \\sum_{i=1}^{C} \\frac{\\text{Support}_i}{\\text{Total Instances}} \\times M_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelAssesment.evaluate_classifier(plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelAssesment.models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note 1: the following cells took 11mins to run in an M2\n",
        "Note 2: grid-search is not a good choice for optimization (enough for an early iteration, tough). We need to get smarter using more advanced techniques (e.g: OPTUNA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=RANDOM_STATE), param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
        "grid_search.fit(dataset.X_category_train_encoded, dataset.y_category_train_encoded)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model.fit(dataset.X_category_train_encoded, dataset.y_category_train_encoded)\n",
        "y_val_pred = best_model.predict(dataset.X_category_val_encoded)\n",
        "y_test_pred = best_model.predict(dataset.X_category_test_encoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NOTEBOOK, AREAS FOR IMPROVEMENTS\n",
        "- Introducing as many ML algorithms for non-predictive tasks: \n",
        "  - Data imputing with KNN \n",
        "  - Cluster analysis to better understand the features \n",
        "  - Doing PCA for features (after correlation analysis) \n",
        "\n",
        "- Distilled models\n",
        "- DevOPs pipeline design\n",
        "- Deeper statistical approach to data preprocessing and EDA\n",
        "- Using CCE and BCE (deep learning multi-class metrics) used for shallow learning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "28_glozSPDm7",
        "LDW_CStuioNs",
        "Uv7LWIIU5X5i",
        "eaHsU58AdvlN",
        "vtG27WXGd3_R",
        "vR3xQcHnLUsk",
        "tJI5MrERTBNz",
        "VDydAnW3alRu",
        "Pi1pwj-9q_v5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
