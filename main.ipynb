{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> DYNAMIC MALWARE CLASSIFICATION</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised, classification, multi-class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szqv1Y9Edzc5"
      },
      "source": [
        "# 0. Basic Set-Up and General info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: there are a lot of non-used imports (to be removed at the end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "from boruta import BorutaPy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "\n",
        "# Import your modules initially\n",
        "import library.pipeline\n",
        "\n",
        "# Reload the modules in case they were updated\n",
        "importlib.reload(library.pipeline)\n",
        "\n",
        "# Now you can import specific classes/functions\n",
        "from library.pipeline.pipeline import Pipeline\n",
        "from library.pipeline.pipeline_manager import PipelineManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation Standarization\n",
        "Follow the following guidelines to make sure our document is consistent and easier to undestand\n",
        "- All constant should be writte in full capitall letters (e.g: MY_CONSTANT). Notebook-level constants should be written in the cell assigned to it (see below)\n",
        "- Every new major section (EDA, Feature engineering) should be written with '# My title'#\n",
        "- Subsequent sections should use '##', '###' or '####' hierarchacically\n",
        "- At the beginning of each major section write a content index with the content included in that section (see 'feature engineering' section as an example)\n",
        "- Please write paragraphs before and after each cell explain what u are about to do and the conclusions, correspondingly. Do not assume they are too obvious.\n",
        "- Avoid excessive ChatGPT-originated comments\n",
        "- Avoid writing more than 20 lines per code cell (exceptions for subroutines, which should be written in utilitied_functions.py)\n",
        "- Ideally, add a \"Questions\" and \"Things to be done\" section in each major sections where u write about futher iterations u want to do (while sharing in with the rest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_path =\"./dataset/dynamic_dataset.csv\"\n",
        "results_path = \"results/results.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_to_evaluate = [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start off with a pipeline for all the models. Then at some moment, whenever we think the pipelines will diverge, we remove the by-object-refrence (that we will create in the next cell), and create a new copy with our modifications ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline = ensembled_pipeline = tree_pipeline = linear_pipeline = baseline_pipeline = example = Pipeline(\n",
        "                        dataset_path=dataset_path, \n",
        "                        results_path=results_path,\n",
        "                        model_type=\"classification\",\n",
        "                        metrics_to_evaluate=metrics_to_evaluate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipelines = {\n",
        "            \"models\": {\n",
        "                  \"ensembled\": ensembled_pipeline,\n",
        "                  \"tree-based\": tree_pipeline,\n",
        "                  \"linear\": linear_pipeline,\n",
        "                  }, \n",
        "            \"baseline\": {\n",
        "                  \"logistic\": baseline_pipeline, \n",
        "                  \"example\":  example               \n",
        "            }\n",
        "}\n",
        "pipeline_manager = PipelineManager(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I show an example of how we will be working with by-object-reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline.dataset.example_attribute = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1'"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.example_attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, if we do a (deep) copy:\n",
        "Note that the difference between a deep and a shallow copy is that the former copies all the objects that are inside the class. We need that.\n",
        "We will overwrite the prior attribute and show is this is not propagated to the other results anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_pipeline = copy.deepcopy(default_pipeline)\n",
        "new_pipeline.dataset.example_attribute = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.example_attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It may also be of interest for you to propagate the same method's invokation to all the pipelines within the pipeline manager. For instance, after a divergence in, lets say, feature scaling, you may need to fit all the models in the corresponding pipelines. You would want to avoid reduntandly calling the same method with the same parameters for all the Pipeline class instances. In order to do so, we present an example that avoids such annoyance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline example in category baseline has diverged\n",
            " Pipeline schema is now: {'models': {'ensembled': <library.pipeline.pipeline.Pipeline object at 0x1311aa510>, 'tree-based': <library.pipeline.pipeline.Pipeline object at 0x1311aa510>, 'linear': <library.pipeline.pipeline.Pipeline object at 0x1311aa510>}, 'baseline': {'logistic': <library.pipeline.pipeline.Pipeline object at 0x1311aa510>, 'example': <library.pipeline.pipeline.Pipeline object at 0x13127e950>}}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<library.pipeline.pipeline.Pipeline at 0x13127e950>"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.create_pipeline_divergence(category=\"baseline\", pipelineName=\"example\", print_results=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whats goooddd from 5119671376. You are at automatic feature selection!\n"
          ]
        }
      ],
      "source": [
        "pipeline_manager.pipelines[\"baseline\"][\"example\"].feature_analysis.feature_selection.automatic_feature_selection.speak(\"Whats goooddd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, world! from 5118797072\n",
            "Hello, world! from 5119666512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'models': {'ensembled': None}, 'baseline': {'example': None}}"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"speak\", message=\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A method present deeper down the class:\n",
        "\n",
        "Instead of doing:\n",
        "'pipeline_manager.pipelines[\"baseline\"][\"logistic\"].feature_analysis.feature_selection.automatic_feature_selection.speak(\"Whats goooddd\") for all objects'\n",
        "\n",
        "we can do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whats good from 5118182736. You are at automatic feature selection!Whats good from 5119671376. You are at automatic feature selection!\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'models': {'ensembled': None}, 'baseline': {'example': None}}"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"feature_analysis.feature_selection.automatic_feature_selection.speak\", message=\"Whats good\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lets delete the example obj\n",
        "del new_pipeline \n",
        "del pipeline_manager.pipelines[\"baseline\"][\"example\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'models': {'ensembled': <library.pipeline.pipeline.Pipeline at 0x1311aa510>,\n",
              "  'tree-based': <library.pipeline.pipeline.Pipeline at 0x1311aa510>,\n",
              "  'linear': <library.pipeline.pipeline.Pipeline at 0x1311aa510>},\n",
              " 'baseline': {'logistic': <library.pipeline.pipeline.Pipeline at 0x1311aa510>}}"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start Of The Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "F0L2m8yFXhJ9",
        "outputId": "c94d1d43-6998-486d-d0ea-8b643d64efa6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "      <th>Hash</th>\n",
              "      <th>Category</th>\n",
              "      <th>Family</th>\n",
              "      <th>reboot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31053</td>\n",
              "      <td>2448</td>\n",
              "      <td>14044</td>\n",
              "      <td>23472</td>\n",
              "      <td>74824</td>\n",
              "      <td>2452</td>\n",
              "      <td>0</td>\n",
              "      <td>8919</td>\n",
              "      <td>4786</td>\n",
              "      <td>4132</td>\n",
              "      <td>...</td>\n",
              "      <td>1635</td>\n",
              "      <td>2351</td>\n",
              "      <td>3285</td>\n",
              "      <td>1551</td>\n",
              "      <td>11221</td>\n",
              "      <td>193</td>\n",
              "      <td>f460abb8f2e4e3fb689966ddaea6d6babbd1738bb691c7...</td>\n",
              "      <td>Trojan_SMS</td>\n",
              "      <td>opfake</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>107787</td>\n",
              "      <td>21976</td>\n",
              "      <td>11852</td>\n",
              "      <td>74548</td>\n",
              "      <td>69052</td>\n",
              "      <td>23152</td>\n",
              "      <td>0</td>\n",
              "      <td>25341</td>\n",
              "      <td>20965</td>\n",
              "      <td>4375</td>\n",
              "      <td>...</td>\n",
              "      <td>1816</td>\n",
              "      <td>826</td>\n",
              "      <td>1544</td>\n",
              "      <td>2045</td>\n",
              "      <td>8457</td>\n",
              "      <td>189</td>\n",
              "      <td>556c238536d837007e647543eaf3ea95ae9aaf1c1a52d0...</td>\n",
              "      <td>Trojan_SMS</td>\n",
              "      <td>opfake</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86584</td>\n",
              "      <td>18460</td>\n",
              "      <td>12284</td>\n",
              "      <td>59992</td>\n",
              "      <td>91548</td>\n",
              "      <td>19376</td>\n",
              "      <td>0</td>\n",
              "      <td>24500</td>\n",
              "      <td>21378</td>\n",
              "      <td>3121</td>\n",
              "      <td>...</td>\n",
              "      <td>2244</td>\n",
              "      <td>3406</td>\n",
              "      <td>1565</td>\n",
              "      <td>2819</td>\n",
              "      <td>10780</td>\n",
              "      <td>195</td>\n",
              "      <td>398322f94b5bfa2a9e7b3756a4cf409764595003280c48...</td>\n",
              "      <td>Trojan_SMS</td>\n",
              "      <td>fakeinst</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41248</td>\n",
              "      <td>924</td>\n",
              "      <td>10328</td>\n",
              "      <td>36280</td>\n",
              "      <td>55768</td>\n",
              "      <td>928</td>\n",
              "      <td>0</td>\n",
              "      <td>10082</td>\n",
              "      <td>7281</td>\n",
              "      <td>2800</td>\n",
              "      <td>...</td>\n",
              "      <td>974</td>\n",
              "      <td>4134</td>\n",
              "      <td>3138</td>\n",
              "      <td>1556</td>\n",
              "      <td>11739</td>\n",
              "      <td>191</td>\n",
              "      <td>4a9c14872b2c66165599a969a1a8654bb6887d7a18ab6d...</td>\n",
              "      <td>Trojan_SMS</td>\n",
              "      <td>fakeinst</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38621</td>\n",
              "      <td>5080</td>\n",
              "      <td>12392</td>\n",
              "      <td>27388</td>\n",
              "      <td>71048</td>\n",
              "      <td>5088</td>\n",
              "      <td>0</td>\n",
              "      <td>9077</td>\n",
              "      <td>5750</td>\n",
              "      <td>3326</td>\n",
              "      <td>...</td>\n",
              "      <td>936</td>\n",
              "      <td>2298</td>\n",
              "      <td>3752</td>\n",
              "      <td>1992</td>\n",
              "      <td>10488</td>\n",
              "      <td>188</td>\n",
              "      <td>6b37b9b9c170727f706b69731e64da4bbca2638b4237a7...</td>\n",
              "      <td>Trojan_SMS</td>\n",
              "      <td>fakeinst</td>\n",
              "      <td>before</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 145 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  Memory_PrivateDirty  \\\n",
              "0            31053             2448               14044                23472   \n",
              "1           107787            21976               11852                74548   \n",
              "2            86584            18460               12284                59992   \n",
              "3            41248              924               10328                36280   \n",
              "4            38621             5080               12392                27388   \n",
              "\n",
              "   Memory_SharedClean  Memory_PrivateClean  Memory_SwapPssDirty  \\\n",
              "0               74824                 2452                    0   \n",
              "1               69052                23152                    0   \n",
              "2               91548                19376                    0   \n",
              "3               55768                  928                    0   \n",
              "4               71048                 5088                    0   \n",
              "\n",
              "   Memory_HeapSize  Memory_HeapAlloc  Memory_HeapFree  ...  Logcat_error  \\\n",
              "0             8919              4786             4132  ...          1635   \n",
              "1            25341             20965             4375  ...          1816   \n",
              "2            24500             21378             3121  ...          2244   \n",
              "3            10082              7281             2800  ...           974   \n",
              "4             9077              5750             3326  ...           936   \n",
              "\n",
              "   Logcat_warning  Logcat_debug  Logcat_verbose  Logcat_total  Process_total  \\\n",
              "0            2351          3285            1551         11221            193   \n",
              "1             826          1544            2045          8457            189   \n",
              "2            3406          1565            2819         10780            195   \n",
              "3            4134          3138            1556         11739            191   \n",
              "4            2298          3752            1992         10488            188   \n",
              "\n",
              "                                                Hash    Category    Family  \\\n",
              "0  f460abb8f2e4e3fb689966ddaea6d6babbd1738bb691c7...  Trojan_SMS    opfake   \n",
              "1  556c238536d837007e647543eaf3ea95ae9aaf1c1a52d0...  Trojan_SMS    opfake   \n",
              "2  398322f94b5bfa2a9e7b3756a4cf409764595003280c48...  Trojan_SMS  fakeinst   \n",
              "3  4a9c14872b2c66165599a969a1a8654bb6887d7a18ab6d...  Trojan_SMS  fakeinst   \n",
              "4  6b37b9b9c170727f706b69731e64da4bbca2638b4237a7...  Trojan_SMS  fakeinst   \n",
              "\n",
              "   reboot  \n",
              "0  before  \n",
              "1  before  \n",
              "2  before  \n",
              "3  before  \n",
              "4  before  \n",
              "\n",
              "[5 rows x 145 columns]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.df.rename(columns={\"reboot\": \"Reboot\"}, inplace=True) # consistency with other features' names\n",
        "default_pipeline.dataset.df.drop(columns=[\"Family\", \"Hash\"], inplace=True) # We have decided to use only category as target variable; Hash is temporary while im debugging (it will be deleted in EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Description of our features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each of our entries in the dataset we have a feature describing the state of the (operating) system under which the malware was running. Here is an intial description for each of them.\n",
        "Please access this document for a complete problem context description: https://docs.google.com/document/d/1yH9gvnJVSH9GLv9ATQ5JQWA2z8Jy4umxxRfMF-y2fiU/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section below shall be deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No duplicates found in the dataset\n"
          ]
        }
      ],
      "source": [
        "default_pipeline.preprocessing.remove_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No missing values found in the dataset\n"
          ]
        }
      ],
      "source": [
        "default_pipeline.preprocessing.get_missing_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we conduct an Exploratory Data Analysis (EDA) on the dynamic malware dataset to gain initial insights into the structure, distribution, and quality of the data prior to modeling. The dataset includes behavioral features extracted from Android applications, along with labels indicating their respective malware categories. Understanding the composition of the dataset, such as class imbalance, feature correlations, and the presence of outliers, is crucial to ensure robust preprocessing, informed feature engineering, and  the success of machine learning classifiers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SUGGESTIONS FOR IMPROVEMENTS\n",
        "- Cluster analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Type Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Get the data types of each column\n",
        "# data_types = dataset.df.dtypes\n",
        "\n",
        "# # Count the frequency of each data type\n",
        "# type_counts = data_types.value_counts()\n",
        "\n",
        "# # Plotting the frequency of data types\n",
        "# type_counts.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# # Add title and labels\n",
        "# plt.title('Frequency of Data Types in DataFrame')\n",
        "# plt.xlabel('Data Type')\n",
        "# plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that most columns are numerical. Lets gets to see which are the variables that are of type object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_onlyCols = dataset.df.select_dtypes(include=[\"object\"]).columns\n",
        "# df_onlyCols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28_glozSPDm7"
      },
      "source": [
        "## Histograms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVSB5cgoYRDr",
        "outputId": "6da84feb-1032-4185-b3e6-de73e05279df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Select only numerical features\\nnumerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\\n\\n# Define number of rows and columns for subplots\\nnum_features = len(numerical_cols)\\ncols = 4  # Number of columns per row\\nrows = math.ceil(num_features / cols)  # Calculate required rows\\n\\n# Create subplots\\nfig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\\naxes = axes.flatten()  # Flatten to easily iterate\\n\\n# Plot histograms\\nfor i, col in enumerate(numerical_cols):\\n    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])  # kde=True for smooth curve\\n    axes[i].set_title(col)\\n\\n# Remove empty subplots\\nfor i in range(num_features, len(axes)):\\n    fig.delaxes(axes[i])\\n\\nplt.tight_layout()\\nplt.show()'"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"# Select only numerical features\n",
        "numerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define number of rows and columns for subplots\n",
        "num_features = len(numerical_cols)\n",
        "cols = 4  # Number of columns per row\n",
        "rows = math.ceil(num_features / cols)  # Calculate required rows\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\n",
        "axes = axes.flatten()  # Flatten to easily iterate\n",
        "\n",
        "# Plot histograms\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])  # kde=True for smooth curve\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(num_features, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see most distributions tend to be right-skewed and only a small portion follows a normal distribution. This right-skewness will be dealt in feature-engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YDO2OqktbPDN",
        "outputId": "59795112-1aaf-4e5d-99fb-999c1b58b662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Select only numerical features\\nnumerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\\n\\n# Define number of rows and columns for subplots\\nnum_features = len(numerical_cols)\\ncols = 4  # Number of columns per row\\nrows = math.ceil(num_features / cols)  # Calculate required rows\\n\\n# Create subplots\\nfig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\\naxes = axes.flatten()  # Flatten to easily iterate\\n\\n# Plot boxplots\\nfor i, col in enumerate(numerical_cols):\\n    sns.boxplot(x=dataset.df[col], ax=axes[i])  # Boxplot for each feature\\n    axes[i].set_title(col)\\n\\n# Remove empty subplots\\nfor i in range(num_features, len(axes)):\\n    fig.delaxes(axes[i])\\n\\nplt.tight_layout()\\nplt.show()'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"# Select only numerical features\n",
        "numerical_cols = dataset.df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Define number of rows and columns for subplots\n",
        "num_features = len(numerical_cols)\n",
        "cols = 4  # Number of columns per row\n",
        "rows = math.ceil(num_features / cols)  # Calculate required rows\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 4))\n",
        "axes = axes.flatten()  # Flatten to easily iterate\n",
        "\n",
        "# Plot boxplots\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    sns.boxplot(x=dataset.df[col], ax=axes[i])  # Boxplot for each feature\n",
        "    axes[i].set_title(col)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(num_features, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDW_CStuioNs"
      },
      "source": [
        "## Numerical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnC_-AYbitFA"
      },
      "source": [
        "Seemed to be grouped by prefixes: Memory, Network, Battery, Logcat, Process y API.\n",
        "\n",
        "According to dataset authors to capture how various malware families and categories behave at runtime, the analysis relies on six distinct sets of features obtained after executing each sample within a controlled emulated environment. These feature groups offer a comprehensive view of the malware's dynamic activity.\n",
        "\n",
        "This categories appear before the first _ in every feature label and are defined as:\n",
        "\n",
        "\n",
        "\"Memory: Memory features define activities performed by malware by utilizing memory.\n",
        "\n",
        "API: Application Programming Interface (API) features delineate the communication between two applications.\n",
        "\n",
        "Network: Network features describe the data transmitted and received between other devices in the network. It indicates foreground and background network usage.\n",
        "\n",
        "Battery: Battery features describe the access to battery wakelock and services by malware.\n",
        "\n",
        "Logcat: Logcat features write log messages corresponding to a function performed by malware.\n",
        "\n",
        "Process: Process features count the interaction of malware with total number of processes.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raKBvgK5ircs",
        "outputId": "6109fdf7-3a42-4496-a39b-f34bfe204c1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'numeric_cols = dataset.df.select_dtypes(include=\\'number\\').columns\\n\\n# Grouping based on the first prefix before \"_\"\\nprefix_groups = defaultdict(list)\\n\\nfor col in numeric_cols:\\n    prefix = col.split(\"_\")[0]  # Get the first word before the underscore\\n    prefix_groups[prefix].append(col)\\n\\nfor prefix, columns in prefix_groups.items():\\n    print(f\"\\n {prefix} ({len(columns)} features):\")\\n    for col in columns:\\n        print(f\"  - {col}\")'"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"numeric_cols = dataset.df.select_dtypes(include='number').columns\n",
        "\n",
        "# Grouping based on the first prefix before \"_\"\n",
        "prefix_groups = defaultdict(list)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    prefix = col.split(\"_\")[0]  # Get the first word before the underscore\n",
        "    prefix_groups[prefix].append(col)\n",
        "\n",
        "for prefix, columns in prefix_groups.items():\n",
        "    print(f\"\\n {prefix} ({len(columns)} features):\")\n",
        "    for col in columns:\n",
        "        print(f\"  - {col}\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv7LWIIU5X5i"
      },
      "source": [
        "## Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "gA-v2L6x5ImZ",
        "outputId": "e7b19c28-c316-4b5a-eec7-90ee38c6dd0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'#Statistical summary for categorical features\\ndataset.df.describe(include=[\"object\", \"category\", \"bool\"])'"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"#Statistical summary for categorical features\n",
        "dataset.df.describe(include=[\"object\", \"category\", \"bool\"])\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJbNriVFrRYE",
        "outputId": "2935f3d0-a88b-4bd4-8b98-f4c76a641092"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"print(dataset.df[['Hash', 'Category', 'Family']].head())\""
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"print(dataset.df[['Hash', 'Category', 'Family']].head())\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBWBDyuc4q47"
      },
      "source": [
        "Hash: unique identifier that represents each malware sample. <<<>>>THIS IS PROBABLY WRONG<<<>>>\n",
        "\n",
        "Category: general classification of the malware sample based on its behavior.\n",
        "\n",
        "Family: more fine-grained grouping of malware based on its codebase or origin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC5leffBsW5W"
      },
      "source": [
        "For hash, it will first be checked if the same malware before and after reboot contains the same hash value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ep2R1vwzRyy",
        "outputId": "3048de1d-c0cd-437d-9044-68a3ed1c3fc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Count how many times each hash appears in \\'before\\' and \\'after\\'\\nhash_reboot_counts =dataset.df.groupby([\\'Hash\\', \\'reboot\\']).size().unstack(fill_value=0)\\n\\n# Hashes in both with exactly one in each\\nhashes_with_one_each = hash_reboot_counts[\\n    (hash_reboot_counts[\\'before\\'] == 1) & (hash_reboot_counts[\\'after\\'] == 1)\\n].index\\n\\n# Hashes in both but with extra rows\\nhashes_in_both_but_not_clean = hash_reboot_counts[\\n    (hash_reboot_counts[\\'before\\'] > 0) &\\n    (hash_reboot_counts[\\'after\\'] > 0) &\\n    ~((hash_reboot_counts[\\'before\\'] == 1) & (hash_reboot_counts[\\'after\\'] == 1))\\n].index\\n\\n# Total unique hashes\\ntotal_unique_hashes = dataset.df[\\'Hash\\'].nunique()\\n\\n# Hashes in only one reboot condition\\nhashes_in_one_condition = hash_reboot_counts[\\n    (hash_reboot_counts[\\'before\\'] == 0) | (hash_reboot_counts[\\'after\\'] == 0)\\n]\\n\\n# Only once in one reboot condition\\nonly_once_in_one = hashes_in_one_condition[\\n    (hashes_in_one_condition[\\'before\\'] == 1) | (hashes_in_one_condition[\\'after\\'] == 1)\\n]\\n\\n# More than once in one reboot condition\\nmore_than_once_in_one = hashes_in_one_condition[\\n    ((hashes_in_one_condition[\\'before\\'] > 1) & (hashes_in_one_condition[\\'after\\'] == 0)) |\\n    ((hashes_in_one_condition[\\'after\\'] > 1) & (hashes_in_one_condition[\\'before\\'] == 0))\\n]\\n\\n# Split those into counts\\nmore_than_once_in_before = more_than_once_in_one[more_than_once_in_one[\\'before\\'] > 1]\\nmore_than_once_in_after = more_than_once_in_one[more_than_once_in_one[\\'after\\'] > 1]\\n\\n# --- PRINT RESULTS ---\\nprint(f\"Hashes with EXACTLY one row in BOTH before and after: {len(hashes_with_one_each)}\")\\nprint(f\"Hashes in BOTH, BUT with extra rows: {len(hashes_in_both_but_not_clean)}\")\\n\\nprint(f\"\\nHashes in ONLY ONE reboot condition:\")\\nprint(f\"• Appearing ONLY ONCE: {len(only_once_in_one)}\")\\nprint(f\"• Appearing MORE THAN ONCE: {len(more_than_once_in_one)}\")\\nprint(f\"   - More than once in BEFORE: {len(more_than_once_in_before)}\")\\nprint(f\"   - More than once in AFTER: {len(more_than_once_in_after)}\")\\n\\nprint(f\"\\nTotal breakdown:\")\\nprint(f\"• In BOTH (any): {len(hashes_with_one_each) + len(hashes_in_both_but_not_clean)}\")\\nprint(f\"• In ONLY ONE reboot: {len(hashes_in_one_condition)}\")\\nprint(f\"• TOTAL unique hashes: {total_unique_hashes}\")\\n'"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"# Count how many times each hash appears in 'before' and 'after'\n",
        "hash_reboot_counts =dataset.df.groupby(['Hash', 'reboot']).size().unstack(fill_value=0)\n",
        "\n",
        "# Hashes in both with exactly one in each\n",
        "hashes_with_one_each = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] == 1) & (hash_reboot_counts['after'] == 1)\n",
        "].index\n",
        "\n",
        "# Hashes in both but with extra rows\n",
        "hashes_in_both_but_not_clean = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] > 0) &\n",
        "    (hash_reboot_counts['after'] > 0) &\n",
        "    ~((hash_reboot_counts['before'] == 1) & (hash_reboot_counts['after'] == 1))\n",
        "].index\n",
        "\n",
        "# Total unique hashes\n",
        "total_unique_hashes = dataset.df['Hash'].nunique()\n",
        "\n",
        "# Hashes in only one reboot condition\n",
        "hashes_in_one_condition = hash_reboot_counts[\n",
        "    (hash_reboot_counts['before'] == 0) | (hash_reboot_counts['after'] == 0)\n",
        "]\n",
        "\n",
        "# Only once in one reboot condition\n",
        "only_once_in_one = hashes_in_one_condition[\n",
        "    (hashes_in_one_condition['before'] == 1) | (hashes_in_one_condition['after'] == 1)\n",
        "]\n",
        "\n",
        "# More than once in one reboot condition\n",
        "more_than_once_in_one = hashes_in_one_condition[\n",
        "    ((hashes_in_one_condition['before'] > 1) & (hashes_in_one_condition['after'] == 0)) |\n",
        "    ((hashes_in_one_condition['after'] > 1) & (hashes_in_one_condition['before'] == 0))\n",
        "]\n",
        "\n",
        "# Split those into counts\n",
        "more_than_once_in_before = more_than_once_in_one[more_than_once_in_one['before'] > 1]\n",
        "more_than_once_in_after = more_than_once_in_one[more_than_once_in_one['after'] > 1]\n",
        "\n",
        "# --- PRINT RESULTS ---\n",
        "print(f\"Hashes with EXACTLY one row in BOTH before and after: {len(hashes_with_one_each)}\")\n",
        "print(f\"Hashes in BOTH, BUT with extra rows: {len(hashes_in_both_but_not_clean)}\")\n",
        "\n",
        "print(f\"\\nHashes in ONLY ONE reboot condition:\")\n",
        "print(f\"• Appearing ONLY ONCE: {len(only_once_in_one)}\")\n",
        "print(f\"• Appearing MORE THAN ONCE: {len(more_than_once_in_one)}\")\n",
        "print(f\"   - More than once in BEFORE: {len(more_than_once_in_before)}\")\n",
        "print(f\"   - More than once in AFTER: {len(more_than_once_in_after)}\")\n",
        "\n",
        "print(f\"\\nTotal breakdown:\")\n",
        "print(f\"• In BOTH (any): {len(hashes_with_one_each) + len(hashes_in_both_but_not_clean)}\")\n",
        "print(f\"• In ONLY ONE reboot: {len(hashes_in_one_condition)}\")\n",
        "print(f\"• TOTAL unique hashes: {total_unique_hashes}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2IGnd2izsOY"
      },
      "source": [
        "A total of 19,169 hashes appear exactly once in both before and after conditions. These are highly reliable for paired  comparisons, ideal for understanding how reboot affects malware behavior.\n",
        "\n",
        "\n",
        "There are 158 hashes that appear in both reboot states but not exactly once in each. These extra instances may come from inconsistencies in data capture like multiple logs for the same sample and should be checked.\n",
        "\n",
        "A significant portion of samples appear only in one reboot condition. This is consistent with limitations described in the original dataset paper, where some malware samples failed to execute after the reboot. However, what is curious is that some still have been logged more than once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "NzJxov9LLOGu",
        "outputId": "a0691e50-f676-4ef2-e150-166b1e96a5ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"dataset.df.drop(columns=['Hash'], inplace=True)\\n'''\\nThe Hash column is a high-cardinality feature, containing unique values for a high number of rows in the dataset.\\nIt serves as an identifier for each malware sample. Including this column in modeling\\nwould not only offer no predictive value but could also lead to overfitting or cause issues with algorithms that are\\nsensitive to high-cardinality categorical features.\\n <<<>>> J.N: may be better to focus the argumentation on ID not being useful rather than high-cardinality per se. Also write the \\n  argumentation in a text cell not in this type of comments. <<<>>>\\n'''\""
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"dataset.df.drop(columns=['Hash'], inplace=True)\n",
        "'''\n",
        "The Hash column is a high-cardinality feature, containing unique values for a high number of rows in the dataset.\n",
        "It serves as an identifier for each malware sample. Including this column in modeling\n",
        "would not only offer no predictive value but could also lead to overfitting or cause issues with algorithms that are\n",
        "sensitive to high-cardinality categorical features.\n",
        " <<<>>> J.N: may be better to focus the argumentation on ID not being useful rather than high-cardinality per se. Also write the \n",
        "  argumentation in a text cell not in this type of comments. <<<>>>\n",
        "'''\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZUupRE1vx5"
      },
      "source": [
        "This research will be using both Category and Family as the target variables for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaHsU58AdvlN"
      },
      "source": [
        "## Reboot Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ymeBe4fKdNi",
        "outputId": "ce478b18-ff6a-4720-f19c-e5ecabfbbbbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'print(dataset.df[\"reboot\"].value_counts())'"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"print(dataset.df[\"reboot\"].value_counts())\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV8hkdL2SHVe"
      },
      "source": [
        "The imbalance observed in the dataset, with 28,380 samples collected before reboot and only 25,059 after reboot, is explained by limitations found during the dynamic analysis. The authors of the dataset note that \"there was no entry point in some Android malware samples and some Android malware samples stopped abruptly.\" This means that certain malware applications either failed to launch or terminated unexpectedly during execution, preventing the collection of dynamic behavior data, particularly after the reboot phase.\n",
        "\n",
        "Additionally, the study highlights another critical limitation: \"the dynamic analysis is performed in an emulator. Some malware samples are able to detect the emulated environment and are not executed.\" This behavior reflects common anti-analysis techniques used by sophisticated malware, which can detect when they are running in a sandbox or emulator and intentionally suspend their malicious actions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<<<>>>THIS ANALYSIS IS SUPER GOOD (you can delete this comment)<<<>>>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuIT6CKgeqcZ"
      },
      "source": [
        "The displayed features are the top 10  most affected by reboot showing a clear reboot-sensitive behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "i9p8S9woa19A",
        "outputId": "7c14fef2-103d-40a4-85e0-50c138fdb448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'#Category distribution across reboot\\nplt.figure(figsize=(12, 6))\\nsns.countplot(data=dataset.df, x=\\'Category\\', hue=\\'reboot\\')\\nplt.title(\"Malware Categories by Reboot Condition\")\\nplt.xticks(rotation=45)\\nplt.tight_layout()\\nplt.show()'"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"#Category distribution across reboot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=dataset.df, x='Category', hue='reboot')\n",
        "plt.title(\"Malware Categories by Reboot Condition\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWwaTM_JeoWk"
      },
      "source": [
        "To identify which numeric features are most influenced by the reboot condition, the dataset will be grouped by the reboot variable, separating entries collected before and after the device reboot. Within each group, the mean of every numeric feature will be computed, allowing for the comparison of average behavior across both states.\n",
        "\n",
        "A new column labeled 'diff' was then added, representing the difference between the mean values after and before the reboot for each feature. A positive value indicates that the feature increased after reboot, while a negative value shows it decreased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "jG1RQO7AcIWG",
        "outputId": "00489ed7-d243-4fae-d577-849381abbca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"reboot_means = dataset.df.groupby('reboot').mean(numeric_only=True).T\\nreboot_means['diff'] = reboot_means['after'] - reboot_means['before']\\nreboot_means_sorted = reboot_means.sort_values(by='diff', ascending=False)\\n\\nreboot_means_sorted.head(10)\""
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"reboot_means = dataset.df.groupby('reboot').mean(numeric_only=True).T\n",
        "reboot_means['diff'] = reboot_means['after'] - reboot_means['before']\n",
        "reboot_means_sorted = reboot_means.sort_values(by='diff', ascending=False)\n",
        "\n",
        "reboot_means_sorted.head(10)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7cm_Z81fWqq"
      },
      "source": [
        "The results reveal that several features show clear shifts after reboot. Specially, network-related features such as Network_TotalReceivedBytes and Network_TotalTransmittedBytes demonstrate significant increases, suggesting that some malware types intensify data transmission once the device has rebooted. Memory features like Memory_SharedClean, Memory_HeapSize, and Memory_HeapAlloc also show increased values after reboot, indicating greater memory use or altered memory management after reboot.\n",
        "This shows that the reboot condition plays an important role in runtime behavior and should be treated as an important factor in exploratory analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'#How many categories each family belongs to\\ndataset.df.groupby(\"Family\")[\"Category\"].nunique().sort_values(ascending=False)'"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"#How many categories each family belongs to\n",
        "dataset.df.groupby(\"Family\")[\"Category\"].nunique().sort_values(ascending=False)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Almost every family is either unknown or unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <<<Error: NameError: name 'family_to_category' is not defined>>> (this Irina's code; copied from Argentinan guy's notebook)\n",
        "# multi_cat_families = family_to_category[family_to_category > 1]\n",
        "# print(f\"Number of families mapping to multiple categories: {len(multi_cat_families)}\")\n",
        "# print(multi_cat_families)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is only one Family that maps to multiple categories, and is the placeholder unknown.\n",
        "\n",
        "The following code displays how many samples with unknown family labels belong to each malware category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()'"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Step 1: Count unique families per category\\nfamily_amount = dataset.df.groupby(\"Category\")[\"Family\"].nunique()\\n\\n# Step 2: Total number of instances per category\\ntotal_per_category = dataset.df[\"Category\"].value_counts()\\n\\n# Step 3: Count how many of those are <unknown> per category\\nunknown_amount = dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()\\n\\n# Step 4: Combine all stats into a summary table\\nsummary_df = pd.DataFrame({\\n    \"Family_amount\": family_amount,\\n    \"Total_category\": total_per_category,\\n    \"Unknown_amount\": unknown_amount\\n}).fillna(0).astype({\"Unknown_amount\": int})\\n\\n# Step 5: Calculate percentage of unknowns per category\\nsummary_df[\"%_Unknown\"] = (summary_df[\"Unknown_amount\"] / summary_df[\"Total_category\"] * 100).round(2)\\n\\n# Reorder columns for readability\\nsummary_df = summary_df[[\"Family_amount\", \"Total_category\", \"Unknown_amount\", \"%_Unknown\"]]\\n\\n# Display the summary\\nprint(summary_df)'"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"# Step 1: Count unique families per category\n",
        "family_amount = dataset.df.groupby(\"Category\")[\"Family\"].nunique()\n",
        "\n",
        "# Step 2: Total number of instances per category\n",
        "total_per_category = dataset.df[\"Category\"].value_counts()\n",
        "\n",
        "# Step 3: Count how many of those are <unknown> per category\n",
        "unknown_amount = dataset.df[dataset.df[\"Family\"] == \"<unknown>\"][\"Category\"].value_counts()\n",
        "\n",
        "# Step 4: Combine all stats into a summary table\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Family_amount\": family_amount,\n",
        "    \"Total_category\": total_per_category,\n",
        "    \"Unknown_amount\": unknown_amount\n",
        "}).fillna(0).astype({\"Unknown_amount\": int})\n",
        "\n",
        "# Step 5: Calculate percentage of unknowns per category\n",
        "summary_df[\"%_Unknown\"] = (summary_df[\"Unknown_amount\"] / summary_df[\"Total_category\"] * 100).round(2)\n",
        "\n",
        "# Reorder columns for readability\n",
        "summary_df = summary_df[[\"Family_amount\", \"Total_category\", \"Unknown_amount\", \"%_Unknown\"]]\n",
        "\n",
        "# Display the summary\n",
        "print(summary_df)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'unknown_count = (dataset.df[\"Family\"] == \"<unknown>\").sum()\\nprint(f\"Number of rows with Family == \\'<unknown>\\': {unknown_count}\")'"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"unknown_count = (dataset.df[\"Family\"] == \"<unknown>\").sum()\n",
        "print(f\"Number of rows with Family == '<unknown>': {unknown_count}\")\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the analysis of family distribution across categories:\n",
        "\n",
        "The Adware category stands out with zero instances labeled as <unknown> and a balanced distribution across 43 families. This makes it a strong candidate for modeling.\n",
        "\n",
        "In contrast, Zero_Day and No_Category The categories Zero_Day and No_Category exhibit extremely high family dispersion, with 2576 and 335 unique families. These values are significantly higher than all other categories, which generally have fewer than 50 families each.\n",
        "\n",
        "\n",
        "This suggests they function more as placeholder labels. In particular, Zero_Day likely serves as a catch-all label for unknown or uncategorized threats, making it ambiguous. In cybersecurity, this term is refered to a new unknown vulnerability, not yet classified in terms of malware behavior, this is why samples are varied. They do not seem to represent a consistent type. On the other hand, No_Category explicitly denotes a lack of category. So, including these instances would only bring noise to the training process, preventing the model from learning meaningful patterns.\n",
        "Therefore, they are excluded from the final dataset to preserve the quality and consistency of the classification task.\n",
        "\n",
        "\n",
        "Additionally, categories like FileInfector show a high percentage of <unknown> families (6.85%) despite having a small total count, raising concerns about label quality. Most other categories maintain a relatively stable level of unknowns (around 3–5%), indicating that the presence of <unknown> is manageable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. DATA SPLITTING\n",
        "### TO BE DONE\n",
        "- Statistical analysis of this\n",
        "- Make sure they follow the same distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting: Category as target variable\n",
        "Originally, we will focus only on category\n",
        "\n",
        "Lets first get the X and y extracted from our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also object!\n",
        "Lets get back to the splitting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before, we split the dataset lets observe the SE of accuracy variation based on our choice of split. \n",
        "Brief explanation: we can model accuracy via a Binomial distribution. We know each event in a binomial distribution can be modelled through a bernoulli distribution, where the outcome represents the probability predicting the correct class or not. We make the assumption that each classification error is independent from each other. For:\n",
        "$$\n",
        "\\text{Bin} \\sim (n, p)\n",
        "$$\n",
        " let us assume that the parameter of this distribution is p = .85 and n is given by the choice of sample split for the test set. The SE of the sample proportion can be modeled via: \n",
        "$$\n",
        "\\text{SE}_{\\hat{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\n",
        "\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we continue to assess all possible choices of split based on a variant n, also note that that choice of evenly distributed split (e.g: 10% for each hold-out set) between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n",
        "\n",
        "Finally, also note that that choice of evenly distributed split between hold-out sets is arbitrary. Proper choice is that which guarantees an equivalent distribution at each hold-out sets, which may not neccesarily be the equivalent split. This brings a new choice of tradeoff between certainity of prediction accuracy (higher test size, smaller validaiton set) but possibly less space for proper hyperparemter optimization or the inverse (higher validation size, smaller set set). As with other tradeoffs, the priority for a given option is rooted in the model's application (which may come derived from a client/employer). For our case, we dont favor either option of the tradeoof, thus we will keep the even hold-out distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/javierdominguezsegura/Academics/College/Courses/2nd year/ML/finalProject/ml-final-project/library/phases/dataset/split/strategies/noTimeSeries.py:112: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  differenceToPriorSE_percentage = (currentSE - priorSE) /  priorSE\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_size (%)</th>\n",
              "      <th>train_size_count</th>\n",
              "      <th>validation_size (%)</th>\n",
              "      <th>validation_size_count</th>\n",
              "      <th>test_size (%)</th>\n",
              "      <th>test_size_coount</th>\n",
              "      <th>currentSE</th>\n",
              "      <th>differenceToPriorSE</th>\n",
              "      <th>differenceToPriorSE (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.95</td>\n",
              "      <td>50767.0</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1336.0</td>\n",
              "      <td>0.025</td>\n",
              "      <td>1336.0</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>inf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.90</td>\n",
              "      <td>48095.0</td>\n",
              "      <td>0.050</td>\n",
              "      <td>2672.0</td>\n",
              "      <td>0.050</td>\n",
              "      <td>2672.0</td>\n",
              "      <td>0.006908</td>\n",
              "      <td>-0.002861</td>\n",
              "      <td>-0.292893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.85</td>\n",
              "      <td>45423.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>4008.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>4008.0</td>\n",
              "      <td>0.005640</td>\n",
              "      <td>-0.001268</td>\n",
              "      <td>-0.183503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.80</td>\n",
              "      <td>42751.0</td>\n",
              "      <td>0.100</td>\n",
              "      <td>5344.0</td>\n",
              "      <td>0.100</td>\n",
              "      <td>5344.0</td>\n",
              "      <td>0.004885</td>\n",
              "      <td>-0.000756</td>\n",
              "      <td>-0.133975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.75</td>\n",
              "      <td>40079.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>6680.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>6680.0</td>\n",
              "      <td>0.004369</td>\n",
              "      <td>-0.000516</td>\n",
              "      <td>-0.105573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.70</td>\n",
              "      <td>37407.0</td>\n",
              "      <td>0.150</td>\n",
              "      <td>8016.0</td>\n",
              "      <td>0.150</td>\n",
              "      <td>8016.0</td>\n",
              "      <td>0.003988</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>-0.087129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.65</td>\n",
              "      <td>34735.0</td>\n",
              "      <td>0.175</td>\n",
              "      <td>9352.0</td>\n",
              "      <td>0.175</td>\n",
              "      <td>9352.0</td>\n",
              "      <td>0.003692</td>\n",
              "      <td>-0.000296</td>\n",
              "      <td>-0.074180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.60</td>\n",
              "      <td>32063.0</td>\n",
              "      <td>0.200</td>\n",
              "      <td>10688.0</td>\n",
              "      <td>0.200</td>\n",
              "      <td>10688.0</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>-0.000238</td>\n",
              "      <td>-0.064586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.55</td>\n",
              "      <td>29391.0</td>\n",
              "      <td>0.225</td>\n",
              "      <td>12024.0</td>\n",
              "      <td>0.225</td>\n",
              "      <td>12024.0</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>-0.057191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.50</td>\n",
              "      <td>26720.0</td>\n",
              "      <td>0.250</td>\n",
              "      <td>13360.0</td>\n",
              "      <td>0.250</td>\n",
              "      <td>13360.0</td>\n",
              "      <td>0.003089</td>\n",
              "      <td>-0.000167</td>\n",
              "      <td>-0.051317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   train_size (%)  train_size_count  validation_size (%)  \\\n",
              "0            0.95           50767.0                0.025   \n",
              "1            0.90           48095.0                0.050   \n",
              "2            0.85           45423.0                0.075   \n",
              "3            0.80           42751.0                0.100   \n",
              "4            0.75           40079.0                0.125   \n",
              "5            0.70           37407.0                0.150   \n",
              "6            0.65           34735.0                0.175   \n",
              "7            0.60           32063.0                0.200   \n",
              "8            0.55           29391.0                0.225   \n",
              "9            0.50           26720.0                0.250   \n",
              "\n",
              "   validation_size_count  test_size (%)  test_size_coount  currentSE  \\\n",
              "0                 1336.0          0.025            1336.0   0.009769   \n",
              "1                 2672.0          0.050            2672.0   0.006908   \n",
              "2                 4008.0          0.075            4008.0   0.005640   \n",
              "3                 5344.0          0.100            5344.0   0.004885   \n",
              "4                 6680.0          0.125            6680.0   0.004369   \n",
              "5                 8016.0          0.150            8016.0   0.003988   \n",
              "6                 9352.0          0.175            9352.0   0.003692   \n",
              "7                10688.0          0.200           10688.0   0.003454   \n",
              "8                12024.0          0.225           12024.0   0.003256   \n",
              "9                13360.0          0.250           13360.0   0.003089   \n",
              "\n",
              "   differenceToPriorSE  differenceToPriorSE (%)  \n",
              "0             0.009769                      inf  \n",
              "1            -0.002861                -0.292893  \n",
              "2            -0.001268                -0.183503  \n",
              "3            -0.000756                -0.133975  \n",
              "4            -0.000516                -0.105573  \n",
              "5            -0.000381                -0.087129  \n",
              "6            -0.000296                -0.074180  \n",
              "7            -0.000238                -0.064586  \n",
              "8            -0.000198                -0.057191  \n",
              "9            -0.000167                -0.051317  "
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.split.asses_split_classifier(p=.85, step=.05, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[We can see a diminishing-returns class graph](https://en.wikipedia.org/wiki/Knee_of_a_curve). The more we decline the training set percentage the slower and more steadier the current SE varies as well as the difference to prior SE, in percentage. We can see that the knee in the curve is between 80 and 90 training set percentage. This represents the area where when you start going below the lower bound, no **significant** improve appears. Given the fact that our criteria for choice of split percentage is to keep as much training possible while increasing hold-out sets size only if the decrease in SE is significant **we are going to select 80% for the training set**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.dataset.split.split_data(y_column=\"Category\",\n",
        "                                   train_size=.8, \n",
        "                                   validation_size=.1,\n",
        "                                   test_size=.1, \n",
        "                                   plot_distribution=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((42751, 142), (5344, 142), (5344, 142), (42751,), (5344,), (5344,))"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.X_train.shape, default_pipeline.dataset.X_val.shape, default_pipeline.dataset.X_test.shape, default_pipeline.dataset.y_train.shape, default_pipeline.dataset.y_val.shape, default_pipeline.dataset.y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will work only with the training set to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataset.X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We eliminate the field `Reboot_before` since it is boolean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocessor.remove_reboot_column()\n",
        "# preprocessor.X_train_without_reboot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocessor.X_train_without_reboot.describe().T[['min', 'max']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secondly, we're gonna check for kurtosis and skewness. If skewness is high we might need to consider standardizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(f\"\\nSkewness:\\n----------------\\n{preprocessor.X_train_without_reboot.skew()}\")\n",
        "# print(f\"\\nKurtosis:\\n----------------\\n{preprocessor.X_train_without_reboot.kurt()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there is a very large number of fields, we need to avoid looking at the distribution and information about each feature. For this, we decided to create an automation that decides the appropriate scaling method -if necessary- for each feature.\n",
        "\n",
        "This function `determine_scaling_method` takes into account outlier detection and skewness and based on that it decides to use a **robust scaler**, **normalize** or **none**.\n",
        "\n",
        "**Note**: we use robust scaler rather than a standard scaler because robust scales with the median and IQR, which is less sensitive to outliers than standardizing is -computed with $\\mu$ and $\\sigma$. It will turn the median to 0 and turn the values in a field in values between -1 and 1 with some outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalization Formula:\n",
        " \n",
        "\n",
        "Robust Scaler Formula:\n",
        " \n",
        "Where:\n",
        "\n",
        "IQR (Interquartile Range) = Q3 - Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocessor.prepare_scaling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train_scaled = preprocessor.apply_scaling()\n",
        "\n",
        "# \"\\nRobust complete! The training data is now ready for modeling.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The histograms show how `RobustScaler` compresses extreme values and brings the bulk of the data closer to a common scale. Despite the presence of outliers, the core distribution becomes more uniform and comparable across features — ideal for many ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section below shall be properly written. It is set now for early debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## !> Pipeline example\n",
        "Let us show an example of the pipeline divergence. We will create a divergence for the baseline pipeline. We will exempt it from being scaled. This specific example is not meant to be kept, but rather show the purpose of the corresponding functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_pipeline = pipeline_manager.create_pipeline_divergence(category=\"baseline\", pipelineName=\"logistic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Network_TotalTransmittedPackets</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.0</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>4.275100e+04</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>4.275100e+04</td>\n",
              "      <td>42751.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>70944.886272</td>\n",
              "      <td>12764.341700</td>\n",
              "      <td>10844.857898</td>\n",
              "      <td>47646.187738</td>\n",
              "      <td>88029.087694</td>\n",
              "      <td>14333.172277</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22278.685996</td>\n",
              "      <td>17539.452434</td>\n",
              "      <td>4738.241304</td>\n",
              "      <td>...</td>\n",
              "      <td>466.232720</td>\n",
              "      <td>3.396201</td>\n",
              "      <td>0.723235</td>\n",
              "      <td>2337.759865</td>\n",
              "      <td>2.370012e+03</td>\n",
              "      <td>2296.848963</td>\n",
              "      <td>2300.166686</td>\n",
              "      <td>2300.399780</td>\n",
              "      <td>1.160519e+04</td>\n",
              "      <td>190.169821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>36266.506343</td>\n",
              "      <td>12900.070827</td>\n",
              "      <td>999.033790</td>\n",
              "      <td>29793.858149</td>\n",
              "      <td>15871.265926</td>\n",
              "      <td>13468.721351</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12570.621975</td>\n",
              "      <td>10519.885159</td>\n",
              "      <td>2939.327428</td>\n",
              "      <td>...</td>\n",
              "      <td>3644.649287</td>\n",
              "      <td>1.518657</td>\n",
              "      <td>0.981457</td>\n",
              "      <td>2203.232434</td>\n",
              "      <td>1.038044e+04</td>\n",
              "      <td>2273.580562</td>\n",
              "      <td>2031.749201</td>\n",
              "      <td>1976.428885</td>\n",
              "      <td>1.101732e+04</td>\n",
              "      <td>2.896415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4808.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7036.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4268.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5113.000000</td>\n",
              "      <td>3249.000000</td>\n",
              "      <td>1012.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000e+01</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>3.121000e+03</td>\n",
              "      <td>172.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>46185.000000</td>\n",
              "      <td>3972.000000</td>\n",
              "      <td>10540.000000</td>\n",
              "      <td>31992.000000</td>\n",
              "      <td>76716.000000</td>\n",
              "      <td>4984.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13486.500000</td>\n",
              "      <td>10670.000000</td>\n",
              "      <td>2994.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1159.000000</td>\n",
              "      <td>1.098000e+03</td>\n",
              "      <td>1107.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>1135.000000</td>\n",
              "      <td>9.422000e+03</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61243.000000</td>\n",
              "      <td>6356.000000</td>\n",
              "      <td>10732.000000</td>\n",
              "      <td>41444.000000</td>\n",
              "      <td>89584.000000</td>\n",
              "      <td>8180.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20447.000000</td>\n",
              "      <td>16560.000000</td>\n",
              "      <td>4070.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2043.000000</td>\n",
              "      <td>2.020000e+03</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>1.097700e+04</td>\n",
              "      <td>190.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>91785.000000</td>\n",
              "      <td>20048.000000</td>\n",
              "      <td>11144.000000</td>\n",
              "      <td>55338.000000</td>\n",
              "      <td>99092.000000</td>\n",
              "      <td>21740.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28743.500000</td>\n",
              "      <td>22846.500000</td>\n",
              "      <td>5838.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>124.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3054.000000</td>\n",
              "      <td>3.028000e+03</td>\n",
              "      <td>3027.000000</td>\n",
              "      <td>3026.000000</td>\n",
              "      <td>3037.000000</td>\n",
              "      <td>1.299250e+04</td>\n",
              "      <td>192.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>819456.000000</td>\n",
              "      <td>106360.000000</td>\n",
              "      <td>40416.000000</td>\n",
              "      <td>809032.000000</td>\n",
              "      <td>183600.000000</td>\n",
              "      <td>110336.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>310155.000000</td>\n",
              "      <td>286959.000000</td>\n",
              "      <td>88630.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>85873.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>219143.000000</td>\n",
              "      <td>2.066046e+06</td>\n",
              "      <td>142010.000000</td>\n",
              "      <td>89456.000000</td>\n",
              "      <td>102860.000000</td>\n",
              "      <td>2.067038e+06</td>\n",
              "      <td>221.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "count     42751.000000     42751.000000        42751.000000   \n",
              "mean      70944.886272     12764.341700        10844.857898   \n",
              "std       36266.506343     12900.070827          999.033790   \n",
              "min        4808.000000         0.000000         7036.000000   \n",
              "25%       46185.000000      3972.000000        10540.000000   \n",
              "50%       61243.000000      6356.000000        10732.000000   \n",
              "75%       91785.000000     20048.000000        11144.000000   \n",
              "max      819456.000000    106360.000000        40416.000000   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "count         42751.000000        42751.000000         42751.000000   \n",
              "mean          47646.187738        88029.087694         14333.172277   \n",
              "std           29793.858149        15871.265926         13468.721351   \n",
              "min              12.000000         4268.000000             0.000000   \n",
              "25%           31992.000000        76716.000000          4984.000000   \n",
              "50%           41444.000000        89584.000000          8180.000000   \n",
              "75%           55338.000000        99092.000000         21740.000000   \n",
              "max          809032.000000       183600.000000        110336.000000   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "count              42751.0     42751.000000      42751.000000   \n",
              "mean                   0.0     22278.685996      17539.452434   \n",
              "std                    0.0     12570.621975      10519.885159   \n",
              "min                    0.0      5113.000000       3249.000000   \n",
              "25%                    0.0     13486.500000      10670.000000   \n",
              "50%                    0.0     20447.000000      16560.000000   \n",
              "75%                    0.0     28743.500000      22846.500000   \n",
              "max                    0.0    310155.000000     286959.000000   \n",
              "\n",
              "       Memory_HeapFree  ...  Network_TotalTransmittedPackets  \\\n",
              "count     42751.000000  ...                     42751.000000   \n",
              "mean       4738.241304  ...                       466.232720   \n",
              "std        2939.327428  ...                      3644.649287   \n",
              "min        1012.000000  ...                         0.000000   \n",
              "25%        2994.000000  ...                         0.000000   \n",
              "50%        4070.000000  ...                        21.000000   \n",
              "75%        5838.000000  ...                       124.500000   \n",
              "max       88630.000000  ...                     85873.000000   \n",
              "\n",
              "       Battery_wakelock  Battery_service    Logcat_info  Logcat_error  \\\n",
              "count      42751.000000     42751.000000   42751.000000  4.275100e+04   \n",
              "mean           3.396201         0.723235    2337.759865  2.370012e+03   \n",
              "std            1.518657         0.981457    2203.232434  1.038044e+04   \n",
              "min            0.000000         0.000000       6.000000  4.000000e+01   \n",
              "25%            2.000000         0.000000    1159.000000  1.098000e+03   \n",
              "50%            3.000000         0.000000    2043.000000  2.020000e+03   \n",
              "75%            4.000000         1.000000    3054.000000  3.028000e+03   \n",
              "max           12.000000        10.000000  219143.000000  2.066046e+06   \n",
              "\n",
              "       Logcat_warning  Logcat_debug  Logcat_verbose  Logcat_total  \\\n",
              "count    42751.000000  42751.000000    42751.000000  4.275100e+04   \n",
              "mean      2296.848963   2300.166686     2300.399780  1.160519e+04   \n",
              "std       2273.580562   2031.749201     1976.428885  1.101732e+04   \n",
              "min         36.000000     45.000000       33.000000  3.121000e+03   \n",
              "25%       1107.000000   1100.000000     1135.000000  9.422000e+03   \n",
              "50%       2000.000000   2015.000000     2004.000000  1.097700e+04   \n",
              "75%       3027.000000   3026.000000     3037.000000  1.299250e+04   \n",
              "max     142010.000000  89456.000000   102860.000000  2.067038e+06   \n",
              "\n",
              "       Process_total  \n",
              "count   42751.000000  \n",
              "mean      190.169821  \n",
              "std         2.896415  \n",
              "min       172.000000  \n",
              "25%       188.000000  \n",
              "50%       190.000000  \n",
              "75%       192.000000  \n",
              "max       221.000000  \n",
              "\n",
              "[8 rows x 141 columns]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.preprocessing.scale_features(scaler=\"robust\",\n",
        "                                      columnsToScale=default_pipeline.dataset.X_train.select_dtypes(include=[\"number\"]).columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Network_TotalTransmittedPackets</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.0</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.212761</td>\n",
              "      <td>0.398628</td>\n",
              "      <td>0.186851</td>\n",
              "      <td>0.265664</td>\n",
              "      <td>-0.069490</td>\n",
              "      <td>0.367222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.120055</td>\n",
              "      <td>0.080438</td>\n",
              "      <td>0.234965</td>\n",
              "      <td>...</td>\n",
              "      <td>3.576166</td>\n",
              "      <td>0.198101</td>\n",
              "      <td>0.723235</td>\n",
              "      <td>0.155546</td>\n",
              "      <td>0.181353</td>\n",
              "      <td>0.154609</td>\n",
              "      <td>0.148062</td>\n",
              "      <td>0.155836</td>\n",
              "      <td>0.175938</td>\n",
              "      <td>0.042455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.795318</td>\n",
              "      <td>0.802443</td>\n",
              "      <td>1.654029</td>\n",
              "      <td>1.276187</td>\n",
              "      <td>0.709299</td>\n",
              "      <td>0.803815</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.823925</td>\n",
              "      <td>0.863950</td>\n",
              "      <td>1.033519</td>\n",
              "      <td>...</td>\n",
              "      <td>29.274291</td>\n",
              "      <td>0.759329</td>\n",
              "      <td>0.981457</td>\n",
              "      <td>1.162656</td>\n",
              "      <td>5.378467</td>\n",
              "      <td>1.184157</td>\n",
              "      <td>1.054906</td>\n",
              "      <td>1.039132</td>\n",
              "      <td>3.085652</td>\n",
              "      <td>0.724104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.237610</td>\n",
              "      <td>-0.395372</td>\n",
              "      <td>-6.119205</td>\n",
              "      <td>-1.774694</td>\n",
              "      <td>-3.812835</td>\n",
              "      <td>-0.488183</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.005047</td>\n",
              "      <td>-1.093171</td>\n",
              "      <td>-1.075246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.168675</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.074934</td>\n",
              "      <td>-1.025907</td>\n",
              "      <td>-1.022917</td>\n",
              "      <td>-1.022845</td>\n",
              "      <td>-1.036278</td>\n",
              "      <td>-2.200252</td>\n",
              "      <td>-4.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.330219</td>\n",
              "      <td>-0.148296</td>\n",
              "      <td>-0.317881</td>\n",
              "      <td>-0.404866</td>\n",
              "      <td>-0.575080</td>\n",
              "      <td>-0.190738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.456217</td>\n",
              "      <td>-0.483719</td>\n",
              "      <td>-0.378340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.168675</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.466491</td>\n",
              "      <td>-0.477720</td>\n",
              "      <td>-0.465104</td>\n",
              "      <td>-0.475078</td>\n",
              "      <td>-0.456887</td>\n",
              "      <td>-0.435513</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.669781</td>\n",
              "      <td>0.851704</td>\n",
              "      <td>0.682119</td>\n",
              "      <td>0.595134</td>\n",
              "      <td>0.424920</td>\n",
              "      <td>0.809262</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.543783</td>\n",
              "      <td>0.516281</td>\n",
              "      <td>0.621660</td>\n",
              "      <td>...</td>\n",
              "      <td>0.831325</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.533509</td>\n",
              "      <td>0.522280</td>\n",
              "      <td>0.534896</td>\n",
              "      <td>0.524922</td>\n",
              "      <td>0.543113</td>\n",
              "      <td>0.564487</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>16.627478</td>\n",
              "      <td>6.220702</td>\n",
              "      <td>49.145695</td>\n",
              "      <td>32.878780</td>\n",
              "      <td>4.201645</td>\n",
              "      <td>6.096682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.988530</td>\n",
              "      <td>22.206628</td>\n",
              "      <td>29.732771</td>\n",
              "      <td>...</td>\n",
              "      <td>689.574297</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>114.564644</td>\n",
              "      <td>1069.443523</td>\n",
              "      <td>72.921875</td>\n",
              "      <td>45.400312</td>\n",
              "      <td>53.026288</td>\n",
              "      <td>575.846800</td>\n",
              "      <td>7.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "count     42751.000000     42751.000000        42751.000000   \n",
              "mean          0.212761         0.398628            0.186851   \n",
              "std           0.795318         0.802443            1.654029   \n",
              "min          -1.237610        -0.395372           -6.119205   \n",
              "25%          -0.330219        -0.148296           -0.317881   \n",
              "50%           0.000000         0.000000            0.000000   \n",
              "75%           0.669781         0.851704            0.682119   \n",
              "max          16.627478         6.220702           49.145695   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "count         42751.000000        42751.000000         42751.000000   \n",
              "mean              0.265664           -0.069490             0.367222   \n",
              "std               1.276187            0.709299             0.803815   \n",
              "min              -1.774694           -3.812835            -0.488183   \n",
              "25%              -0.404866           -0.575080            -0.190738   \n",
              "50%               0.000000            0.000000             0.000000   \n",
              "75%               0.595134            0.424920             0.809262   \n",
              "max              32.878780            4.201645             6.096682   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "count              42751.0     42751.000000      42751.000000   \n",
              "mean                   0.0         0.120055          0.080438   \n",
              "std                    0.0         0.823925          0.863950   \n",
              "min                    0.0        -1.005047         -1.093171   \n",
              "25%                    0.0        -0.456217         -0.483719   \n",
              "50%                    0.0         0.000000          0.000000   \n",
              "75%                    0.0         0.543783          0.516281   \n",
              "max                    0.0        18.988530         22.206628   \n",
              "\n",
              "       Memory_HeapFree  ...  Network_TotalTransmittedPackets  \\\n",
              "count     42751.000000  ...                     42751.000000   \n",
              "mean          0.234965  ...                         3.576166   \n",
              "std           1.033519  ...                        29.274291   \n",
              "min          -1.075246  ...                        -0.168675   \n",
              "25%          -0.378340  ...                        -0.168675   \n",
              "50%           0.000000  ...                         0.000000   \n",
              "75%           0.621660  ...                         0.831325   \n",
              "max          29.732771  ...                       689.574297   \n",
              "\n",
              "       Battery_wakelock  Battery_service   Logcat_info  Logcat_error  \\\n",
              "count      42751.000000     42751.000000  42751.000000  42751.000000   \n",
              "mean           0.198101         0.723235      0.155546      0.181353   \n",
              "std            0.759329         0.981457      1.162656      5.378467   \n",
              "min           -1.500000         0.000000     -1.074934     -1.025907   \n",
              "25%           -0.500000         0.000000     -0.466491     -0.477720   \n",
              "50%            0.000000         0.000000      0.000000      0.000000   \n",
              "75%            0.500000         1.000000      0.533509      0.522280   \n",
              "max            4.500000        10.000000    114.564644   1069.443523   \n",
              "\n",
              "       Logcat_warning  Logcat_debug  Logcat_verbose  Logcat_total  \\\n",
              "count    42751.000000  42751.000000    42751.000000  42751.000000   \n",
              "mean         0.154609      0.148062        0.155836      0.175938   \n",
              "std          1.184157      1.054906        1.039132      3.085652   \n",
              "min         -1.022917     -1.022845       -1.036278     -2.200252   \n",
              "25%         -0.465104     -0.475078       -0.456887     -0.435513   \n",
              "50%          0.000000      0.000000        0.000000      0.000000   \n",
              "75%          0.534896      0.524922        0.543113      0.564487   \n",
              "max         72.921875     45.400312       53.026288    575.846800   \n",
              "\n",
              "       Process_total  \n",
              "count   42751.000000  \n",
              "mean        0.042455  \n",
              "std         0.724104  \n",
              "min        -4.500000  \n",
              "25%        -0.500000  \n",
              "50%         0.000000  \n",
              "75%         0.500000  \n",
              "max         7.750000  \n",
              "\n",
              "[8 rows x 141 columns]"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Network_TotalTransmittedPackets</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.0</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.212761</td>\n",
              "      <td>0.398628</td>\n",
              "      <td>0.186851</td>\n",
              "      <td>0.265664</td>\n",
              "      <td>-0.069490</td>\n",
              "      <td>0.367222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.120055</td>\n",
              "      <td>0.080438</td>\n",
              "      <td>0.234965</td>\n",
              "      <td>...</td>\n",
              "      <td>3.576166</td>\n",
              "      <td>0.198101</td>\n",
              "      <td>0.723235</td>\n",
              "      <td>0.155546</td>\n",
              "      <td>0.181353</td>\n",
              "      <td>0.154609</td>\n",
              "      <td>0.148062</td>\n",
              "      <td>0.155836</td>\n",
              "      <td>0.175938</td>\n",
              "      <td>0.042455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.795318</td>\n",
              "      <td>0.802443</td>\n",
              "      <td>1.654029</td>\n",
              "      <td>1.276187</td>\n",
              "      <td>0.709299</td>\n",
              "      <td>0.803815</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.823925</td>\n",
              "      <td>0.863950</td>\n",
              "      <td>1.033519</td>\n",
              "      <td>...</td>\n",
              "      <td>29.274291</td>\n",
              "      <td>0.759329</td>\n",
              "      <td>0.981457</td>\n",
              "      <td>1.162656</td>\n",
              "      <td>5.378467</td>\n",
              "      <td>1.184157</td>\n",
              "      <td>1.054906</td>\n",
              "      <td>1.039132</td>\n",
              "      <td>3.085652</td>\n",
              "      <td>0.724104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.237610</td>\n",
              "      <td>-0.395372</td>\n",
              "      <td>-6.119205</td>\n",
              "      <td>-1.774694</td>\n",
              "      <td>-3.812835</td>\n",
              "      <td>-0.488183</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.005047</td>\n",
              "      <td>-1.093171</td>\n",
              "      <td>-1.075246</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.168675</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.074934</td>\n",
              "      <td>-1.025907</td>\n",
              "      <td>-1.022917</td>\n",
              "      <td>-1.022845</td>\n",
              "      <td>-1.036278</td>\n",
              "      <td>-2.200252</td>\n",
              "      <td>-4.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.330219</td>\n",
              "      <td>-0.148296</td>\n",
              "      <td>-0.317881</td>\n",
              "      <td>-0.404866</td>\n",
              "      <td>-0.575080</td>\n",
              "      <td>-0.190738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.456217</td>\n",
              "      <td>-0.483719</td>\n",
              "      <td>-0.378340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.168675</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.466491</td>\n",
              "      <td>-0.477720</td>\n",
              "      <td>-0.465104</td>\n",
              "      <td>-0.475078</td>\n",
              "      <td>-0.456887</td>\n",
              "      <td>-0.435513</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.669781</td>\n",
              "      <td>0.851704</td>\n",
              "      <td>0.682119</td>\n",
              "      <td>0.595134</td>\n",
              "      <td>0.424920</td>\n",
              "      <td>0.809262</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.543783</td>\n",
              "      <td>0.516281</td>\n",
              "      <td>0.621660</td>\n",
              "      <td>...</td>\n",
              "      <td>0.831325</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.533509</td>\n",
              "      <td>0.522280</td>\n",
              "      <td>0.534896</td>\n",
              "      <td>0.524922</td>\n",
              "      <td>0.543113</td>\n",
              "      <td>0.564487</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>16.627478</td>\n",
              "      <td>6.220702</td>\n",
              "      <td>49.145695</td>\n",
              "      <td>32.878780</td>\n",
              "      <td>4.201645</td>\n",
              "      <td>6.096682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.988530</td>\n",
              "      <td>22.206628</td>\n",
              "      <td>29.732771</td>\n",
              "      <td>...</td>\n",
              "      <td>689.574297</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>114.564644</td>\n",
              "      <td>1069.443523</td>\n",
              "      <td>72.921875</td>\n",
              "      <td>45.400312</td>\n",
              "      <td>53.026288</td>\n",
              "      <td>575.846800</td>\n",
              "      <td>7.750000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "count     42751.000000     42751.000000        42751.000000   \n",
              "mean          0.212761         0.398628            0.186851   \n",
              "std           0.795318         0.802443            1.654029   \n",
              "min          -1.237610        -0.395372           -6.119205   \n",
              "25%          -0.330219        -0.148296           -0.317881   \n",
              "50%           0.000000         0.000000            0.000000   \n",
              "75%           0.669781         0.851704            0.682119   \n",
              "max          16.627478         6.220702           49.145695   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "count         42751.000000        42751.000000         42751.000000   \n",
              "mean              0.265664           -0.069490             0.367222   \n",
              "std               1.276187            0.709299             0.803815   \n",
              "min              -1.774694           -3.812835            -0.488183   \n",
              "25%              -0.404866           -0.575080            -0.190738   \n",
              "50%               0.000000            0.000000             0.000000   \n",
              "75%               0.595134            0.424920             0.809262   \n",
              "max              32.878780            4.201645             6.096682   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "count              42751.0     42751.000000      42751.000000   \n",
              "mean                   0.0         0.120055          0.080438   \n",
              "std                    0.0         0.823925          0.863950   \n",
              "min                    0.0        -1.005047         -1.093171   \n",
              "25%                    0.0        -0.456217         -0.483719   \n",
              "50%                    0.0         0.000000          0.000000   \n",
              "75%                    0.0         0.543783          0.516281   \n",
              "max                    0.0        18.988530         22.206628   \n",
              "\n",
              "       Memory_HeapFree  ...  Network_TotalTransmittedPackets  \\\n",
              "count     42751.000000  ...                     42751.000000   \n",
              "mean          0.234965  ...                         3.576166   \n",
              "std           1.033519  ...                        29.274291   \n",
              "min          -1.075246  ...                        -0.168675   \n",
              "25%          -0.378340  ...                        -0.168675   \n",
              "50%           0.000000  ...                         0.000000   \n",
              "75%           0.621660  ...                         0.831325   \n",
              "max          29.732771  ...                       689.574297   \n",
              "\n",
              "       Battery_wakelock  Battery_service   Logcat_info  Logcat_error  \\\n",
              "count      42751.000000     42751.000000  42751.000000  42751.000000   \n",
              "mean           0.198101         0.723235      0.155546      0.181353   \n",
              "std            0.759329         0.981457      1.162656      5.378467   \n",
              "min           -1.500000         0.000000     -1.074934     -1.025907   \n",
              "25%           -0.500000         0.000000     -0.466491     -0.477720   \n",
              "50%            0.000000         0.000000      0.000000      0.000000   \n",
              "75%            0.500000         1.000000      0.533509      0.522280   \n",
              "max            4.500000        10.000000    114.564644   1069.443523   \n",
              "\n",
              "       Logcat_warning  Logcat_debug  Logcat_verbose  Logcat_total  \\\n",
              "count    42751.000000  42751.000000    42751.000000  42751.000000   \n",
              "mean         0.154609      0.148062        0.155836      0.175938   \n",
              "std          1.184157      1.054906        1.039132      3.085652   \n",
              "min         -1.022917     -1.022845       -1.036278     -2.200252   \n",
              "25%         -0.465104     -0.475078       -0.456887     -0.435513   \n",
              "50%          0.000000      0.000000        0.000000      0.000000   \n",
              "75%          0.534896      0.524922        0.543113      0.564487   \n",
              "max         72.921875     45.400312       53.026288    575.846800   \n",
              "\n",
              "       Process_total  \n",
              "count   42751.000000  \n",
              "mean        0.042455  \n",
              "std         0.724104  \n",
              "min        -4.500000  \n",
              "25%        -0.500000  \n",
              "50%         0.000000  \n",
              "75%         0.500000  \n",
              "max         7.750000  \n",
              "\n",
              "[8 rows x 141 columns]"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Network_TotalTransmittedPackets</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.0</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>4.275100e+04</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>42751.000000</td>\n",
              "      <td>4.275100e+04</td>\n",
              "      <td>42751.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>70944.886272</td>\n",
              "      <td>12764.341700</td>\n",
              "      <td>10844.857898</td>\n",
              "      <td>47646.187738</td>\n",
              "      <td>88029.087694</td>\n",
              "      <td>14333.172277</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22278.685996</td>\n",
              "      <td>17539.452434</td>\n",
              "      <td>4738.241304</td>\n",
              "      <td>...</td>\n",
              "      <td>466.232720</td>\n",
              "      <td>3.396201</td>\n",
              "      <td>0.723235</td>\n",
              "      <td>2337.759865</td>\n",
              "      <td>2.370012e+03</td>\n",
              "      <td>2296.848963</td>\n",
              "      <td>2300.166686</td>\n",
              "      <td>2300.399780</td>\n",
              "      <td>1.160519e+04</td>\n",
              "      <td>190.169821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>36266.506343</td>\n",
              "      <td>12900.070827</td>\n",
              "      <td>999.033790</td>\n",
              "      <td>29793.858149</td>\n",
              "      <td>15871.265926</td>\n",
              "      <td>13468.721351</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12570.621975</td>\n",
              "      <td>10519.885159</td>\n",
              "      <td>2939.327428</td>\n",
              "      <td>...</td>\n",
              "      <td>3644.649287</td>\n",
              "      <td>1.518657</td>\n",
              "      <td>0.981457</td>\n",
              "      <td>2203.232434</td>\n",
              "      <td>1.038044e+04</td>\n",
              "      <td>2273.580562</td>\n",
              "      <td>2031.749201</td>\n",
              "      <td>1976.428885</td>\n",
              "      <td>1.101732e+04</td>\n",
              "      <td>2.896415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4808.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7036.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4268.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5113.000000</td>\n",
              "      <td>3249.000000</td>\n",
              "      <td>1012.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000e+01</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>3.121000e+03</td>\n",
              "      <td>172.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>46185.000000</td>\n",
              "      <td>3972.000000</td>\n",
              "      <td>10540.000000</td>\n",
              "      <td>31992.000000</td>\n",
              "      <td>76716.000000</td>\n",
              "      <td>4984.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13486.500000</td>\n",
              "      <td>10670.000000</td>\n",
              "      <td>2994.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1159.000000</td>\n",
              "      <td>1.098000e+03</td>\n",
              "      <td>1107.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>1135.000000</td>\n",
              "      <td>9.422000e+03</td>\n",
              "      <td>188.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61243.000000</td>\n",
              "      <td>6356.000000</td>\n",
              "      <td>10732.000000</td>\n",
              "      <td>41444.000000</td>\n",
              "      <td>89584.000000</td>\n",
              "      <td>8180.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20447.000000</td>\n",
              "      <td>16560.000000</td>\n",
              "      <td>4070.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2043.000000</td>\n",
              "      <td>2.020000e+03</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>1.097700e+04</td>\n",
              "      <td>190.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>91785.000000</td>\n",
              "      <td>20048.000000</td>\n",
              "      <td>11144.000000</td>\n",
              "      <td>55338.000000</td>\n",
              "      <td>99092.000000</td>\n",
              "      <td>21740.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28743.500000</td>\n",
              "      <td>22846.500000</td>\n",
              "      <td>5838.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>124.500000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3054.000000</td>\n",
              "      <td>3.028000e+03</td>\n",
              "      <td>3027.000000</td>\n",
              "      <td>3026.000000</td>\n",
              "      <td>3037.000000</td>\n",
              "      <td>1.299250e+04</td>\n",
              "      <td>192.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>819456.000000</td>\n",
              "      <td>106360.000000</td>\n",
              "      <td>40416.000000</td>\n",
              "      <td>809032.000000</td>\n",
              "      <td>183600.000000</td>\n",
              "      <td>110336.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>310155.000000</td>\n",
              "      <td>286959.000000</td>\n",
              "      <td>88630.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>85873.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>219143.000000</td>\n",
              "      <td>2.066046e+06</td>\n",
              "      <td>142010.000000</td>\n",
              "      <td>89456.000000</td>\n",
              "      <td>102860.000000</td>\n",
              "      <td>2.067038e+06</td>\n",
              "      <td>221.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 141 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "count     42751.000000     42751.000000        42751.000000   \n",
              "mean      70944.886272     12764.341700        10844.857898   \n",
              "std       36266.506343     12900.070827          999.033790   \n",
              "min        4808.000000         0.000000         7036.000000   \n",
              "25%       46185.000000      3972.000000        10540.000000   \n",
              "50%       61243.000000      6356.000000        10732.000000   \n",
              "75%       91785.000000     20048.000000        11144.000000   \n",
              "max      819456.000000    106360.000000        40416.000000   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "count         42751.000000        42751.000000         42751.000000   \n",
              "mean          47646.187738        88029.087694         14333.172277   \n",
              "std           29793.858149        15871.265926         13468.721351   \n",
              "min              12.000000         4268.000000             0.000000   \n",
              "25%           31992.000000        76716.000000          4984.000000   \n",
              "50%           41444.000000        89584.000000          8180.000000   \n",
              "75%           55338.000000        99092.000000         21740.000000   \n",
              "max          809032.000000       183600.000000        110336.000000   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "count              42751.0     42751.000000      42751.000000   \n",
              "mean                   0.0     22278.685996      17539.452434   \n",
              "std                    0.0     12570.621975      10519.885159   \n",
              "min                    0.0      5113.000000       3249.000000   \n",
              "25%                    0.0     13486.500000      10670.000000   \n",
              "50%                    0.0     20447.000000      16560.000000   \n",
              "75%                    0.0     28743.500000      22846.500000   \n",
              "max                    0.0    310155.000000     286959.000000   \n",
              "\n",
              "       Memory_HeapFree  ...  Network_TotalTransmittedPackets  \\\n",
              "count     42751.000000  ...                     42751.000000   \n",
              "mean       4738.241304  ...                       466.232720   \n",
              "std        2939.327428  ...                      3644.649287   \n",
              "min        1012.000000  ...                         0.000000   \n",
              "25%        2994.000000  ...                         0.000000   \n",
              "50%        4070.000000  ...                        21.000000   \n",
              "75%        5838.000000  ...                       124.500000   \n",
              "max       88630.000000  ...                     85873.000000   \n",
              "\n",
              "       Battery_wakelock  Battery_service    Logcat_info  Logcat_error  \\\n",
              "count      42751.000000     42751.000000   42751.000000  4.275100e+04   \n",
              "mean           3.396201         0.723235    2337.759865  2.370012e+03   \n",
              "std            1.518657         0.981457    2203.232434  1.038044e+04   \n",
              "min            0.000000         0.000000       6.000000  4.000000e+01   \n",
              "25%            2.000000         0.000000    1159.000000  1.098000e+03   \n",
              "50%            3.000000         0.000000    2043.000000  2.020000e+03   \n",
              "75%            4.000000         1.000000    3054.000000  3.028000e+03   \n",
              "max           12.000000        10.000000  219143.000000  2.066046e+06   \n",
              "\n",
              "       Logcat_warning  Logcat_debug  Logcat_verbose  Logcat_total  \\\n",
              "count    42751.000000  42751.000000    42751.000000  4.275100e+04   \n",
              "mean      2296.848963   2300.166686     2300.399780  1.160519e+04   \n",
              "std       2273.580562   2031.749201     1976.428885  1.101732e+04   \n",
              "min         36.000000     45.000000       33.000000  3.121000e+03   \n",
              "25%       1107.000000   1100.000000     1135.000000  9.422000e+03   \n",
              "50%       2000.000000   2015.000000     2004.000000  1.097700e+04   \n",
              "75%       3027.000000   3026.000000     3037.000000  1.299250e+04   \n",
              "max     142010.000000  89456.000000   102860.000000  2.067038e+06   \n",
              "\n",
              "       Process_total  \n",
              "count   42751.000000  \n",
              "mean      190.169821  \n",
              "std         2.896415  \n",
              "min       172.000000  \n",
              "25%       188.000000  \n",
              "50%       190.000000  \n",
              "75%       192.000000  \n",
              "max       221.000000  \n",
              "\n",
              "[8 rows x 141 columns]"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is not updated (as expected)\n",
        "baseline_pipeline.dataset.X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outliers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.preprocessing.get_outliers_df(plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. FEATURE ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will take adjust the features that compose the learning inputs to our model. The correctness of this section is pivotal for proper learning by the model\n",
        "\n",
        "\n",
        "## FEATURE ENGINEERING\n",
        "- Domain-specific features\n",
        "- Binning\n",
        "- Interaction terms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "featuresToEncode = [\"Reboot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline logistic in category baseline has executed feature_analysis.feature_transformation.get_categorical_features_encoded. Result is: {'Adware': 0, 'Backdoor': 1, 'FileInfector': 2, 'No_Category': 3, 'PUA': 4, 'Ransomware': 5, 'Riskware': 6, 'Scareware': 7, 'Trojan': 8, 'Trojan_Banker': 9, 'Trojan_Dropper': 10, 'Trojan_SMS': 11, 'Trojan_Spy': 12, 'Zero_Day': 13}\n",
            "Pipeline ensembled in category models has executed feature_analysis.feature_transformation.get_categorical_features_encoded. Result is: {'Adware': 0, 'Backdoor': 1, 'FileInfector': 2, 'No_Category': 3, 'PUA': 4, 'Ransomware': 5, 'Riskware': 6, 'Scareware': 7, 'Trojan': 8, 'Trojan_Banker': 9, 'Trojan_Dropper': 10, 'Trojan_SMS': 11, 'Trojan_Spy': 12, 'Zero_Day': 13}\n"
          ]
        }
      ],
      "source": [
        "encoded_maps_perPipeline = pipeline_manager.all_pipelines_execute(methodName=\"feature_analysis.feature_transformation.get_categorical_features_encoded\", verbose=True, features=featuresToEncode, encode_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets visualize the results of the encoding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "      <th>Reboot_before</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48143</th>\n",
              "      <td>-0.572982</td>\n",
              "      <td>-0.212739</td>\n",
              "      <td>3.662252</td>\n",
              "      <td>-0.680374</td>\n",
              "      <td>-0.269575</td>\n",
              "      <td>-0.304846</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.744642</td>\n",
              "      <td>-0.903544</td>\n",
              "      <td>-0.190928</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.229024</td>\n",
              "      <td>0.414508</td>\n",
              "      <td>-0.328646</td>\n",
              "      <td>-0.232606</td>\n",
              "      <td>1.436909</td>\n",
              "      <td>1.088923</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10388</th>\n",
              "      <td>0.223136</td>\n",
              "      <td>0.310774</td>\n",
              "      <td>-2.933775</td>\n",
              "      <td>0.225135</td>\n",
              "      <td>-0.211834</td>\n",
              "      <td>0.196228</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.345350</td>\n",
              "      <td>-0.408902</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.712401</td>\n",
              "      <td>-0.995337</td>\n",
              "      <td>0.449479</td>\n",
              "      <td>-0.162513</td>\n",
              "      <td>0.420610</td>\n",
              "      <td>-0.788685</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11029</th>\n",
              "      <td>1.298399</td>\n",
              "      <td>1.525006</td>\n",
              "      <td>-0.046358</td>\n",
              "      <td>1.288786</td>\n",
              "      <td>1.902038</td>\n",
              "      <td>1.401289</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.092089</td>\n",
              "      <td>1.166345</td>\n",
              "      <td>0.800281</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.486528</td>\n",
              "      <td>-0.559896</td>\n",
              "      <td>-0.995327</td>\n",
              "      <td>0.742376</td>\n",
              "      <td>-0.324044</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36609</th>\n",
              "      <td>-0.407325</td>\n",
              "      <td>-0.136850</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>-0.495674</td>\n",
              "      <td>0.071505</td>\n",
              "      <td>-0.239198</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.469555</td>\n",
              "      <td>-0.483883</td>\n",
              "      <td>-0.511955</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.147757</td>\n",
              "      <td>-0.969430</td>\n",
              "      <td>-0.633854</td>\n",
              "      <td>-0.031672</td>\n",
              "      <td>0.475289</td>\n",
              "      <td>-0.801008</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26413</th>\n",
              "      <td>0.097281</td>\n",
              "      <td>-0.192834</td>\n",
              "      <td>0.099338</td>\n",
              "      <td>0.481453</td>\n",
              "      <td>0.522524</td>\n",
              "      <td>-0.129864</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.760766</td>\n",
              "      <td>0.679752</td>\n",
              "      <td>1.106188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.120317</td>\n",
              "      <td>2.872539</td>\n",
              "      <td>-0.715625</td>\n",
              "      <td>-0.098131</td>\n",
              "      <td>-0.980547</td>\n",
              "      <td>0.405826</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42697</th>\n",
              "      <td>-0.562982</td>\n",
              "      <td>-0.382433</td>\n",
              "      <td>0.543046</td>\n",
              "      <td>-0.389446</td>\n",
              "      <td>-1.457991</td>\n",
              "      <td>-0.475531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.748509</td>\n",
              "      <td>-0.895085</td>\n",
              "      <td>-0.247890</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.127704</td>\n",
              "      <td>-0.263212</td>\n",
              "      <td>-0.177083</td>\n",
              "      <td>-0.367082</td>\n",
              "      <td>-0.841220</td>\n",
              "      <td>-1.202073</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36008</th>\n",
              "      <td>-0.192939</td>\n",
              "      <td>-0.072157</td>\n",
              "      <td>2.847682</td>\n",
              "      <td>-0.232845</td>\n",
              "      <td>0.407222</td>\n",
              "      <td>-0.058009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.160779</td>\n",
              "      <td>-0.123270</td>\n",
              "      <td>-0.399437</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.018470</td>\n",
              "      <td>0.315026</td>\n",
              "      <td>-0.623958</td>\n",
              "      <td>0.314642</td>\n",
              "      <td>2.321241</td>\n",
              "      <td>0.449797</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46265</th>\n",
              "      <td>-0.526820</td>\n",
              "      <td>-0.179895</td>\n",
              "      <td>3.112583</td>\n",
              "      <td>-0.588366</td>\n",
              "      <td>-0.299428</td>\n",
              "      <td>-0.280019</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.681523</td>\n",
              "      <td>-0.782819</td>\n",
              "      <td>-0.369198</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.812137</td>\n",
              "      <td>1.255440</td>\n",
              "      <td>-0.403646</td>\n",
              "      <td>-0.323988</td>\n",
              "      <td>0.519453</td>\n",
              "      <td>0.743873</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23587</th>\n",
              "      <td>1.155789</td>\n",
              "      <td>2.092560</td>\n",
              "      <td>0.198675</td>\n",
              "      <td>0.827551</td>\n",
              "      <td>0.464247</td>\n",
              "      <td>1.995942</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.539293</td>\n",
              "      <td>1.188190</td>\n",
              "      <td>3.105837</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.589446</td>\n",
              "      <td>0.521762</td>\n",
              "      <td>-0.255208</td>\n",
              "      <td>-0.055556</td>\n",
              "      <td>1.700315</td>\n",
              "      <td>0.457079</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29313</th>\n",
              "      <td>0.400395</td>\n",
              "      <td>-0.054740</td>\n",
              "      <td>-0.218543</td>\n",
              "      <td>1.075645</td>\n",
              "      <td>0.225778</td>\n",
              "      <td>-0.155168</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.471062</td>\n",
              "      <td>0.339014</td>\n",
              "      <td>1.010900</td>\n",
              "      <td>...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.645383</td>\n",
              "      <td>-0.991710</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>0.774143</td>\n",
              "      <td>0.313880</td>\n",
              "      <td>-0.543621</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42751 rows × 142 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "48143        -0.572982        -0.212739            3.662252   \n",
              "10388         0.223136         0.310774           -2.933775   \n",
              "11029         1.298399         1.525006           -0.046358   \n",
              "36609        -0.407325        -0.136850            3.000000   \n",
              "26413         0.097281        -0.192834            0.099338   \n",
              "...                ...              ...                 ...   \n",
              "42697        -0.562982        -0.382433            0.543046   \n",
              "36008        -0.192939        -0.072157            2.847682   \n",
              "46265        -0.526820        -0.179895            3.112583   \n",
              "23587         1.155789         2.092560            0.198675   \n",
              "29313         0.400395        -0.054740           -0.218543   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "48143            -0.680374           -0.269575            -0.304846   \n",
              "10388             0.225135           -0.211834             0.196228   \n",
              "11029             1.288786            1.902038             1.401289   \n",
              "36609            -0.495674            0.071505            -0.239198   \n",
              "26413             0.481453            0.522524            -0.129864   \n",
              "...                    ...                 ...                  ...   \n",
              "42697            -0.389446           -1.457991            -0.475531   \n",
              "36008            -0.232845            0.407222            -0.058009   \n",
              "46265            -0.588366           -0.299428            -0.280019   \n",
              "23587             0.827551            0.464247             1.995942   \n",
              "29313             1.075645            0.225778            -0.155168   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "48143                  0.0        -0.744642         -0.903544   \n",
              "10388                  0.0        -0.345350         -0.408902   \n",
              "11029                  0.0         1.092089          1.166345   \n",
              "36609                  0.0        -0.469555         -0.483883   \n",
              "26413                  0.0         0.760766          0.679752   \n",
              "...                    ...              ...               ...   \n",
              "42697                  0.0        -0.748509         -0.895085   \n",
              "36008                  0.0        -0.160779         -0.123270   \n",
              "46265                  0.0        -0.681523         -0.782819   \n",
              "23587                  0.0         1.539293          1.188190   \n",
              "29313                  0.0         0.471062          0.339014   \n",
              "\n",
              "       Memory_HeapFree  ...  Battery_wakelock  Battery_service  Logcat_info  \\\n",
              "48143        -0.190928  ...               0.0              0.0     1.229024   \n",
              "10388        -0.166667  ...              -0.5              2.0    -0.712401   \n",
              "11029         0.800281  ...              -0.5              1.0     0.200000   \n",
              "36609        -0.511955  ...               1.0              2.0     0.147757   \n",
              "26413         1.106188  ...               0.0              0.0     0.120317   \n",
              "...                ...  ...               ...              ...          ...   \n",
              "42697        -0.247890  ...               0.0              1.0    -0.127704   \n",
              "36008        -0.399437  ...               0.0              0.0    -1.018470   \n",
              "46265        -0.369198  ...               1.0              1.0     0.812137   \n",
              "23587         3.105837  ...               1.0              0.0    -0.589446   \n",
              "29313         1.010900  ...               2.5              1.0    -0.645383   \n",
              "\n",
              "       Logcat_error  Logcat_warning  Logcat_debug  Logcat_verbose  \\\n",
              "48143      0.414508       -0.328646     -0.232606        1.436909   \n",
              "10388     -0.995337        0.449479     -0.162513        0.420610   \n",
              "11029      0.486528       -0.559896     -0.995327        0.742376   \n",
              "36609     -0.969430       -0.633854     -0.031672        0.475289   \n",
              "26413      2.872539       -0.715625     -0.098131       -0.980547   \n",
              "...             ...             ...           ...             ...   \n",
              "42697     -0.263212       -0.177083     -0.367082       -0.841220   \n",
              "36008      0.315026       -0.623958      0.314642        2.321241   \n",
              "46265      1.255440       -0.403646     -0.323988        0.519453   \n",
              "23587      0.521762       -0.255208     -0.055556        1.700315   \n",
              "29313     -0.991710        0.001563      0.774143        0.313880   \n",
              "\n",
              "       Logcat_total  Process_total  Reboot_before  \n",
              "48143      1.088923          -0.50              1  \n",
              "10388     -0.788685           0.25              0  \n",
              "11029     -0.324044           1.50              0  \n",
              "36609     -0.801008           0.50              0  \n",
              "26413      0.405826           0.50              0  \n",
              "...             ...            ...            ...  \n",
              "42697     -1.202073          -0.25              1  \n",
              "36008      0.449797           0.75              0  \n",
              "46265      0.743873          -0.25              1  \n",
              "23587      0.457079          -0.75              1  \n",
              "29313     -0.543621           0.25              0  \n",
              "\n",
              "[42751 rows x 142 columns]"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48143     8\n",
              "10388     0\n",
              "11029     0\n",
              "36609     6\n",
              "26413    13\n",
              "         ..\n",
              "42697    12\n",
              "36008     6\n",
              "46265     8\n",
              "23587     6\n",
              "29313     8\n",
              "Length: 42751, dtype: int64"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "default_pipeline.dataset.y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Memory_PssTotal</th>\n",
              "      <th>Memory_PssClean</th>\n",
              "      <th>Memory_SharedDirty</th>\n",
              "      <th>Memory_PrivateDirty</th>\n",
              "      <th>Memory_SharedClean</th>\n",
              "      <th>Memory_PrivateClean</th>\n",
              "      <th>Memory_SwapPssDirty</th>\n",
              "      <th>Memory_HeapSize</th>\n",
              "      <th>Memory_HeapAlloc</th>\n",
              "      <th>Memory_HeapFree</th>\n",
              "      <th>...</th>\n",
              "      <th>Battery_wakelock</th>\n",
              "      <th>Battery_service</th>\n",
              "      <th>Logcat_info</th>\n",
              "      <th>Logcat_error</th>\n",
              "      <th>Logcat_warning</th>\n",
              "      <th>Logcat_debug</th>\n",
              "      <th>Logcat_verbose</th>\n",
              "      <th>Logcat_total</th>\n",
              "      <th>Process_total</th>\n",
              "      <th>Reboot_before</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48143</th>\n",
              "      <td>35115</td>\n",
              "      <td>2936</td>\n",
              "      <td>12944</td>\n",
              "      <td>25560</td>\n",
              "      <td>83552</td>\n",
              "      <td>3072</td>\n",
              "      <td>0</td>\n",
              "      <td>9086</td>\n",
              "      <td>5558</td>\n",
              "      <td>3527</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4372</td>\n",
              "      <td>2820</td>\n",
              "      <td>1369</td>\n",
              "      <td>1567</td>\n",
              "      <td>4737</td>\n",
              "      <td>14865</td>\n",
              "      <td>188</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10388</th>\n",
              "      <td>71418</td>\n",
              "      <td>11352</td>\n",
              "      <td>8960</td>\n",
              "      <td>46700</td>\n",
              "      <td>84844</td>\n",
              "      <td>11468</td>\n",
              "      <td>0</td>\n",
              "      <td>15178</td>\n",
              "      <td>11581</td>\n",
              "      <td>3596</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>693</td>\n",
              "      <td>99</td>\n",
              "      <td>2863</td>\n",
              "      <td>1702</td>\n",
              "      <td>2804</td>\n",
              "      <td>8161</td>\n",
              "      <td>191</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11029</th>\n",
              "      <td>120450</td>\n",
              "      <td>30872</td>\n",
              "      <td>10704</td>\n",
              "      <td>71532</td>\n",
              "      <td>132144</td>\n",
              "      <td>31660</td>\n",
              "      <td>0</td>\n",
              "      <td>37109</td>\n",
              "      <td>30762</td>\n",
              "      <td>6346</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2422</td>\n",
              "      <td>2959</td>\n",
              "      <td>925</td>\n",
              "      <td>98</td>\n",
              "      <td>3416</td>\n",
              "      <td>9820</td>\n",
              "      <td>196</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36609</th>\n",
              "      <td>42669</td>\n",
              "      <td>4156</td>\n",
              "      <td>12544</td>\n",
              "      <td>29872</td>\n",
              "      <td>91184</td>\n",
              "      <td>4172</td>\n",
              "      <td>0</td>\n",
              "      <td>13283</td>\n",
              "      <td>10668</td>\n",
              "      <td>2614</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2323</td>\n",
              "      <td>149</td>\n",
              "      <td>783</td>\n",
              "      <td>1954</td>\n",
              "      <td>2908</td>\n",
              "      <td>8117</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26413</th>\n",
              "      <td>65679</td>\n",
              "      <td>3256</td>\n",
              "      <td>10792</td>\n",
              "      <td>52684</td>\n",
              "      <td>101276</td>\n",
              "      <td>6004</td>\n",
              "      <td>0</td>\n",
              "      <td>32054</td>\n",
              "      <td>24837</td>\n",
              "      <td>7216</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2271</td>\n",
              "      <td>7564</td>\n",
              "      <td>626</td>\n",
              "      <td>1826</td>\n",
              "      <td>139</td>\n",
              "      <td>12426</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42697</th>\n",
              "      <td>35571</td>\n",
              "      <td>208</td>\n",
              "      <td>11060</td>\n",
              "      <td>32352</td>\n",
              "      <td>56960</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>9027</td>\n",
              "      <td>5661</td>\n",
              "      <td>3365</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1801</td>\n",
              "      <td>1512</td>\n",
              "      <td>1660</td>\n",
              "      <td>1308</td>\n",
              "      <td>404</td>\n",
              "      <td>6685</td>\n",
              "      <td>189</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36008</th>\n",
              "      <td>52445</td>\n",
              "      <td>5196</td>\n",
              "      <td>12452</td>\n",
              "      <td>36008</td>\n",
              "      <td>98696</td>\n",
              "      <td>7208</td>\n",
              "      <td>0</td>\n",
              "      <td>17994</td>\n",
              "      <td>15059</td>\n",
              "      <td>2934</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>2628</td>\n",
              "      <td>802</td>\n",
              "      <td>2621</td>\n",
              "      <td>6419</td>\n",
              "      <td>12583</td>\n",
              "      <td>193</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46265</th>\n",
              "      <td>37220</td>\n",
              "      <td>3464</td>\n",
              "      <td>12612</td>\n",
              "      <td>27708</td>\n",
              "      <td>82884</td>\n",
              "      <td>3488</td>\n",
              "      <td>0</td>\n",
              "      <td>10049</td>\n",
              "      <td>7028</td>\n",
              "      <td>3020</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3582</td>\n",
              "      <td>4443</td>\n",
              "      <td>1225</td>\n",
              "      <td>1391</td>\n",
              "      <td>2992</td>\n",
              "      <td>13633</td>\n",
              "      <td>189</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23587</th>\n",
              "      <td>113947</td>\n",
              "      <td>39996</td>\n",
              "      <td>10852</td>\n",
              "      <td>60764</td>\n",
              "      <td>99972</td>\n",
              "      <td>41624</td>\n",
              "      <td>0</td>\n",
              "      <td>43932</td>\n",
              "      <td>31028</td>\n",
              "      <td>12903</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>926</td>\n",
              "      <td>3027</td>\n",
              "      <td>1510</td>\n",
              "      <td>1908</td>\n",
              "      <td>5238</td>\n",
              "      <td>12609</td>\n",
              "      <td>187</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29313</th>\n",
              "      <td>79501</td>\n",
              "      <td>5476</td>\n",
              "      <td>10600</td>\n",
              "      <td>66556</td>\n",
              "      <td>94636</td>\n",
              "      <td>5580</td>\n",
              "      <td>0</td>\n",
              "      <td>27634</td>\n",
              "      <td>20688</td>\n",
              "      <td>6945</td>\n",
              "      <td>...</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>820</td>\n",
              "      <td>106</td>\n",
              "      <td>2003</td>\n",
              "      <td>3506</td>\n",
              "      <td>2601</td>\n",
              "      <td>9036</td>\n",
              "      <td>191</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>42751 rows × 142 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Memory_PssTotal  Memory_PssClean  Memory_SharedDirty  \\\n",
              "48143            35115             2936               12944   \n",
              "10388            71418            11352                8960   \n",
              "11029           120450            30872               10704   \n",
              "36609            42669             4156               12544   \n",
              "26413            65679             3256               10792   \n",
              "...                ...              ...                 ...   \n",
              "42697            35571              208               11060   \n",
              "36008            52445             5196               12452   \n",
              "46265            37220             3464               12612   \n",
              "23587           113947            39996               10852   \n",
              "29313            79501             5476               10600   \n",
              "\n",
              "       Memory_PrivateDirty  Memory_SharedClean  Memory_PrivateClean  \\\n",
              "48143                25560               83552                 3072   \n",
              "10388                46700               84844                11468   \n",
              "11029                71532              132144                31660   \n",
              "36609                29872               91184                 4172   \n",
              "26413                52684              101276                 6004   \n",
              "...                    ...                 ...                  ...   \n",
              "42697                32352               56960                  212   \n",
              "36008                36008               98696                 7208   \n",
              "46265                27708               82884                 3488   \n",
              "23587                60764               99972                41624   \n",
              "29313                66556               94636                 5580   \n",
              "\n",
              "       Memory_SwapPssDirty  Memory_HeapSize  Memory_HeapAlloc  \\\n",
              "48143                    0             9086              5558   \n",
              "10388                    0            15178             11581   \n",
              "11029                    0            37109             30762   \n",
              "36609                    0            13283             10668   \n",
              "26413                    0            32054             24837   \n",
              "...                    ...              ...               ...   \n",
              "42697                    0             9027              5661   \n",
              "36008                    0            17994             15059   \n",
              "46265                    0            10049              7028   \n",
              "23587                    0            43932             31028   \n",
              "29313                    0            27634             20688   \n",
              "\n",
              "       Memory_HeapFree  ...  Battery_wakelock  Battery_service  Logcat_info  \\\n",
              "48143             3527  ...                 3                0         4372   \n",
              "10388             3596  ...                 2                2          693   \n",
              "11029             6346  ...                 2                1         2422   \n",
              "36609             2614  ...                 5                2         2323   \n",
              "26413             7216  ...                 3                0         2271   \n",
              "...                ...  ...               ...              ...          ...   \n",
              "42697             3365  ...                 3                1         1801   \n",
              "36008             2934  ...                 3                0          113   \n",
              "46265             3020  ...                 5                1         3582   \n",
              "23587            12903  ...                 5                0          926   \n",
              "29313             6945  ...                 8                1          820   \n",
              "\n",
              "       Logcat_error  Logcat_warning  Logcat_debug  Logcat_verbose  \\\n",
              "48143          2820            1369          1567            4737   \n",
              "10388            99            2863          1702            2804   \n",
              "11029          2959             925            98            3416   \n",
              "36609           149             783          1954            2908   \n",
              "26413          7564             626          1826             139   \n",
              "...             ...             ...           ...             ...   \n",
              "42697          1512            1660          1308             404   \n",
              "36008          2628             802          2621            6419   \n",
              "46265          4443            1225          1391            2992   \n",
              "23587          3027            1510          1908            5238   \n",
              "29313           106            2003          3506            2601   \n",
              "\n",
              "       Logcat_total  Process_total  Reboot_before  \n",
              "48143         14865            188              1  \n",
              "10388          8161            191              0  \n",
              "11029          9820            196              0  \n",
              "36609          8117            192              0  \n",
              "26413         12426            192              0  \n",
              "...             ...            ...            ...  \n",
              "42697          6685            189              1  \n",
              "36008         12583            193              0  \n",
              "46265         13633            189              1  \n",
              "23587         12609            187              1  \n",
              "29313          9036            191              0  \n",
              "\n",
              "[42751 rows x 142 columns]"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_pipeline.dataset.X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "48143     8\n",
              "10388     0\n",
              "11029     0\n",
              "36609     6\n",
              "26413    13\n",
              "         ..\n",
              "42697    12\n",
              "36008     6\n",
              "46265     8\n",
              "23587     6\n",
              "29313     8\n",
              "Length: 42751, dtype: int64"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "baseline_pipeline.dataset.y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FEATURE SELECTION\n",
        "- Analyze correlation and low-variances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection \n",
        "As explained before, we now proceed to carefully reduce the present high-dimensionality. High-dimensionality increases the chances of the model overfitting (capturing noise from irrelevant features; increasing variance and reducing bias), as well as introducing a significant computational overhead. We match this high-dimensionality with a highly filtering models_pipeline of feature selection.\n",
        "The most extensive cut comes given at the first level, with the mutual information threshold-based cut. This metric captures the level of uncertainity between the feature and the target variable (cnt). In marked contrast with pearson coefficeint (correlation), it is able to model non-linear and linea relationships altoghther. \n",
        "This feature-selectin models_pipeline is compromised of five (3 as of the final models_pipeline) cuts:\n",
        " - mutual information\n",
        " - low variance\n",
        " - multicolinearity analysis\n",
        " - PCA\n",
        " - Boruta and/or Lasso\n",
        "\n",
        "The different thresholds for each of this cuts have been altered over the different models_pipeline iterations. Specifically, in the bias-variance tradeoff (to be elaborated in further detail later), I increased all the thresholds in order to avoid por performance.\n",
        "\n",
        "#### Feature to target variable mutual information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"MutualInformation\", threshold=.2, delete_features=True, plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Eliminating low-variances features\n",
        "Features with low variances provide little new information for the model to learn from, thus they could introduce statistical noise. Due to this reason, they should be elimanted from the dataset. The reason why we don't focus on high-variance is because this symbolizes outliers, which have been dealt with before in data preporcesing.\n",
        "This function call eliminate low-variance (based on threshold) and all cosntant variables (regardless of threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start off this analysis eliminating univariate features (i.e: the featuers with constant values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "# default_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"LowVariances\", \n",
        "#                                                                          threshold=0.5, \n",
        "#                                                                          delete_features=True, \n",
        "#                                                                          plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As explained in scikit-learn's in (1.13.1 Feature selection)[https://scikit-learn.org/stable/modules/feature_selection.html], the variance threshold must be selected carefully. Too low may delete few variables, and too may be too restrictive, deleting more variables than it should."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Eliminating highly correlated feature \n",
        "Highly correlated variables (multicolinearity) are problem for models because they introduce a redundancy (features that contain significantly related similar information are not bringing much new insight into the model's input) to the model that can introduce significant variance. This is due to the fact that small changes in the data may make the coefficeints of the highly correlated variables **swing** more than it should\n",
        "\n",
        "A note on the shape of this heatmap: due to the high amount of features, and the redundancy to measure the correlation between features (where corr(A,B) = corr(B,A)) we set 'np.triu(np.ones_like(corr, dtype=bool))' in the utilities functions in order to show only new non-redudant correlations between features, thus the right triangle shape.\n",
        "Well, thats a lot to digest! We can see some solid red (high positive correlation) and medium-solid blue (some high negative correlation). \n",
        "Lets use a non-visual methodology to confirm our initial hypothesis. We will use variance inflaction factor (VIF) along with checking manually.\n",
        "A brief explanation on VIF. \n",
        "Formula: \n",
        "$$\n",
        "VIF_i = \\frac{1}{1 - R_i^2}\n",
        "$$\n",
        "You regress (i.e: do linear regression) on the ith feature as target variable, and all other features as predictors. You then compute the coefficient of determination as a way to measure how well the predictors fit the target variable. VIF values ranges from [1, inf], where lower bound signifies little multicolinearity (R^2 = 0), and upper bound occurs when R^2=1 (perfect multicolineairty). 5 is considered a standard threshold for VIF as it symbolized an 80% R^2.\n",
        "Once you ve obtained the results of VIF, you need to delete the variable with the highest VIF, and recompute VIF until there is no multicolinearity. For example, say there are three features that are 4 features, 3 of them being linear combinations of each other. You would delete the variables with VIF until there is no VIF (when only one of those linear combinations remains). You cant delete all n-1 variables where n are the amount of variables with exceeding (with respect to the chosen threshold), because you may be deleting one that has high VIF due to another feature.\n",
        "\n",
        "A relevant note on why one-hot-encoding must be done dropping the first one:\n",
        "If we were to not remove one of the labels of one hot encoding, you would be able to predict which level of the categorical variable based on all other ones (there is only degree of freedom for categories levels; they are a linear combination). You would essentially see inf VIF in that area and delete it in this section. \n",
        "The VIF has to be computed every time we delete a feature due to high multicolinearity. Lets do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"VIF\", threshold=10, delete_features=True, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA\n",
        "PCA can still bring some more value to feature selection. It will take into account interaction effects by itself and find the principals that capture as much variance as we specify. Thus, its inclusion in the feature selection models_pipeline.\n",
        "It has been excldued due to underperformance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "#models_pipeline.feature_analysis.feature_selection.manual_feature_selection.fit(type=\"PCA\", threshold=.95, delete_features=False, plot=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Automatic Feature Selection\n",
        "#### L1 regularization\n",
        "Because of L1 regularization being able to set weights 0, we can briefly train our logistic regression model with such regulartization and see which features it uses. The reason why it sets 0 to insignificant feature is because the objective function is not only the MSE but added a component of the wieght magnitude (which is trying to minimize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# excluded_features, predictive_features, coefficients = models_pipeline.feature_analysis.feature_selection.automatic_feature_selection.fit(type=\"L1\",\n",
        "#                                                                                                                                     max_iter=1000, \n",
        "#                                                                                                                                     print_results=True, delete_features=False)\n",
        "# excluded_features, len(excluded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BORUTA\n",
        "Boruta is a more powerful feature selection method (thus we use it as a reference for variable deletion). It is more powerful that L1 becuase it compares the importance of features to shuffled versions, ensuring robust feature selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# excluded_features, predictive_features = models_pipeline.feature_analysis.feature_selection.automatic_feature_selection.fit(type=\"Boruta\", max_iter=10, print_results=True, delete_features=False)\n",
        "# excluded_features, len(excluded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome, lets move onto the actual modelling part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QUESTIONS FOR THIS SECTION\n",
        "- Can the ROC curve be used for multiclass?\n",
        "- Can a unsupervised learning algorithm (e.g: KNN) be used for this problem even tough its nature is to be supervised?\n",
        "- Does val to train deltas are meaningful here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TO BE DONE FOR THIS SECTION\n",
        "- Multiple models as classifiers\n",
        "- Learn more about each alogrithms' paremters\n",
        "- Can KNN be used here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest & Decision Trees\n",
        "Instead of the originally planned logistic regression, we will be using an ensembled model first: random forest (collection of week decision trees). This model is not only likely to outperform the original choice because its nature to handle multiclass better, but also does this several orders of magnitude faster. We will add its not ensembled version too, along with gradient boosted machine\n",
        "Note we are not using not-by-default multiclass classifiers (e.g: logistic regressions, svms)\n",
        "\n",
        "## Non-optimized fitting\n",
        "We first fit all these models with the default paramterers. This is done to constrat more starkly the difference between pre and post tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "gradientBoostingModel = GradientBoostingClassifier()\n",
        "randomForestModel = RandomForestClassifier()\n",
        "\n",
        "# Tree-based models\n",
        "decisionTreeModel = DecisionTreeClassifier()\n",
        "\n",
        "# Linear models\n",
        "supportVectorModel = SVC()\n",
        "\n",
        "# Baseline\n",
        "logisticRegressionModel = LogisticRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### MODEL PERFORMANCE, HYPOTHESIS\n",
        "- Trees (e.g: random forest, decision trees):\n",
        "  - Time to fit:\n",
        "    - Ensemble models (random forest) take more time to train due to the fact that they are larger and heavier than their non-ensembled version. \n",
        "  - Correctness:\n",
        "     - High\n",
        "- Binary-classifiers by default models (e.g: SMV, logistic regression)\n",
        "  - Time to fit:\n",
        "    - compute C models for all C number of classes. Each trained to detect a single class, then when we make predictions, we select the ones that has the highest probability in its predictions (\"the most confident in its prediction\"). This strategy is called One-vs-Rest, note however, a single logistic regression may be used if we used the softmax objective function (instead of log-odds) (it still heavy computationally, tough). They will be sloder than ensemble models\n",
        "  - Correctness:\n",
        "     - Very low if the problem is non-linear which it is for the SVM and logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'models': {'ensembled': <library.pipeline.pipeline.Pipeline at 0x1311aa510>,\n",
              "  'tree-based': <library.pipeline.pipeline.Pipeline at 0x1311aa510>,\n",
              "  'linear': <library.pipeline.pipeline.Pipeline at 0x1311aa510>},\n",
              " 'baseline': {'logistic': <library.pipeline.pipeline.Pipeline at 0x1311a8e90>}}"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "ensembled_pipeline = pipeline_manager.create_pipeline_divergence(category=\"models\", pipelineName=\"ensembled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'models': {'ensembled': <library.pipeline.pipeline.Pipeline at 0x131072b10>,\n",
              "  'tree-based': <library.pipeline.pipeline.Pipeline at 0x1311aa510>,\n",
              "  'linear': <library.pipeline.pipeline.Pipeline at 0x1311aa510>},\n",
              " 'baseline': {'logistic': <library.pipeline.pipeline.Pipeline at 0x1311a8e90>}}"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "ensembled_pipeline.model_selection.add_model(\"Gradient Boosting\", gradientBoostingModel)\n",
        "ensembled_pipeline.model_selection.add_model(\"Random Forest\", randomForestModel)\n",
        "\n",
        "# Tree-based models\n",
        "tree_pipeline.model_selection.add_model(\"Decision Tree\", decisionTreeModel)\n",
        "\n",
        "# Linear models\n",
        "linear_pipeline.model_selection.add_model(\"SVM\", supportVectorModel) \n",
        "\n",
        "# Baseline\n",
        "baseline_pipeline.model_selection.add_model(\"Logistic Regression\", logisticRegressionModel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While we debug, lets exlclude some models we dont need for now (they are very slow to train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensembled models\n",
        "ensembled_pipeline.model_selection.models_to_exclude = [\"Gradient Boosting\"]\n",
        "\n",
        "# Tree-based models\n",
        "tree_pipeline.model_selection.models_to_exclude = []\n",
        "\n",
        "# Linear models\n",
        "linear_pipeline.model_selection.models_to_exclude = [\"SVM\"]\n",
        "\n",
        "# Baseline\n",
        "baseline_pipeline.model_selection.models_to_exclude = [\"Logistic Regression\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All models have been fitted and made predictions in parallel.\n",
            "=> Fitting Random Forest model\n",
            "=> Fitting Decision Tree model\n",
            "Sklearn model: RandomForestClassifier()\n",
            "!> Started fitting Random Forest\n",
            "Lenght of X_data: 42751\n",
            "Sklearn model: DecisionTreeClassifier()\n",
            "!> Started fitting Decision Tree\n",
            "Lenght of X_data: 42751\n",
            "\t\t => Fitted Decision Tree. Took 1.9673247337341309 seconds\n",
            "=> Predicting Decision Tree model\n",
            "!> Started predicting Decision Tree\n",
            "\t\t => Predicted Decision Tree. Took 0.004732847213745117 seconds\n",
            "All models have been fitted and made predictions in parallel.\n",
            "\t\t => Fitted Random Forest. Took 10.130177021026611 seconds\n",
            "=> Predicting Random Forest model\n",
            "!> Started predicting Random Forest\n",
            "\t\t => Predicted Random Forest. Took 0.11943292617797852 seconds\n",
            "All models have been fitted and made predictions in parallel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'models': {'tree-based': None, 'ensembled': None},\n",
              " 'baseline': {'logistic': None}}"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_manager.all_pipelines_execute(methodName=\"model_selection.fit_models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's make sure the predictions vary between holdout sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<i> the aforeshown diagram was originally done with the sole intention to debug an error that made predictions be the same across sets. Insights may not be much meaningful after correctio, but it is worth keeping until the end of the notebook development </i>\n",
        "\n",
        "### PREDICTIONS RESULTS \n",
        "Before we get into the actual results, lets elaborate briefly on all the metrics that we are using to asses our classifiers:\n",
        "\n",
        "- Accuracy => total correctly predicted elemetnts (sigma over the moments we predicted x_i and it was actually x_i / number_of_samples)\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\sum_{i} \\mathbf{1}(\\hat{y}_i = y_i)}{N}\n",
        "$$\n",
        "- Precision => out of how many predicted for that class were actually from that class (predicted for class x when it was x/ predicted for class x when it was x + predicted for class x when it was NOT x)\n",
        "$$\n",
        "\\text{Precision}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FP}_x}\n",
        "$$\n",
        "- Recall => out of all cases that were positive how many got predicted correctly?\n",
        "$$\n",
        "\\text{Recall}_x = \\frac{\\text{TP}_x}{\\text{TP}_x + \\text{FN}_x}\n",
        "$$\n",
        "- F1-score => harmonic mean of precision and recall (balances both metrics, heavily penalize spreadness between ratios that are being averaged out)\n",
        "$$\n",
        "\\text{F1}_x = 2 \\times \\frac{\\text{Precision}_x \\times \\text{Recall}_x}{\\text{Precision}_x + \\text{Recall}_x}\n",
        "$$\n",
        "- Support => number of actual occurences of class in the dataset\n",
        "- macro avg => averages given metric across all classes\n",
        "$$\n",
        "\\text{Macro Avg} = \\frac{1}{C} \\sum_{i=1}^{C} M_i\n",
        "$$\n",
        "- weighted avg => averages with weights per class occurence (considers frequency of class in average computation)\n",
        "$$\n",
        "\\text{Weighted Avg} = \\sum_{i=1}^{C} \\frac{\\text{Support}_i}{\\text{Total Instances}} \\times M_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "comments = \"wiLL THIS work?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "phaseProcess={\"is_EDA_done\": False,\n",
        "             \"is_DataPreprocessing_done\": False,\n",
        "              \"is_FeatureAnalysis_done\": False,\n",
        "              \"is_HyperParameterOptimization_done\": False\n",
        "              }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All models have been evaluated.\n",
            "[DEBUG] Starting store_results\n",
            "[DEBUG 0.11s] Completed store_results, processed 0 models\n",
            "METRIC RESULTS FOR Decision Tree => F1: 0.7593815744970357, Precision: 0.7586043655687297, Recall: 0.7604790419161677, Accuracy: 0.7604790419161677\n",
            "Metrics stored in assesment\n",
            "METRIC RESULTS FOR Random Forest => F1: 0.8359066691920427, Precision: 0.8440028457175942, Recall: 0.8415044910179641, Accuracy: 0.8415044910179641\n",
            "Metrics stored in assesment\n",
            "All models have been evaluated.\n",
            "[DEBUG] Starting store_results\n",
            "Metadata is: {'id': None, 'timeStamp': None, 'comments': None, 'modelName': 'Decision Tree', 'status': 'pre_tuning', 'features_used': None, 'hyperParameters': None, 'timeToFit': 1.9673247337341309, 'timeToMakePredictions': 0.004732847213745117, 'is_EDA_done': None, 'is_DataPreprocessing_done': None, 'is_FeatureAnalysis_done': None, 'is_HyperParameterOptimization_done': None, 'accuracy': 0.7604790419161677, 'precision': 0.7586043655687297, 'recall': 0.7604790419161677, 'f1-score': 0.7593815744970357, 'predictions': array([6, 6, 0, ..., 8, 0, 6]), 'model_sklearn': DecisionTreeClassifier(), 'conf_matrix': array([[ 791,   12,    3,   27,    7,    2,   59,    6,   29,    4,    4,\n",
            "           9,    2,   41],\n",
            "       [   9,   69,    0,    2,    0,    0,   17,    2,    1,    0,    3,\n",
            "           4,    1,    4],\n",
            "       [   6,    0,   16,    0,    0,    0,    1,    0,    1,    0,    0,\n",
            "           0,    0,    1],\n",
            "       [  23,    5,    0,   83,    8,    7,   18,    6,   13,    0,    8,\n",
            "           6,    7,   23],\n",
            "       [   6,    3,    0,    5,   85,    2,   10,    1,    3,    0,    0,\n",
            "           0,    0,    7],\n",
            "       [   4,    0,    0,    7,    1,  271,    4,    1,   10,    0,   31,\n",
            "           1,   15,   10],\n",
            "       [  55,   11,    0,   14,    8,    2, 1217,    3,   44,    0,    1,\n",
            "           9,    5,   47],\n",
            "       [   4,    1,    0,    3,    1,    0,    5,   54,    2,    0,    0,\n",
            "           1,    1,    3],\n",
            "       [  31,    3,    0,   20,    1,   10,   53,    1,  712,    3,    3,\n",
            "          16,    9,   26],\n",
            "       [   0,    0,    0,    2,    0,    1,    1,    0,    1,   14,    4,\n",
            "           1,    1,    2],\n",
            "       [   3,    1,    0,    2,    1,   35,    9,    1,    2,    1,  100,\n",
            "           1,    2,    9],\n",
            "       [  10,    1,    0,    7,    0,    4,   10,    2,   14,    1,    1,\n",
            "         135,    6,    7],\n",
            "       [   1,    0,    0,    6,    2,   12,    2,    0,    8,    0,    2,\n",
            "           7,  246,    6],\n",
            "       [  59,    2,    1,   13,    5,    4,   52,    2,   32,    2,    8,\n",
            "           8,    5,  271]])}\n",
            "Col is: id\n",
            "Col is: timeStamp\n",
            "Col is: comments\n",
            "Col is: modelName\n",
            "Col is: status\n",
            "Col is: features_used\n",
            "Col is: hyperParameters\n",
            "Col is: timeToFit\n",
            "Col is: timeToMakePredictions\n",
            "Col is: is_EDA_done\n",
            "Col is: is_DataPreprocessing_done\n",
            "Col is: is_FeatureAnalysis_done\n",
            "Col is: is_HyperParameterOptimization_done\n",
            "Col is: accuracy\n",
            "Col is: precision\n",
            "Col is: recall\n",
            "Col is: f1-score\n",
            "Col is: predictions\n",
            "Col is: model_sklearn\n",
            "Col is: conf_matrix\n",
            "[DEBUG 0.00s] Completed store_results, processed 1 models\n",
            "All models have been evaluated.\n",
            "[DEBUG] Starting store_results\n",
            "Metadata is: {'id': None, 'timeStamp': None, 'comments': None, 'modelName': 'Random Forest', 'status': 'pre_tuning', 'features_used': None, 'hyperParameters': None, 'timeToFit': 10.130177021026611, 'timeToMakePredictions': 0.11943292617797852, 'is_EDA_done': None, 'is_DataPreprocessing_done': None, 'is_FeatureAnalysis_done': None, 'is_HyperParameterOptimization_done': None, 'accuracy': 0.8415044910179641, 'precision': 0.8440028457175942, 'recall': 0.8415044910179641, 'f1-score': 0.8359066691920427, 'predictions': array([6, 6, 0, ..., 8, 0, 0]), 'model_sklearn': RandomForestClassifier(), 'conf_matrix': array([[ 951,    2,    1,    1,    2,    0,   26,    1,    9,    0,    0,\n",
            "           0,    0,    3],\n",
            "       [  15,   77,    0,    1,    0,    0,   12,    0,    3,    0,    0,\n",
            "           3,    0,    1],\n",
            "       [   7,    0,   16,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    1,    1],\n",
            "       [  50,    2,    0,   84,    0,   10,   13,    1,   22,    0,    0,\n",
            "           2,    3,   20],\n",
            "       [  13,    0,    0,    0,   95,    1,    7,    3,    0,    0,    0,\n",
            "           0,    0,    3],\n",
            "       [   1,    0,    0,    1,    1,  313,    3,    0,    5,    0,   13,\n",
            "           0,   15,    3],\n",
            "       [  71,    5,    0,    0,    3,    2, 1281,    1,   27,    0,    0,\n",
            "           1,    0,   25],\n",
            "       [   4,    1,    0,    0,    0,    0,    1,   64,    2,    0,    0,\n",
            "           1,    0,    2],\n",
            "       [  31,    0,    0,    1,    0,    3,   26,    2,  796,    1,    2,\n",
            "           8,    1,   17],\n",
            "       [   0,    0,    0,    0,    0,    2,    0,    0,    1,   18,    2,\n",
            "           0,    2,    2],\n",
            "       [   5,    1,    0,    1,    0,   41,    7,    0,    3,    2,   98,\n",
            "           1,    1,    7],\n",
            "       [  20,    0,    0,    1,    0,    2,    9,    0,   16,    1,    0,\n",
            "         144,    2,    3],\n",
            "       [   5,    1,    0,    1,    0,    5,    0,    0,    8,    0,    1,\n",
            "           0,  266,    5],\n",
            "       [  62,    0,    0,   10,    2,   11,   46,    4,   28,    0,    1,\n",
            "           4,    2,  294]])}\n",
            "Col is: id\n",
            "Col is: timeStamp\n",
            "Col is: comments\n",
            "Col is: modelName\n",
            "Col is: status\n",
            "Col is: features_used\n",
            "Col is: hyperParameters\n",
            "Col is: timeToFit\n",
            "Col is: timeToMakePredictions\n",
            "Col is: is_EDA_done\n",
            "Col is: is_DataPreprocessing_done\n",
            "Col is: is_FeatureAnalysis_done\n",
            "Col is: is_HyperParameterOptimization_done\n",
            "Col is: accuracy\n",
            "Col is: precision\n",
            "Col is: recall\n",
            "Col is: f1-score\n",
            "Col is: predictions\n",
            "Col is: model_sklearn\n",
            "Col is: conf_matrix\n",
            "[DEBUG 0.00s] Completed store_results, processed 1 models\n"
          ]
        }
      ],
      "source": [
        "model_results = pipeline_manager.all_pipelines_execute(methodName=\"model_selection.evaluate_models\", phaseProcess=phaseProcess, comments=comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>timeStamp</th>\n",
              "      <th>comments</th>\n",
              "      <th>modelName</th>\n",
              "      <th>status</th>\n",
              "      <th>features_used</th>\n",
              "      <th>hyperParameters</th>\n",
              "      <th>timeToFit</th>\n",
              "      <th>timeToMakePredictions</th>\n",
              "      <th>is_EDA_done</th>\n",
              "      <th>...</th>\n",
              "      <th>is_FeatureAnalysis_done</th>\n",
              "      <th>is_HyperParameterOptimization_done</th>\n",
              "      <th>accuracy_val</th>\n",
              "      <th>accuracy_test</th>\n",
              "      <th>precision_val</th>\n",
              "      <th>precision_test</th>\n",
              "      <th>recall_val</th>\n",
              "      <th>recall_test</th>\n",
              "      <th>f1-score_val</th>\n",
              "      <th>f1-score_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>972adcb09acb6aa202c809cd4ea408be1d92ef4e2a906a...</td>\n",
              "      <td>2025-04-14 16:35:22</td>\n",
              "      <td>wiLL THIS work?</td>\n",
              "      <td>Random Forest</td>\n",
              "      <td>pre_tuning</td>\n",
              "      <td>[Memory_PssTotal, Memory_PssClean, Memory_Shar...</td>\n",
              "      <td>NA</td>\n",
              "      <td>10.130177</td>\n",
              "      <td>0.119433</td>\n",
              "      <td>No</td>\n",
              "      <td>...</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>0.841504</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.844003</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.841504</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.835907</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  id            timeStamp  \\\n",
              "0  972adcb09acb6aa202c809cd4ea408be1d92ef4e2a906a...  2025-04-14 16:35:22   \n",
              "\n",
              "          comments      modelName      status  \\\n",
              "0  wiLL THIS work?  Random Forest  pre_tuning   \n",
              "\n",
              "                                       features_used hyperParameters  \\\n",
              "0  [Memory_PssTotal, Memory_PssClean, Memory_Shar...              NA   \n",
              "\n",
              "   timeToFit  timeToMakePredictions is_EDA_done  ... is_FeatureAnalysis_done  \\\n",
              "0  10.130177               0.119433          No  ...                      No   \n",
              "\n",
              "  is_HyperParameterOptimization_done accuracy_val  accuracy_test  \\\n",
              "0                                 No     0.841504             -1   \n",
              "\n",
              "   precision_val  precision_test  recall_val  recall_test  f1-score_val  \\\n",
              "0       0.844003              -1    0.841504           -1      0.835907   \n",
              "\n",
              "   f1-score_test  \n",
              "0             -1  \n",
              "\n",
              "[1 rows x 21 columns]"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_results[\"models\"][\"ensembled\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observed the updated object with all the correct attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'columns'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'columns'"
          ]
        }
      ],
      "source": [
        "model_results.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VALIDATION OBSERVATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "default_pipeline.model_selection.results_analysis[\"pre\"].plot_multiple_model_metrics([\"accuracy_val\", \"precision_val\", \"recall_val\", \"f1-score_val\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PLOTS:\n",
        "- Difference between performance \n",
        "- Early signs of overfitting\n",
        "- Feature importance \n",
        "- Plot errors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importances = models_pipeline.model_selection.list_of_models[\"Random Forest\"].preTuningState.assesment[\"model_sklearn\"].feature_importances_\n",
        "# Make a nice DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': models_pipeline.dataset.X_train.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "feature_importance_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    x=\"Importance\",\n",
        "    y=\"Feature\",\n",
        "    data=feature_importance_df\n",
        ")\n",
        "plt.title(\"Feature Importances from Random Forest\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "28_glozSPDm7",
        "LDW_CStuioNs",
        "Uv7LWIIU5X5i",
        "eaHsU58AdvlN",
        "vtG27WXGd3_R",
        "vR3xQcHnLUsk",
        "tJI5MrERTBNz",
        "VDydAnW3alRu",
        "Pi1pwj-9q_v5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
